


































































[{"body":"When you first installed Karpenter, you set up a default Provisioner. The Provisioner sets constraints on the nodes that can be created by Karpenter and the pods that can run on those nodes. The Provisioner can be set to do things like:\n Define taints to limit the pods that can run on nodes Karpenter creates Limit node creation to certain zones, instance types, and computer architectures Set defaults for node expiration  You can change your provisioner or add other provisioners to Karpenter. Here are things you should know about Provisioners:\n Karpenter won’t do anything if there is not at least one Provisioner configured. Each Provisioner that is configured is looped through by Karpenter. If Karpenter encounters a taint in the Provisioner that is not tolerated by a Pod, Karpenter won’t use that Provisioner to provision the pod. It is recommended to create Provisioners that are mutually exclusive. So no Pod should match multiple Provisioners. If multiple Provisioners are matched, Karpenter will randomly choose which to use.  If you want to modify or add provisioners to Karpenter, do the following:\n Review the following Provisioner documents:   Provisioner in the Getting Started guide for a sample default Provisioner Provisioner API for descriptions of Provisioner API values Provisioning Configuration for cloud-specific settings  Apply the new or modified Provisioner to the cluster.  The following examples illustrate different aspects of Provisioners. Refer to Running pods to see how the same features are used in Pod specs to determine where pods run.\nExample: Requirements This provisioner limits nodes to specific zones. It is flexible to both spot and on-demand capacity types.\napiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: westzones spec: requirements: - key: \"topology.kubernetes.io/zone\" operator: In values: [\"us-west-2a\", \"us-west-2b\", \"us-west-2c\"] - key: \"karpenter.sh/capacity-type\" operator: In values: [\"spot\", \"on-demand\"] provider: instanceProfile: myprofile-cluster101 With these settings, the provisioner is able to launch nodes in three availability zones and is flexible to both spot and on-demand purchase types.\nExample: Isolating Expensive Hardware A provisioner can be set up to only provision nodes on particular processor types. The following example sets a taint that only allows pods with tolerations for Nvidia GPUs to be scheduled:\napiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: gpu spec: ttlSecondsAfterEmpty: 60 requirements: - key: node.kubernetes.io/instance-type operator: In values: [\"p3.8xlarge\", \"p3.16xlarge\"] taints: - key: nvidia.com/gpu value: true effect: “NoSchedule” In order for a pod to run on a node defined in this provisioner, it must tolerate nvidia.com/gpu in its pod spec.\n","categories":"","description":"","excerpt":"When you first installed Karpenter, you set up a default Provisioner. …","ref":"/preview/tasks/provisioning-nodes/","tags":"","title":"Provisioning"},{"body":"When you first installed Karpenter, you set up a default Provisioner. The Provisioner sets constraints on the nodes that can be created by Karpenter and the pods that can run on those nodes. The Provisioner can be set to do things like:\n Define taints to limit the pods that can run on nodes Karpenter creates Limit node creation to certain zones, instance types, and computer architectures Set defaults for node expiration  You can change your provisioner or add other provisioners to Karpenter. Here are things you should know about Provisioners:\n Karpenter won’t do anything if there is not at least one Provisioner configured. Each Provisioner that is configured is looped through by Karpenter. If Karpenter encounters a taint in the Provisioner that is not tolerated by a Pod, Karpenter won’t use that Provisioner to provision the pod. It is recommended to create Provisioners that are mutually exclusive. So no Pod should match multiple Provisioners. If multiple Provisioners are matched, Karpenter will randomly choose which to use.  If you want to modify or add provisioners to Karpenter, do the following:\n Review the following Provisioner documents:   Provisioner in the Getting Started guide for a sample default Provisioner Provisioner API for descriptions of Provisioner API values Provisioning Configuration for cloud-specific settings  Apply the new or modified Provisioner to the cluster.  The following examples illustrate different aspects of Provisioners. Refer to Running pods to see how the same features are used in Pod specs to determine where pods run.\nExample: Requirements This provisioner limits nodes to specific zones. It is flexible to both spot and on-demand capacity types.\napiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: westzones spec: requirements: - key: \"topology.kubernetes.io/zone\" operator: In values: [\"us-west-2a\", \"us-west-2b\", \"us-west-2c\"] - key: \"karpenter.sh/capacity-type\" operator: In values: [\"spot\", \"on-demand\"] provider: instanceProfile: myprofile-cluster101 With these settings, the provisioner is able to launch nodes in three availability zones and is flexible to both spot and on-demand purchase types.\nExample: Isolating Expensive Hardware A provisioner can be set up to only provision nodes on particular processor types. The following example sets a taint that only allows pods with tolerations for Nvidia GPUs to be scheduled:\napiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: gpu spec: ttlSecondsAfterEmpty: 60 requirements: - key: node.kubernetes.io/instance-type operator: In values: [\"p3.8xlarge\", \"p3.16xlarge\"] taints: - key: nvidia.com/gpu value: true effect: “NoSchedule” In order for a pod to run on a node defined in this provisioner, it must tolerate nvidia.com/gpu in its pod spec.\n","categories":"","description":"","excerpt":"When you first installed Karpenter, you set up a default Provisioner. …","ref":"/v0.5.0/tasks/provisioning-task/","tags":"","title":"Provisioning nodes"},{"body":"When you first installed Karpenter, you set up a default Provisioner. The Provisioner sets constraints on the nodes that can be created by Karpenter and the pods that can run on those nodes. The Provisioner can be set to do things like:\n Define taints to limit the pods that can run on nodes Karpenter creates Limit node creation to certain zones, instance types, and computer architectures Set defaults for node expiration  You can change your provisioner or add other provisioners to Karpenter. Here are things you should know about Provisioners:\n Karpenter won’t do anything if there is not at least one Provisioner configured. Each Provisioner that is configured is looped through by Karpenter. If Karpenter encounters a taint in the Provisioner that is not tolerated by a Pod, Karpenter won’t use that Provisioner to provision the pod. It is recommended to create Provisioners that are mutually exclusive. So no Pod should match multiple Provisioners. If multiple Provisioners are matched, Karpenter will randomly choose which to use.  If you want to modify or add provisioners to Karpenter, do the following:\n Review the following Provisioner documents:   Provisioner in the Getting Started guide for a sample default Provisioner Provisioner API for descriptions of Provisioner API values Provisioning Configuration for cloud-specific settings  Apply the new or modified Provisioner to the cluster.  The following examples illustrate different aspects of Provisioners. Refer to Running pods to see how the same features are used in Pod specs to determine where pods run.\nExample: Requirements This provisioner limits nodes to specific zones. It is flexible to both spot and on-demand capacity types.\napiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: westzones spec: requirements: - key: \"topology.kubernetes.io/zone\" operator: In values: [\"us-west-2a\", \"us-west-2b\", \"us-west-2c\"] - key: \"karpenter.sh/capacity-type\" operator: In values: [\"spot\", \"on-demand\"] provider: instanceProfile: myprofile-cluster101 With these settings, the provisioner is able to launch nodes in three availability zones and is flexible to both spot and on-demand purchase types.\nExample: Isolating Expensive Hardware A provisioner can be set up to only provision nodes on particular processor types. The following example sets a taint that only allows pods with tolerations for Nvidia GPUs to be scheduled:\napiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: gpu spec: ttlSecondsAfterEmpty: 60 requirements: - key: node.kubernetes.io/instance-type operator: In values: [\"p3.8xlarge\", \"p3.16xlarge\"] taints: - key: nvidia.com/gpu value: true effect: “NoSchedule” In order for a pod to run on a node defined in this provisioner, it must tolerate nvidia.com/gpu in its pod spec.\n","categories":"","description":"","excerpt":"When you first installed Karpenter, you set up a default Provisioner. …","ref":"/v0.5.2/tasks/provisioning-task/","tags":"","title":"Provisioning nodes"},{"body":"When you first installed Karpenter, you set up a default Provisioner. The Provisioner sets constraints on the nodes that can be created by Karpenter and the pods that can run on those nodes. The Provisioner can be set to do things like:\n Define taints to limit the pods that can run on nodes Karpenter creates Limit node creation to certain zones, instance types, and computer architectures Set defaults for node expiration  You can change your provisioner or add other provisioners to Karpenter. Here are things you should know about Provisioners:\n Karpenter won’t do anything if there is not at least one Provisioner configured. Each Provisioner that is configured is looped through by Karpenter. If Karpenter encounters a taint in the Provisioner that is not tolerated by a Pod, Karpenter won’t use that Provisioner to provision the pod. It is recommended to create Provisioners that are mutually exclusive. So no Pod should match multiple Provisioners. If multiple Provisioners are matched, Karpenter will randomly choose which to use.  If you want to modify or add provisioners to Karpenter, do the following:\n Review the following Provisioner documents:   Provisioner in the Getting Started guide for a sample default Provisioner Provisioner API for descriptions of Provisioner API values Provisioning Configuration for cloud-specific settings  Apply the new or modified Provisioner to the cluster.  The following examples illustrate different aspects of Provisioners. Refer to Running pods to see how the same features are used in Pod specs to determine where pods run.\nExample: Requirements This provisioner limits nodes to specific zones. It is flexible to both spot and on-demand capacity types.\napiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: westzones spec: requirements: - key: \"topology.kubernetes.io/zone\" operator: In values: [\"us-west-2a\", \"us-west-2b\", \"us-west-2c\"] - key: \"karpenter.sh/capacity-type\" operator: In values: [\"spot\", \"on-demand\"] provider: instanceProfile: myprofile-cluster101 With these settings, the provisioner is able to launch nodes in three availability zones and is flexible to both spot and on-demand purchase types.\nExample: Isolating Expensive Hardware A provisioner can be set up to only provision nodes on particular processor types. The following example sets a taint that only allows pods with tolerations for Nvidia GPUs to be scheduled:\napiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: gpu spec: ttlSecondsAfterEmpty: 60 requirements: - key: node.kubernetes.io/instance-type operator: In values: [\"p3.8xlarge\", \"p3.16xlarge\"] taints: - key: nvidia.com/gpu value: true effect: “NoSchedule” In order for a pod to run on a node defined in this provisioner, it must tolerate nvidia.com/gpu in its pod spec.\n","categories":"","description":"","excerpt":"When you first installed Karpenter, you set up a default Provisioner. …","ref":"/v0.5.3/tasks/provisioning-task/","tags":"","title":"Provisioning nodes"},{"body":"Deletion Workflow Finalizer Karpenter adds a finalizer to provisioned nodes. Review how finalizers work.\nDrain Nodes Review how to safely drain a node.\nDelete Node Karpenter changes the behavior of kubectl delete node. Nodes will be drained, and then the underlying instance will be deleted.\nDisruption Budget Karpenter respects Pod Disruption Budgets. Review what disruptions are, and how to configure them.\nGenerally, pod workloads may be configured with .spec.minAvailable and/or .spec.maxUnavailable. Karpenter provisions nodes to accommodate these constraints.\nEmptiness Karpenter will delete nodes (and the instance) that are considered empty of pods. Daemonset pods are not included in this calculation.\nExpiry Nodes may be configured to expire. That is, a maximum lifetime in seconds starting with the node joining the cluster. Review the ttlSecondsUntilExpired field of the provisioner API.\nNote that newly created nodes have a Kubernetes version matching the control plane. One use case for node expiry is to handle node upgrades. Old nodes (with a potentially outdated Kubernetes version) are deleted, and replaced with nodes on the current version.\n","categories":"","description":"","excerpt":"Deletion Workflow Finalizer Karpenter adds a finalizer to provisioned …","ref":"/preview/tasks/deprovisioning-nodes/","tags":"","title":"Deprovisioning"},{"body":"Karpenter automatically provisions new nodes in response to unschedulable pods. Karpenter does this by observing events within the Kubernetes cluster, and then sending commands to the underlying cloud provider.\nIn this example, the cluster is running on Amazon Web Services (AWS) Elastic Kubernetes Service (EKS). Karpenter is designed to be cloud provider agnostic, but currently only supports AWS. Contributions are welcomed.\nThis guide should take less than 1 hour to complete, and cost less than $0.25. Follow the clean-up instructions to reduce any charges.\nInstall Karpenter is installed in clusters with a helm chart.\nKarpenter additionally requires IAM Roles for Service Accounts (IRSA). IRSA permits Karpenter (within the cluster) to make privileged requests to AWS (as the cloud provider).\nRequired Utilities Install these tools before proceeding:\n AWS CLI kubectl - the Kubernetes CLI eksctl - the CLI for AWS EKS helm - the package manager for Kubernetes  Login to the AWS CLI with a user that has sufficient privileges to create a cluster.\nEnvironment Variables After setting up the tools, set the following environment variables to store commonly used values.\nexport CLUSTER_NAME=$USER-karpenter-demo export AWS_DEFAULT_REGION=us-west-2 AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) Create a Cluster Create a cluster with eksctl. This example configuration file specifies a basic cluster with one initial node and sets up an IAM OIDC provider for the cluster to enable IAM roles for pods:\ncat \u003c\u003cEOF \u003e cluster.yaml --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} version: \"1.21\" managedNodeGroups: - instanceType: m5.large amiFamily: AmazonLinux2 name: ${CLUSTER_NAME}-ng desiredCapacity: 1 minSize: 1 maxSize: 10 iam: withOIDC: true EOF eksctl create cluster -f cluster.yaml This guide uses a managed node group to host Karpenter.\nKarpenter itself can run anywhere, including on self-managed node groups, managed node groups, or AWS Fargate.\nKarpenter will provision EC2 instances in your account.\nTag Subnets Karpenter discovers subnets tagged kubernetes.io/cluster/$CLUSTER_NAME. Add this tag to subnets associated configured for your cluster. Retrieve the subnet IDs and tag them with the cluster name.\nSUBNET_IDS=$(aws cloudformation describe-stacks \\  --stack-name eksctl-${CLUSTER_NAME}-cluster \\  --query 'Stacks[].Outputs[?OutputKey==`SubnetsPrivate`].OutputValue' \\  --output text) aws ec2 create-tags \\  --resources $(echo $SUBNET_IDS | tr ',' '\\n') \\  --tags Key=\"kubernetes.io/cluster/${CLUSTER_NAME}\",Value= Create the KarpenterNode IAM Role Instances launched by Karpenter must run with an InstanceProfile that grants permissions necessary to run containers and configure networking. Karpenter discovers the InstanceProfile using the name KarpenterNodeRole-${ClusterName}.\nFirst, create the IAM resources using AWS CloudFormation.\nTEMPOUT=$(mktemp) curl -fsSL https://karpenter.sh/docs/getting-started/cloudformation.yaml \u003e $TEMPOUT \\ \u0026\u0026 aws cloudformation deploy \\  --stack-name Karpenter-${CLUSTER_NAME} \\  --template-file ${TEMPOUT} \\  --capabilities CAPABILITY_NAMED_IAM \\  --parameter-overrides ClusterName=${CLUSTER_NAME} Second, grant access to instances using the profile to connect to the cluster. This command adds the Karpenter node role to your aws-auth configmap, allowing nodes with this role to connect to the cluster.\neksctl create iamidentitymapping \\  --username system:node:{{EC2PrivateDNSName}} \\  --cluster ${CLUSTER_NAME} \\  --arn arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeRole-${CLUSTER_NAME} \\  --group system:bootstrappers \\  --group system:nodes Now, Karpenter can launch new EC2 instances and those instances can connect to your cluster.\nCreate the KarpenterController IAM Role Karpenter requires permissions like launching instances. This will create an AWS IAM Role, Kubernetes service account, and associate them using IRSA.\neksctl create iamserviceaccount \\ --cluster $CLUSTER_NAME --name karpenter --namespace karpenter \\ --attach-policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/KarpenterControllerPolicy-$CLUSTER_NAME \\ --approve Create the EC2 Spot Service Linked Role This step is only necessary if this is the first time you’re using EC2 Spot in this account. More details are available here.\naws iam create-service-linked-role --aws-service-name spot.amazonaws.com # If the role has already been successfully created, you will see: # An error occurred (InvalidInput) when calling the CreateServiceLinkedRole operation: Service role name AWSServiceRoleForEC2Spot has been taken in this account, please try a different suffix. Install Karpenter Helm Chart Use helm to deploy Karpenter to the cluster.\nWe created a Kubernetes service account when we created the cluster using eksctl. Thus, we don’t need the helm chart to do that.\nhelm repo add karpenter https://charts.karpenter.sh helm repo update helm upgrade --install karpenter karpenter/karpenter --namespace karpenter \\  --create-namespace --set serviceAccount.create=false --version 0.5.4 \\  --set controller.clusterName=${CLUSTER_NAME} \\  --set controller.clusterEndpoint=$(aws eks describe-cluster --name ${CLUSTER_NAME} --query \"cluster.endpoint\" --output json) \\  --wait # for the defaulting webhook to install before creating a Provisioner Enable Debug Logging (optional) kubectl patch configmap config-logging -n karpenter --patch '{\"data\":{\"loglevel.controller\":\"debug\"}}' Create Grafana dashboards (optional) The Karpenter repo contains multiple importable dashboards for an existing Grafana instance. See the Grafana documentation for instructions to import a dashboard.\nDeploy a temporary Prometheus and Grafana stack (optional) The following commands will deploy a Prometheus and Grafana stack that is suitable for this guide but does not include persistent storage or other configurations that would be necessary for monitoring a production deployment of Karpenter.\nhelm repo add grafana-charts https://grafana.github.io/helm-charts helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update kubectl create namespace monitoring curl -fsSL https://karpenter.sh/docs/getting-started/prometheus-values.yaml | tee prometheus-values.yaml helm install --namespace monitoring prometheus prometheus-community/prometheus --values prometheus-values.yaml curl -fsSL https://karpenter.sh/docs/getting-started/grafana-values.yaml | tee grafana-values.yaml helm install --namespace monitoring grafana grafana-charts/grafana --values grafana-values.yaml The Grafana instance may be accessed using port forwarding.\nkubectl port-forward --namespace monitoring svc/grafana 3000:80 The new stack has only one user, admin, and the password is stored in a secret. The following command will retrieve the password.\nkubectl get secret --namespace monitoring grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode Provisioner A single Karpenter provisioner is capable of handling many different pod shapes. Karpenter makes scheduling and provisioning decisions based on pod attributes such as labels and affinity. In other words, Karpenter eliminates the need to manage many different node groups.\nCreate a default provisioner using the command below. This provisioner configures instances to connect to your cluster’s endpoint and discovers resources like subnets and security groups using the cluster’s name.\nThe ttlSecondsAfterEmpty value configures Karpenter to terminate empty nodes. This behavior can be disabled by leaving the value undefined.\nReview the provisioner CRD for more information. For example, ttlSecondsUntilExpired configures Karpenter to terminate nodes when a maximum age is reached.\nNote: This provisioner will create capacity as long as the sum of all created capacity is less than the specified limit.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\"] limits: resources: cpu: 1000 provider: instanceProfile: KarpenterNodeInstanceProfile-${CLUSTER_NAME} ttlSecondsAfterEmpty: 30 EOF First Use Karpenter is now active and ready to begin provisioning nodes. Create some pods using a deployment, and watch Karpenter provision nodes in response.\nAutomatic Node Provisioning This deployment uses the pause image and starts with zero replicas.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: inflate spec: replicas: 0 selector: matchLabels: app: inflate template: metadata: labels: app: inflate spec: terminationGracePeriodSeconds: 0 containers: - name: inflate image: public.ecr.aws/eks-distro/kubernetes/pause:3.2 resources: requests: cpu: 1 EOF kubectl scale deployment inflate --replicas 5 kubectl logs -f -n karpenter $(kubectl get pods -n karpenter -l karpenter=controller -o name) Automatic Node Termination Now, delete the deployment. After 30 seconds (ttlSecondsAfterEmpty), Karpenter should terminate the now empty nodes.\nkubectl delete deployment inflate kubectl logs -f -n karpenter $(kubectl get pods -n karpenter -l karpenter=controller -o name) Manual Node Termination If you delete a node with kubectl, Karpenter will gracefully cordon, drain, and shutdown the corresponding instance. Under the hood, Karpenter adds a finalizer to the node object, which blocks deletion until all pods are drained and the instance is terminated. Keep in mind, this only works for nodes provisioned by Karpenter.\nkubectl delete node $NODE_NAME Cleanup To avoid additional charges, remove the demo infrastructure from your AWS account.\nhelm uninstall karpenter --namespace karpenter eksctl delete iamserviceaccount --cluster ${CLUSTER_NAME} --name karpenter --namespace karpenter aws cloudformation delete-stack --stack-name Karpenter-${CLUSTER_NAME} aws ec2 describe-launch-templates \\  | jq -r \".LaunchTemplates[].LaunchTemplateName\" \\  | grep -i Karpenter-${CLUSTER_NAME} \\  | xargs -I{} aws ec2 delete-launch-template --launch-template-name {} eksctl delete cluster --name ${CLUSTER_NAME}  Next Steps: ","categories":"","description":"","excerpt":"Karpenter automatically provisions new nodes in response to …","ref":"/preview/getting-started/","tags":"","title":"Getting Started with Karpenter on AWS"},{"body":"Karpenter automatically provisions new nodes in response to unschedulable pods. Karpenter does this by observing events within the Kubernetes cluster, and then sending commands to the underlying cloud provider.\nIn this example, the cluster is running on Amazon Web Services (AWS) Elastic Kubernetes Service (EKS). Karpenter is designed to be cloud provider agnostic, but currently only supports AWS. Contributions are welcomed.\nThis guide should take less than 1 hour to complete, and cost less than $0.25. Follow the clean-up instructions to reduce any charges.\nInstall Karpenter is installed in clusters with a helm chart.\nKarpenter additionally requires IAM Roles for Service Accounts (IRSA). IRSA permits Karpenter (within the cluster) to make privileged requests to AWS (as the cloud provider).\nRequired Utilities Install these tools before proceeding:\n AWS CLI kubectl - the Kubernetes CLI eksctl - the CLI for AWS EKS helm - the package manager for Kubernetes  Login to the AWS CLI with a user that has sufficient privileges to create a cluster.\nEnvironment Variables After setting up the tools, set the following environment variables to store commonly used values.\nexport CLUSTER_NAME=$USER-karpenter-demo export AWS_DEFAULT_REGION=us-west-2 AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) Create a Cluster Create a cluster with eksctl. This example configuration file specifies a basic cluster with one initial node and sets up an IAM OIDC provider for the cluster to enable IAM roles for pods:\ncat \u003c\u003cEOF \u003e cluster.yaml --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} version: \"1.21\" managedNodeGroups: - instanceType: m5.large amiFamily: AmazonLinux2 name: ${CLUSTER_NAME}-ng desiredCapacity: 1 minSize: 1 maxSize: 10 iam: withOIDC: true EOF eksctl create cluster -f cluster.yaml This guide uses a self-managed node group to host Karpenter.\nKarpenter itself can run anywhere, including on self-managed node groups, managed node groups, or AWS Fargate.\nKarpenter will provision EC2 instances in your account.\nTag Subnets Karpenter discovers subnets tagged kubernetes.io/cluster/$CLUSTER_NAME. Add this tag to subnets associated configured for your cluster. Retrieve the subnet IDs and tag them with the cluster name.\nSUBNET_IDS=$(aws cloudformation describe-stacks \\  --stack-name eksctl-${CLUSTER_NAME}-cluster \\  --query 'Stacks[].Outputs[?OutputKey==`SubnetsPrivate`].OutputValue' \\  --output text) aws ec2 create-tags \\  --resources $(echo $SUBNET_IDS | tr ',' '\\n') \\  --tags Key=\"kubernetes.io/cluster/${CLUSTER_NAME}\",Value= Create the KarpenterNode IAM Role Instances launched by Karpenter must run with an InstanceProfile that grants permissions necessary to run containers and configure networking. Karpenter discovers the InstanceProfile using the name KarpenterNodeRole-${ClusterName}.\nFirst, create the IAM resources using AWS CloudFormation.\nTEMPOUT=$(mktemp) curl -fsSL https://karpenter.sh/docs/getting-started/cloudformation.yaml \u003e $TEMPOUT \\ \u0026\u0026 aws cloudformation deploy \\  --stack-name Karpenter-${CLUSTER_NAME} \\  --template-file ${TEMPOUT} \\  --capabilities CAPABILITY_NAMED_IAM \\  --parameter-overrides ClusterName=${CLUSTER_NAME} Second, grant access to instances using the profile to connect to the cluster. This command adds the Karpenter node role to your aws-auth configmap, allowing nodes with this role to connect to the cluster.\neksctl create iamidentitymapping \\  --username system:node:{{EC2PrivateDNSName}} \\  --cluster ${CLUSTER_NAME} \\  --arn arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeRole-${CLUSTER_NAME} \\  --group system:bootstrappers \\  --group system:nodes Now, Karpenter can launch new EC2 instances and those instances can connect to your cluster.\nCreate the KarpenterController IAM Role Karpenter requires permissions like launching instances. This will create an AWS IAM Role, Kubernetes service account, and associate them using IRSA.\neksctl create iamserviceaccount \\ --cluster $CLUSTER_NAME --name karpenter --namespace karpenter \\ --attach-policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/KarpenterControllerPolicy-$CLUSTER_NAME \\ --approve Create the EC2 Spot Service Linked Role This step is only necessary if this is the first time you’re using EC2 Spot in this account. More details are available here.\naws iam create-service-linked-role --aws-service-name spot.amazonaws.com # If the role has already been successfully created, you will see: # An error occurred (InvalidInput) when calling the CreateServiceLinkedRole operation: Service role name AWSServiceRoleForEC2Spot has been taken in this account, please try a different suffix. Install Karpenter Helm Chart Use helm to deploy Karpenter to the cluster.\nWe created a Kubernetes service account when we created the cluster using eksctl. Thus, we don’t need the helm chart to do that.\nhelm repo add karpenter https://charts.karpenter.sh helm repo update helm upgrade --install karpenter karpenter/karpenter --namespace karpenter \\  --create-namespace --set serviceAccount.create=false --version 0.4.3 \\  --set controller.clusterName=${CLUSTER_NAME} \\  --set controller.clusterEndpoint=$(aws eks describe-cluster --name ${CLUSTER_NAME} --query \"cluster.endpoint\" --output json) \\  --set defaultProvisioner.create=false \\  --wait # for the defaulting webhook to install before creating a Provisioner Enable Debug Logging (optional) kubectl patch configmap config-logging -n karpenter --patch '{\"data\":{\"loglevel.controller\":\"debug\"}}' Create Grafana dashboards (optional) The Karpenter repo contains multiple importable dashboards for an existing Grafana instance. See the Grafana documentation for instructions to import a dashboard.\nDeploy a temporary Prometheus and Grafana stack (optional) The following commands will deploy a Prometheus and Grafana stack that is suitable for this guide but does not include persistent storage or other configurations that would be necessary for monitoring a production deployment of Karpenter.\nhelm repo add grafana-charts https://grafana.github.io/helm-charts helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update kubectl create namespace monitoring curl -fsSL https://karpenter.sh/docs/getting-started/prometheus-values.yaml helm install --namespace monitoring prometheus prometheus-community/prometheus --values prometheus-values.yaml curl -fsSL https://karpenter.sh/docs/getting-started/grafana-values.yaml helm install --namespace monitoring grafana grafana-charts/grafana --values grafana-values.yaml The Grafana instance may be accessed using port forwarding.\nkubectl port-forward --namespace monitoring svc/grafana 3000:80 The new stack has only one user, admin, and the password is stored in a secret. The following command will retrieve the password.\nkubectl get secret --namespace monitoring grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode Provisioner A single Karpenter provisioner is capable of handling many different pod shapes. Karpenter makes scheduling and provisioning decisions based on pod attributes such as labels and affinity. In other words, Karpenter eliminates the need to manage many different node groups.\nCreate a default provisioner using the command below. This provisioner configures instances to connect to your cluster’s endpoint and discovers resources like subnets and security groups using the cluster’s name.\nThe ttlSecondsAfterEmpty value configures Karpenter to terminate empty nodes. This behavior can be disabled by leaving the value undefined.\nReview the provisioner CRD for more information. For example, ttlSecondsUntilExpired configures Karpenter to terminate nodes when a maximum age is reached.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\"] provider: instanceProfile: KarpenterNodeInstanceProfile-${CLUSTER_NAME} ttlSecondsAfterEmpty: 30 EOF First Use Karpenter is now active and ready to begin provisioning nodes. Create some pods using a deployment, and watch Karpenter provision nodes in response.\nAutomatic Node Provisioning This deployment uses the pause image and starts with zero replicas.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: inflate spec: replicas: 0 selector: matchLabels: app: inflate template: metadata: labels: app: inflate spec: containers: - name: inflate image: public.ecr.aws/eks-distro/kubernetes/pause:3.2 resources: requests: cpu: 1 EOF kubectl scale deployment inflate --replicas 5 kubectl logs -f -n karpenter $(kubectl get pods -n karpenter -l karpenter=controller -o name) Automatic Node Termination Now, delete the deployment. After 30 seconds (ttlSecondsAfterEmpty), Karpenter should terminate the now empty nodes.\nkubectl delete deployment inflate kubectl logs -f -n karpenter $(kubectl get pods -n karpenter -l karpenter=controller -o name) Manual Node Termination If you delete a node with kubectl, Karpenter will gracefully cordon, drain, and shutdown the corresponding instance. Under the hood, Karpenter adds a finalizer to the node object, which blocks deletion until all pods are drained and the instance is terminated. Keep in mind, this only works for nodes provisioned by Karpenter.\nkubectl delete node $NODE_NAME Cleanup To avoid additional charges, remove the demo infrastructure from your AWS account.\nhelm uninstall karpenter --namespace karpenter eksctl delete iamserviceaccount --cluster ${CLUSTER_NAME} --name karpenter --namespace karpenter aws cloudformation delete-stack --stack-name Karpenter-${CLUSTER_NAME} aws ec2 describe-launch-templates \\  | jq -r \".LaunchTemplates[].LaunchTemplateName\" \\  | grep -i Karpenter-${CLUSTER_NAME} \\  | xargs -I{} aws ec2 delete-launch-template --launch-template-name {} eksctl delete cluster --name ${CLUSTER_NAME} ","categories":"","description":"","excerpt":"Karpenter automatically provisions new nodes in response to …","ref":"/v0.4.3/getting-started/","tags":"","title":"Getting Started with Karpenter on AWS"},{"body":"Karpenter automatically provisions new nodes in response to unschedulable pods. Karpenter does this by observing events within the Kubernetes cluster, and then sending commands to the underlying cloud provider.\nIn this example, the cluster is running on Amazon Web Services (AWS) Elastic Kubernetes Service (EKS). Karpenter is designed to be cloud provider agnostic, but currently only supports AWS. Contributions are welcomed.\nThis guide should take less than 1 hour to complete, and cost less than $0.25. Follow the clean-up instructions to reduce any charges.\nInstall Karpenter is installed in clusters with a helm chart.\nKarpenter additionally requires IAM Roles for Service Accounts (IRSA). IRSA permits Karpenter (within the cluster) to make privileged requests to AWS (as the cloud provider).\nRequired Utilities Install these tools before proceeding:\n AWS CLI kubectl - the Kubernetes CLI eksctl - the CLI for AWS EKS helm - the package manager for Kubernetes  Login to the AWS CLI with a user that has sufficient privileges to create a cluster.\nEnvironment Variables After setting up the tools, set the following environment variables to store commonly used values.\nexport CLUSTER_NAME=$USER-karpenter-demo export AWS_DEFAULT_REGION=us-west-2 AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) Create a Cluster Create a cluster with eksctl. This example configuration file specifies a basic cluster with one initial node and sets up an IAM OIDC provider for the cluster to enable IAM roles for pods:\ncat \u003c\u003cEOF \u003e cluster.yaml --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} version: \"1.21\" managedNodeGroups: - instanceType: m5.large amiFamily: AmazonLinux2 name: ${CLUSTER_NAME}-ng desiredCapacity: 1 minSize: 1 maxSize: 10 iam: withOIDC: true EOF eksctl create cluster -f cluster.yaml This guide uses a self-managed node group to host Karpenter.\nKarpenter itself can run anywhere, including on self-managed node groups, managed node groups, or AWS Fargate.\nKarpenter will provision EC2 instances in your account.\nTag Subnets Karpenter discovers subnets tagged kubernetes.io/cluster/$CLUSTER_NAME. Add this tag to subnets associated configured for your cluster. Retrieve the subnet IDs and tag them with the cluster name.\nSUBNET_IDS=$(aws cloudformation describe-stacks \\  --stack-name eksctl-${CLUSTER_NAME}-cluster \\  --query 'Stacks[].Outputs[?OutputKey==`SubnetsPrivate`].OutputValue' \\  --output text) aws ec2 create-tags \\  --resources $(echo $SUBNET_IDS | tr ',' '\\n') \\  --tags Key=\"kubernetes.io/cluster/${CLUSTER_NAME}\",Value= Create the KarpenterNode IAM Role Instances launched by Karpenter must run with an InstanceProfile that grants permissions necessary to run containers and configure networking. Karpenter discovers the InstanceProfile using the name KarpenterNodeRole-${ClusterName}.\nFirst, create the IAM resources using AWS CloudFormation.\nTEMPOUT=$(mktemp) curl -fsSL https://karpenter.sh/docs/getting-started/cloudformation.yaml \u003e $TEMPOUT \\ \u0026\u0026 aws cloudformation deploy \\  --stack-name Karpenter-${CLUSTER_NAME} \\  --template-file ${TEMPOUT} \\  --capabilities CAPABILITY_NAMED_IAM \\  --parameter-overrides ClusterName=${CLUSTER_NAME} Second, grant access to instances using the profile to connect to the cluster. This command adds the Karpenter node role to your aws-auth configmap, allowing nodes with this role to connect to the cluster.\neksctl create iamidentitymapping \\  --username system:node:{{EC2PrivateDNSName}} \\  --cluster ${CLUSTER_NAME} \\  --arn arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeRole-${CLUSTER_NAME} \\  --group system:bootstrappers \\  --group system:nodes Now, Karpenter can launch new EC2 instances and those instances can connect to your cluster.\nCreate the KarpenterController IAM Role Karpenter requires permissions like launching instances. This will create an AWS IAM Role, Kubernetes service account, and associate them using IRSA.\neksctl create iamserviceaccount \\ --cluster $CLUSTER_NAME --name karpenter --namespace karpenter \\ --attach-policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/KarpenterControllerPolicy-$CLUSTER_NAME \\ --approve Create the EC2 Spot Service Linked Role This step is only necessary if this is the first time you’re using EC2 Spot in this account. More details are available here.\naws iam create-service-linked-role --aws-service-name spot.amazonaws.com # If the role has already been successfully created, you will see: # An error occurred (InvalidInput) when calling the CreateServiceLinkedRole operation: Service role name AWSServiceRoleForEC2Spot has been taken in this account, please try a different suffix. Install Karpenter Helm Chart Use helm to deploy Karpenter to the cluster.\nWe created a Kubernetes service account when we created the cluster using eksctl. Thus, we don’t need the helm chart to do that.\nhelm repo add karpenter https://charts.karpenter.sh helm repo update helm upgrade --install karpenter karpenter/karpenter --namespace karpenter \\  --create-namespace --set serviceAccount.create=false --version 0.5.4 \\  --set controller.clusterName=${CLUSTER_NAME} \\  --set controller.clusterEndpoint=$(aws eks describe-cluster --name ${CLUSTER_NAME} --query \"cluster.endpoint\" --output json) \\  --wait # for the defaulting webhook to install before creating a Provisioner Enable Debug Logging (optional) kubectl patch configmap config-logging -n karpenter --patch '{\"data\":{\"loglevel.controller\":\"debug\"}}' Provisioner A single Karpenter provisioner is capable of handling many different pod shapes. Karpenter makes scheduling and provisioning decisions based on pod attributes such as labels and affinity. In other words, Karpenter eliminates the need to manage many different node groups.\nCreate a default provisioner using the command below. This provisioner configures instances to connect to your cluster’s endpoint and discovers resources like subnets and security groups using the cluster’s name.\nThe ttlSecondsAfterEmpty value configures Karpenter to terminate empty nodes. This behavior can be disabled by leaving the value undefined.\nReview the provisioner CRD for more information. For example, ttlSecondsUntilExpired configures Karpenter to terminate nodes when a maximum age is reached.\nNote: This provisioner will create capacity as long as the sum of all created capacity is less than the specified limit.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\"] limits: resources: cpu: 1000 provider: instanceProfile: KarpenterNodeInstanceProfile-${CLUSTER_NAME} ttlSecondsAfterEmpty: 30 EOF First Use Karpenter is now active and ready to begin provisioning nodes. Create some pods using a deployment, and watch Karpenter provision nodes in response.\nAutomatic Node Provisioning This deployment uses the pause image and starts with zero replicas.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: inflate spec: replicas: 0 selector: matchLabels: app: inflate template: metadata: labels: app: inflate spec: terminationGracePeriodSeconds: 0 containers: - name: inflate image: public.ecr.aws/eks-distro/kubernetes/pause:3.2 resources: requests: cpu: 1 EOF kubectl scale deployment inflate --replicas 5 kubectl logs -f -n karpenter $(kubectl get pods -n karpenter -l karpenter=controller -o name) Automatic Node Termination Now, delete the deployment. After 30 seconds (ttlSecondsAfterEmpty), Karpenter should terminate the now empty nodes.\nkubectl delete deployment inflate kubectl logs -f -n karpenter $(kubectl get pods -n karpenter -l karpenter=controller -o name) Manual Node Termination If you delete a node with kubectl, Karpenter will gracefully cordon, drain, and shutdown the corresponding instance. Under the hood, Karpenter adds a finalizer to the node object, which blocks deletion until all pods are drained and the instance is terminated. Keep in mind, this only works for nodes provisioned by Karpenter.\nkubectl delete node $NODE_NAME Cleanup To avoid additional charges, remove the demo infrastructure from your AWS account.\nhelm uninstall karpenter --namespace karpenter eksctl delete iamserviceaccount --cluster ${CLUSTER_NAME} --name karpenter --namespace karpenter aws cloudformation delete-stack --stack-name Karpenter-${CLUSTER_NAME} aws ec2 describe-launch-templates \\  | jq -r \".LaunchTemplates[].LaunchTemplateName\" \\  | grep -i Karpenter-${CLUSTER_NAME} \\  | xargs -I{} aws ec2 delete-launch-template --launch-template-name {} eksctl delete cluster --name ${CLUSTER_NAME}  Next Steps: ","categories":"","description":"","excerpt":"Karpenter automatically provisions new nodes in response to …","ref":"/v0.5.0/getting-started/","tags":"","title":"Getting Started with Karpenter on AWS"},{"body":"Karpenter automatically provisions new nodes in response to unschedulable pods. Karpenter does this by observing events within the Kubernetes cluster, and then sending commands to the underlying cloud provider.\nIn this example, the cluster is running on Amazon Web Services (AWS) Elastic Kubernetes Service (EKS). Karpenter is designed to be cloud provider agnostic, but currently only supports AWS. Contributions are welcomed.\nThis guide should take less than 1 hour to complete, and cost less than $0.25. Follow the clean-up instructions to reduce any charges.\nInstall Karpenter is installed in clusters with a helm chart.\nKarpenter additionally requires IAM Roles for Service Accounts (IRSA). IRSA permits Karpenter (within the cluster) to make privileged requests to AWS (as the cloud provider).\nRequired Utilities Install these tools before proceeding:\n AWS CLI kubectl - the Kubernetes CLI eksctl - the CLI for AWS EKS helm - the package manager for Kubernetes  Login to the AWS CLI with a user that has sufficient privileges to create a cluster.\nEnvironment Variables After setting up the tools, set the following environment variables to store commonly used values.\nexport CLUSTER_NAME=$USER-karpenter-demo export AWS_DEFAULT_REGION=us-west-2 AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) Create a Cluster Create a cluster with eksctl. This example configuration file specifies a basic cluster with one initial node and sets up an IAM OIDC provider for the cluster to enable IAM roles for pods:\ncat \u003c\u003cEOF \u003e cluster.yaml --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} version: \"1.21\" managedNodeGroups: - instanceType: m5.large amiFamily: AmazonLinux2 name: ${CLUSTER_NAME}-ng desiredCapacity: 1 minSize: 1 maxSize: 10 iam: withOIDC: true EOF eksctl create cluster -f cluster.yaml This guide uses a self-managed node group to host Karpenter.\nKarpenter itself can run anywhere, including on self-managed node groups, managed node groups, or AWS Fargate.\nKarpenter will provision EC2 instances in your account.\nTag Subnets Karpenter discovers subnets tagged kubernetes.io/cluster/$CLUSTER_NAME. Add this tag to subnets associated configured for your cluster. Retrieve the subnet IDs and tag them with the cluster name.\nSUBNET_IDS=$(aws cloudformation describe-stacks \\  --stack-name eksctl-${CLUSTER_NAME}-cluster \\  --query 'Stacks[].Outputs[?OutputKey==`SubnetsPrivate`].OutputValue' \\  --output text) aws ec2 create-tags \\  --resources $(echo $SUBNET_IDS | tr ',' '\\n') \\  --tags Key=\"kubernetes.io/cluster/${CLUSTER_NAME}\",Value= Create the KarpenterNode IAM Role Instances launched by Karpenter must run with an InstanceProfile that grants permissions necessary to run containers and configure networking. Karpenter discovers the InstanceProfile using the name KarpenterNodeRole-${ClusterName}.\nFirst, create the IAM resources using AWS CloudFormation.\nTEMPOUT=$(mktemp) curl -fsSL https://karpenter.sh/docs/getting-started/cloudformation.yaml \u003e $TEMPOUT \\ \u0026\u0026 aws cloudformation deploy \\  --stack-name Karpenter-${CLUSTER_NAME} \\  --template-file ${TEMPOUT} \\  --capabilities CAPABILITY_NAMED_IAM \\  --parameter-overrides ClusterName=${CLUSTER_NAME} Second, grant access to instances using the profile to connect to the cluster. This command adds the Karpenter node role to your aws-auth configmap, allowing nodes with this role to connect to the cluster.\neksctl create iamidentitymapping \\  --username system:node:{{EC2PrivateDNSName}} \\  --cluster ${CLUSTER_NAME} \\  --arn arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeRole-${CLUSTER_NAME} \\  --group system:bootstrappers \\  --group system:nodes Now, Karpenter can launch new EC2 instances and those instances can connect to your cluster.\nCreate the KarpenterController IAM Role Karpenter requires permissions like launching instances. This will create an AWS IAM Role, Kubernetes service account, and associate them using IRSA.\neksctl create iamserviceaccount \\ --cluster $CLUSTER_NAME --name karpenter --namespace karpenter \\ --attach-policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/KarpenterControllerPolicy-$CLUSTER_NAME \\ --approve Create the EC2 Spot Service Linked Role This step is only necessary if this is the first time you’re using EC2 Spot in this account. More details are available here.\naws iam create-service-linked-role --aws-service-name spot.amazonaws.com # If the role has already been successfully created, you will see: # An error occurred (InvalidInput) when calling the CreateServiceLinkedRole operation: Service role name AWSServiceRoleForEC2Spot has been taken in this account, please try a different suffix. Install Karpenter Helm Chart Use helm to deploy Karpenter to the cluster.\nWe created a Kubernetes service account when we created the cluster using eksctl. Thus, we don’t need the helm chart to do that.\nhelm repo add karpenter https://charts.karpenter.sh helm repo update helm upgrade --install karpenter karpenter/karpenter --namespace karpenter \\  --create-namespace --set serviceAccount.create=false --version 0.5.4 \\  --set controller.clusterName=${CLUSTER_NAME} \\  --set controller.clusterEndpoint=$(aws eks describe-cluster --name ${CLUSTER_NAME} --query \"cluster.endpoint\" --output json) \\  --wait # for the defaulting webhook to install before creating a Provisioner Enable Debug Logging (optional) kubectl patch configmap config-logging -n karpenter --patch '{\"data\":{\"loglevel.controller\":\"debug\"}}' Provisioner A single Karpenter provisioner is capable of handling many different pod shapes. Karpenter makes scheduling and provisioning decisions based on pod attributes such as labels and affinity. In other words, Karpenter eliminates the need to manage many different node groups.\nCreate a default provisioner using the command below. This provisioner configures instances to connect to your cluster’s endpoint and discovers resources like subnets and security groups using the cluster’s name.\nThe ttlSecondsAfterEmpty value configures Karpenter to terminate empty nodes. This behavior can be disabled by leaving the value undefined.\nReview the provisioner CRD for more information. For example, ttlSecondsUntilExpired configures Karpenter to terminate nodes when a maximum age is reached.\nNote: This provisioner will create capacity as long as the sum of all created capacity is less than the specified limit.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\"] limits: resources: cpu: 1000 provider: instanceProfile: KarpenterNodeInstanceProfile-${CLUSTER_NAME} ttlSecondsAfterEmpty: 30 EOF First Use Karpenter is now active and ready to begin provisioning nodes. Create some pods using a deployment, and watch Karpenter provision nodes in response.\nAutomatic Node Provisioning This deployment uses the pause image and starts with zero replicas.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: inflate spec: replicas: 0 selector: matchLabels: app: inflate template: metadata: labels: app: inflate spec: terminationGracePeriodSeconds: 0 containers: - name: inflate image: public.ecr.aws/eks-distro/kubernetes/pause:3.2 resources: requests: cpu: 1 EOF kubectl scale deployment inflate --replicas 5 kubectl logs -f -n karpenter $(kubectl get pods -n karpenter -l karpenter=controller -o name) Automatic Node Termination Now, delete the deployment. After 30 seconds (ttlSecondsAfterEmpty), Karpenter should terminate the now empty nodes.\nkubectl delete deployment inflate kubectl logs -f -n karpenter $(kubectl get pods -n karpenter -l karpenter=controller -o name) Manual Node Termination If you delete a node with kubectl, Karpenter will gracefully cordon, drain, and shutdown the corresponding instance. Under the hood, Karpenter adds a finalizer to the node object, which blocks deletion until all pods are drained and the instance is terminated. Keep in mind, this only works for nodes provisioned by Karpenter.\nkubectl delete node $NODE_NAME Cleanup To avoid additional charges, remove the demo infrastructure from your AWS account.\nhelm uninstall karpenter --namespace karpenter eksctl delete iamserviceaccount --cluster ${CLUSTER_NAME} --name karpenter --namespace karpenter aws cloudformation delete-stack --stack-name Karpenter-${CLUSTER_NAME} aws ec2 describe-launch-templates \\  | jq -r \".LaunchTemplates[].LaunchTemplateName\" \\  | grep -i Karpenter-${CLUSTER_NAME} \\  | xargs -I{} aws ec2 delete-launch-template --launch-template-name {} eksctl delete cluster --name ${CLUSTER_NAME}  Next Steps: ","categories":"","description":"","excerpt":"Karpenter automatically provisions new nodes in response to …","ref":"/v0.5.2/getting-started/","tags":"","title":"Getting Started with Karpenter on AWS"},{"body":"Karpenter automatically provisions new nodes in response to unschedulable pods. Karpenter does this by observing events within the Kubernetes cluster, and then sending commands to the underlying cloud provider.\nIn this example, the cluster is running on Amazon Web Services (AWS) Elastic Kubernetes Service (EKS). Karpenter is designed to be cloud provider agnostic, but currently only supports AWS. Contributions are welcomed.\nThis guide should take less than 1 hour to complete, and cost less than $0.25. Follow the clean-up instructions to reduce any charges.\nInstall Karpenter is installed in clusters with a helm chart.\nKarpenter additionally requires IAM Roles for Service Accounts (IRSA). IRSA permits Karpenter (within the cluster) to make privileged requests to AWS (as the cloud provider).\nRequired Utilities Install these tools before proceeding:\n AWS CLI kubectl - the Kubernetes CLI eksctl - the CLI for AWS EKS helm - the package manager for Kubernetes  Login to the AWS CLI with a user that has sufficient privileges to create a cluster.\nEnvironment Variables After setting up the tools, set the following environment variables to store commonly used values.\nexport CLUSTER_NAME=$USER-karpenter-demo export AWS_DEFAULT_REGION=us-west-2 AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) Create a Cluster Create a cluster with eksctl. This example configuration file specifies a basic cluster with one initial node and sets up an IAM OIDC provider for the cluster to enable IAM roles for pods:\ncat \u003c\u003cEOF \u003e cluster.yaml --- apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} version: \"1.21\" managedNodeGroups: - instanceType: m5.large amiFamily: AmazonLinux2 name: ${CLUSTER_NAME}-ng desiredCapacity: 1 minSize: 1 maxSize: 10 iam: withOIDC: true EOF eksctl create cluster -f cluster.yaml This guide uses a managed node group to host Karpenter.\nKarpenter itself can run anywhere, including on self-managed node groups, managed node groups, or AWS Fargate.\nKarpenter will provision EC2 instances in your account.\nTag Subnets Karpenter discovers subnets tagged kubernetes.io/cluster/$CLUSTER_NAME. Add this tag to subnets associated configured for your cluster. Retrieve the subnet IDs and tag them with the cluster name.\nSUBNET_IDS=$(aws cloudformation describe-stacks \\  --stack-name eksctl-${CLUSTER_NAME}-cluster \\  --query 'Stacks[].Outputs[?OutputKey==`SubnetsPrivate`].OutputValue' \\  --output text) aws ec2 create-tags \\  --resources $(echo $SUBNET_IDS | tr ',' '\\n') \\  --tags Key=\"kubernetes.io/cluster/${CLUSTER_NAME}\",Value= Create the KarpenterNode IAM Role Instances launched by Karpenter must run with an InstanceProfile that grants permissions necessary to run containers and configure networking. Karpenter discovers the InstanceProfile using the name KarpenterNodeRole-${ClusterName}.\nFirst, create the IAM resources using AWS CloudFormation.\nTEMPOUT=$(mktemp) curl -fsSL https://karpenter.sh/docs/getting-started/cloudformation.yaml \u003e $TEMPOUT \\ \u0026\u0026 aws cloudformation deploy \\  --stack-name Karpenter-${CLUSTER_NAME} \\  --template-file ${TEMPOUT} \\  --capabilities CAPABILITY_NAMED_IAM \\  --parameter-overrides ClusterName=${CLUSTER_NAME} Second, grant access to instances using the profile to connect to the cluster. This command adds the Karpenter node role to your aws-auth configmap, allowing nodes with this role to connect to the cluster.\neksctl create iamidentitymapping \\  --username system:node:{{EC2PrivateDNSName}} \\  --cluster ${CLUSTER_NAME} \\  --arn arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeRole-${CLUSTER_NAME} \\  --group system:bootstrappers \\  --group system:nodes Now, Karpenter can launch new EC2 instances and those instances can connect to your cluster.\nCreate the KarpenterController IAM Role Karpenter requires permissions like launching instances. This will create an AWS IAM Role, Kubernetes service account, and associate them using IRSA.\neksctl create iamserviceaccount \\ --cluster $CLUSTER_NAME --name karpenter --namespace karpenter \\ --attach-policy-arn arn:aws:iam::$AWS_ACCOUNT_ID:policy/KarpenterControllerPolicy-$CLUSTER_NAME \\ --approve Create the EC2 Spot Service Linked Role This step is only necessary if this is the first time you’re using EC2 Spot in this account. More details are available here.\naws iam create-service-linked-role --aws-service-name spot.amazonaws.com # If the role has already been successfully created, you will see: # An error occurred (InvalidInput) when calling the CreateServiceLinkedRole operation: Service role name AWSServiceRoleForEC2Spot has been taken in this account, please try a different suffix. Install Karpenter Helm Chart Use helm to deploy Karpenter to the cluster.\nWe created a Kubernetes service account when we created the cluster using eksctl. Thus, we don’t need the helm chart to do that.\nhelm repo add karpenter https://charts.karpenter.sh helm repo update helm upgrade --install karpenter karpenter/karpenter --namespace karpenter \\  --create-namespace --set serviceAccount.create=false --version 0.5.4 \\  --set controller.clusterName=${CLUSTER_NAME} \\  --set controller.clusterEndpoint=$(aws eks describe-cluster --name ${CLUSTER_NAME} --query \"cluster.endpoint\" --output json) \\  --wait # for the defaulting webhook to install before creating a Provisioner Enable Debug Logging (optional) kubectl patch configmap config-logging -n karpenter --patch '{\"data\":{\"loglevel.controller\":\"debug\"}}' Create Grafana dashboards (optional) The Karpenter repo contains multiple importable dashboards for an existing Grafana instance. See the Grafana documentation for instructions to import a dashboard.\nDeploy a temporary Prometheus and Grafana stack (optional) The following commands will deploy a Prometheus and Grafana stack that is suitable for this guide but does not include persistent storage or other configurations that would be necessary for monitoring a production deployment of Karpenter.\nhelm repo add grafana-charts https://grafana.github.io/helm-charts helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update kubectl create namespace monitoring curl -fsSL https://karpenter.sh/docs/getting-started/prometheus-values.yaml | tee prometheus-values.yaml helm install --namespace monitoring prometheus prometheus-community/prometheus --values prometheus-values.yaml curl -fsSL https://karpenter.sh/docs/getting-started/grafana-values.yaml | tee grafana-values.yaml helm install --namespace monitoring grafana grafana-charts/grafana --values grafana-values.yaml The Grafana instance may be accessed using port forwarding.\nkubectl port-forward --namespace monitoring svc/grafana 3000:80 The new stack has only one user, admin, and the password is stored in a secret. The following command will retrieve the password.\nkubectl get secret --namespace monitoring grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode Provisioner A single Karpenter provisioner is capable of handling many different pod shapes. Karpenter makes scheduling and provisioning decisions based on pod attributes such as labels and affinity. In other words, Karpenter eliminates the need to manage many different node groups.\nCreate a default provisioner using the command below. This provisioner configures instances to connect to your cluster’s endpoint and discovers resources like subnets and security groups using the cluster’s name.\nThe ttlSecondsAfterEmpty value configures Karpenter to terminate empty nodes. This behavior can be disabled by leaving the value undefined.\nReview the provisioner CRD for more information. For example, ttlSecondsUntilExpired configures Karpenter to terminate nodes when a maximum age is reached.\nNote: This provisioner will create capacity as long as the sum of all created capacity is less than the specified limit.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\"] limits: resources: cpu: 1000 provider: instanceProfile: KarpenterNodeInstanceProfile-${CLUSTER_NAME} ttlSecondsAfterEmpty: 30 EOF First Use Karpenter is now active and ready to begin provisioning nodes. Create some pods using a deployment, and watch Karpenter provision nodes in response.\nAutomatic Node Provisioning This deployment uses the pause image and starts with zero replicas.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: inflate spec: replicas: 0 selector: matchLabels: app: inflate template: metadata: labels: app: inflate spec: terminationGracePeriodSeconds: 0 containers: - name: inflate image: public.ecr.aws/eks-distro/kubernetes/pause:3.2 resources: requests: cpu: 1 EOF kubectl scale deployment inflate --replicas 5 kubectl logs -f -n karpenter $(kubectl get pods -n karpenter -l karpenter=controller -o name) Automatic Node Termination Now, delete the deployment. After 30 seconds (ttlSecondsAfterEmpty), Karpenter should terminate the now empty nodes.\nkubectl delete deployment inflate kubectl logs -f -n karpenter $(kubectl get pods -n karpenter -l karpenter=controller -o name) Manual Node Termination If you delete a node with kubectl, Karpenter will gracefully cordon, drain, and shutdown the corresponding instance. Under the hood, Karpenter adds a finalizer to the node object, which blocks deletion until all pods are drained and the instance is terminated. Keep in mind, this only works for nodes provisioned by Karpenter.\nkubectl delete node $NODE_NAME Cleanup To avoid additional charges, remove the demo infrastructure from your AWS account.\nhelm uninstall karpenter --namespace karpenter eksctl delete iamserviceaccount --cluster ${CLUSTER_NAME} --name karpenter --namespace karpenter aws cloudformation delete-stack --stack-name Karpenter-${CLUSTER_NAME} aws ec2 describe-launch-templates \\  | jq -r \".LaunchTemplates[].LaunchTemplateName\" \\  | grep -i Karpenter-${CLUSTER_NAME} \\  | xargs -I{} aws ec2 delete-launch-template --launch-template-name {} eksctl delete cluster --name ${CLUSTER_NAME}  Next Steps: ","categories":"","description":"","excerpt":"Karpenter automatically provisions new nodes in response to …","ref":"/v0.5.3/getting-started/","tags":"","title":"Getting Started with Karpenter on AWS"},{"body":"In this example, the cluster will be running on Amazon Web Services (AWS) managed by kOps. Karpenter is designed to be cloud provider agnostic, but currently only supports AWS. Contributions are welcomed\nKarpenter is supported on kOps as of 1.24.0-alpha.2, but sits behind a feature flag as the interface between kOps and Karpenter is still work in progress and is likely to change significantly. This guide is intended for users that wants to test Karpenter on kOps and provide feedback to Karpenter and kOps developers. Read more about how Karpenter works on kOps and the current limitations in the kOPs Karpenter documentation.\nThis guide should take less than 1 hour to complete, and cost less than $0.25. Follow the clean-up instructions to reduce any charges.\nThis guide assumes you already have a kOps state store and a hosted zone. If you do not have one, run through the kOps getting started on AWS documentation up until “Creating your first cluster”.\nInstall Karpenter is installed in clusters as a managed addon. kOps will automatically create and manage the necessary the IAM roles and policies Karpenter needs.\nRequired Utilities Install these tools before proceeding:\n kubectl - the Kubernetes CLI kops - kubectl, but for clusters v1.24.0 or later  Environment Variables After setting up the tools, set the following environment variables used by kOps.\nexport KOPS_FEATURE_FLAGS=Karpenter export CLUSTER_NAME=${USER}-karpenter-demo.example.com export ZONES=us-west-2a export KOPS_STATE_STORE=s3://prefix-example-com-state-store export KOPS_OIDC_STORE=s3://prefix-example-com-oidc-store/discovery Create a Cluster kOps installs Karpenter on the control plane. Once the control plane is running, Karpenter will provision the the worker nodes needed for non-Control Plane Deployments such as CoreDNS and CSI drivers.\nThe following command will launch a cluster with Karpenter-managed worker nodes:\nkops create cluster \\  --zones=$ZONES \\  --discovery-store=${KOPS_OIDC_STORE} \\  --instance-manager=karpenter \\  --networking=amazonvpc \\  ${CLUSTER_NAME} \\  --yes Note: we are using AWS VPC CNI for networking as Karpenter’s binpacking logic assumes ENI-based networking.\nProvisioner A single Karpenter provisioner is capable of handling many different pod shapes. Karpenter makes scheduling and provisioning decisions based on pod attributes such as labels and affinity. In other words, Karpenter eliminates the need to manage many different InstanceGroups.\nkOps manage provisioners through InstanceGroups. Your cluster will already have one Provisioner that will contain a suitable set of instance types for Karpenter to choose from.\nManaging Provisioner resources directly is possible, but not straight-forward. Read more about managing provisioners in the kOPs Karpenter documentation\nFirst Use Karpenter is now active and ready to begin provisioning nodes. As mentioned above, you should already have some Karpenter-managed nodes in your cluster used by other kOps addons. Create additional pods using a Deployment, and watch Karpenter provision nodes in response.\nAutomatic Node Provisioning This deployment uses the pause image and starts with zero replicas.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: inflate spec: replicas: 0 selector: matchLabels: app: inflate template: metadata: labels: app: inflate spec: terminationGracePeriodSeconds: 0 containers: - name: inflate image: public.ecr.aws/eks-distro/kubernetes/pause:3.2 resources: requests: cpu: 1 EOF kubectl scale deployment inflate --replicas 5 kubectl logs -f -n kube-system -l karpenter=controller Automatic Node Termination Now, delete the deployment. After 30 seconds, Karpenter should terminate the now empty nodes.\nkubectl delete deployment inflate kubectl logs -f -n karpenter -l karpenter=controller Manual Node Termination If you delete a node with kubectl, Karpenter will gracefully cordon, drain, and shutdown the corresponding instance. Under the hood, Karpenter adds a finalizer to the node object, which blocks deletion until all pods are drained and the instance is terminated. Keep in mind, this only works for nodes provisioned by Karpenter.\nkubectl delete node $NODE_NAME This is similar to kops delete instance $NODE_NAME except for that kOps will not respect karpenter.sh/do-not-evict.\nUpgrading a Cluster kOps is aware of nodes managed by Karpenter and will handle rolling upgrades of those nodes the same way as any other node:\nkops upgrade cluster --yes kops update cluster --yes kops rolling-update cluster --yes Karpenter-managed InstanceGroups supports setting maxUnavailable, but since Karpenter instances do not run in an Auto Scaling Group, setting maxSurge will not have any effect.\nDuring rolling updates, karpenter.sh/do-not-evict is not respected.\nCleanup To avoid additional charges, remove the demo infrastructure from your AWS account.\nkops delete cluster $CLUSTER_NAME --yes ","categories":"","description":"","excerpt":"In this example, the cluster will be running on Amazon Web Services …","ref":"/preview/getting-started-with-kops/","tags":"","title":"Getting Started with kOps"},{"body":"In this example, the cluster will be running on Amazon Web Services (AWS) managed by kOps. Karpenter is designed to be cloud provider agnostic, but currently only supports AWS. Contributions are welcomed\nKarpenter is supported on kOps as of 1.24.0-alpha.2, but sits behind a feature flag as the interface between kOps and Karpenter is still work in progress and is likely to change significantly. This guide is intended for users that wants to test Karpenter on kOps and provide feedback to Karpenter and kOps developers. Read more about how Karpenter works on kOps and the current limitations in the kOPs Karpenter documentation.\nThis guide should take less than 1 hour to complete, and cost less than $0.25. Follow the clean-up instructions to reduce any charges.\nThis guide assumes you already have a kOps state store and a hosted zone. If you do not have one, run through the kOps getting started on AWS documentation up until “Creating your first cluster”.\nInstall Karpenter is installed in clusters as a managed addon. kOps will automatically create and manage the necessary the IAM roles and policies Karpenter needs.\nRequired Utilities Install these tools before proceeding:\n kubectl - the Kubernetes CLI kops - kubectl, but for clusters v1.24.0 or later  Environment Variables After setting up the tools, set the following environment variables used by kOps.\nexport KOPS_FEATURE_FLAGS=Karpenter export CLUSTER_NAME=${USER}-karpenter-demo.example.com export ZONES=us-west-2a export KOPS_STATE_STORE=s3://prefix-example-com-state-store export KOPS_OIDC_STORE=s3://prefix-example-com-oidc-store/discovery Create a Cluster kOps installs Karpenter on the control plane. Once the control plane is running, Karpenter will provision the the worker nodes needed for non-Control Plane Deployments such as CoreDNS and CSI drivers.\nThe following command will launch a cluster with Karpenter-managed worker nodes:\nkops create cluster \\  --zones=$ZONES \\  --discovery-store=${KOPS_OIDC_STORE} \\  --instance-manager=karpenter \\  --networking=amazonvpc \\  ${CLUSTER_NAME} \\  --yes Note: we are using AWS VPC CNI for networking as Karpenter’s binpacking logic assumes ENI-based networking.\nProvisioner A single Karpenter provisioner is capable of handling many different pod shapes. Karpenter makes scheduling and provisioning decisions based on pod attributes such as labels and affinity. In other words, Karpenter eliminates the need to manage many different InstanceGroups.\nkOps manage provisioners through InstanceGroups. Your cluster will already have one Provisioner that will contain a suitable set of instance types for Karpenter to choose from.\nManaging Provisioner resources directly is possible, but not straight-forward. Read more about managing provisioners in the kOPs Karpenter documentation\nFirst Use Karpenter is now active and ready to begin provisioning nodes. As mentioned above, you should already have some Karpenter-managed nodes in your cluster used by other kOps addons. Create additional pods using a Deployment, and watch Karpenter provision nodes in response.\nAutomatic Node Provisioning This deployment uses the pause image and starts with zero replicas.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: inflate spec: replicas: 0 selector: matchLabels: app: inflate template: metadata: labels: app: inflate spec: terminationGracePeriodSeconds: 0 containers: - name: inflate image: public.ecr.aws/eks-distro/kubernetes/pause:3.2 resources: requests: cpu: 1 EOF kubectl scale deployment inflate --replicas 5 kubectl logs -f -n kube-system -l karpenter=controller Automatic Node Termination Now, delete the deployment. After 30 seconds, Karpenter should terminate the now empty nodes.\nkubectl delete deployment inflate kubectl logs -f -n karpenter -l karpenter=controller Manual Node Termination If you delete a node with kubectl, Karpenter will gracefully cordon, drain, and shutdown the corresponding instance. Under the hood, Karpenter adds a finalizer to the node object, which blocks deletion until all pods are drained and the instance is terminated. Keep in mind, this only works for nodes provisioned by Karpenter.\nkubectl delete node $NODE_NAME This is similar to kops delete instance $NODE_NAME except for that kOps will not respect karpenter.sh/do-not-evict.\nUpgrading a Cluster kOps is aware of nodes managed by Karpenter and will handle rolling upgrades of those nodes the same way as any other node:\nkops upgrade cluster --yes kops update cluster --yes kops rolling-update cluster --yes Karpenter-managed InstanceGroups supports setting maxUnavailable, but since Karpenter instances do not run in an Auto Scaling Group, setting maxSurge will not have any effect.\nDuring rolling updates, karpenter.sh/do-not-evict is not respected.\nCleanup To avoid additional charges, remove the demo infrastructure from your AWS account.\nkops delete cluster $CLUSTER_NAME --yes ","categories":"","description":"","excerpt":"In this example, the cluster will be running on Amazon Web Services …","ref":"/v0.5.3/getting-started-with-kops/","tags":"","title":"Getting Started with kOps"},{"body":"Karpenter automatically provisions new nodes in response to unschedulable pods. Karpenter does this by observing events within the Kubernetes cluster, and then sending commands to the underlying cloud provider.\nIn this example, the cluster is running on Amazon Web Services (AWS) Elastic Kubernetes Service (EKS). Karpenter is designed to be cloud provider agnostic, but currently only supports AWS. Contributions are welcomed.\nThis guide should take less than 1 hour to complete, and cost less than $0.25. Follow the clean-up instructions to reduce any charges.\nInstall Karpenter is installed in clusters with a helm chart.\nKarpenter additionally requires IAM Roles for Service Accounts (IRSA). IRSA permits Karpenter (within the cluster) to make privileged requests to AWS (as the cloud provider).\nRequired Utilities Install these tools before proceeding:\n AWS CLI kubectl - the Kubernetes CLI terraform - infrastructure-as-code tool made by HashiCorp helm - the package manager for Kubernetes  Login to the AWS CLI with a user that has sufficient privileges to create a cluster.\nSetting up Variables After setting up the tools, set the following environment variables to store commonly used values.\nexport CLUSTER_NAME=$USER-karpenter-demo export AWS_DEFAULT_REGION=us-west-2 The first thing we need to do is create our main.tf file and place the following in it. This will let us pass in a cluster name that will be used throughout the remainder of our config.\nvariable \"cluster_name\" { description = \"The name of the cluster\" type = string } Create a Cluster We’re going to use two different Terraform modules to create our cluster - one to create the VPC and another for the cluster itself. The key part of this is that we need to tag the VPC subnets that we want to use for the worker nodes.\nPlace the following Terraform config into your main.tf file.\nmodule \"vpc\" { source = \"terraform-aws-modules/vpc/aws\" name = var.cluster_name cidr = \"10.0.0.0/16\" azs = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"] private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"] public_subnets = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"] enable_nat_gateway = true single_nat_gateway = true one_nat_gateway_per_az = false private_subnet_tags = { \"kubernetes.io/cluster/${var.cluster_name}\" = \"owned\" } } module \"eks\" { source = \"terraform-aws-modules/eks/aws\" version = \"\u003c18\" cluster_version = \"1.21\" cluster_name = var.cluster_name vpc_id = module.vpc.vpc_id subnets = module.vpc.private_subnets enable_irsa = true# Only need one node to get Karpenter up and running worker_groups = [ { instance_type = \"t3a.medium\" asg_max_size = 1 } ] } At this point, go ahead and apply what we’ve done to create the VPC and cluster. This may take some time.\nterraform init terraform apply -var cluster_name=$CLUSTER_NAME There’s a good chance it will fail when trying to configure the aws-auth ConfigMap. And that’s because we need to use the kubeconfig file that was generated during the cluster install. To use it, run the following. This will configure both your local CLI and Terraform to use the file. Then try the apply again.\nexport KUBECONFIG=${PWD}/kubeconfig_${CLUSTER_NAME} export KUBE_CONFIG_PATH=$KUBECONFIG terraform apply -var cluster_name=$CLUSTER_NAME Everything should apply successfully now!\nConfigure the KarpenterNode IAM Role The EKS module creates an IAM role for worker nodes. We’ll use that for Karpenter (so we don’t have to reconfigure the aws-auth ConfigMap), but we need to add one more policy and create an instance profile.\nPlace the following into your main.tf to add the policy and create an instance profile.\ndata \"aws_iam_policy\" \"ssm_managed_instance\" { arn = \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\" } resource \"aws_iam_role_policy_attachment\" \"karpenter_ssm_policy\" { role = module.eks.worker_iam_role_name policy_arn = data.aws_iam_policy.ssm_managed_instance.arn } resource \"aws_iam_instance_profile\" \"karpenter\" { name = \"KarpenterNodeInstanceProfile-${var.cluster_name}\" role = module.eks.worker_iam_role_name } Go ahead and apply the changes.\nterraform apply -var cluster_name=$CLUSTER_NAME Now, Karpenter can use this instance profile to launch new EC2 instances and those instances will be able to connect to your cluster.\nCreate the KarpenterController IAM Role Karpenter requires permissions like launching instances, which means it needs an IAM role that grants it access. The config below will create an AWS IAM Role, attach a policy, and authorize the Service Account to assume the role using IRSA. We will create the ServiceAccount and connect it to this role during the Helm chart install.\nmodule \"iam_assumable_role_karpenter\" { source = \"terraform-aws-modules/iam/aws//modules/iam-assumable-role-with-oidc\" version = \"4.7.0\" create_role = true role_name = \"karpenter-controller-${var.cluster_name}\" provider_url = module.eks.cluster_oidc_issuer_url oidc_fully_qualified_subjects = [\"system:serviceaccount:karpenter:karpenter\"] } resource \"aws_iam_role_policy\" \"karpenter_controller\" { name = \"karpenter-policy-${var.cluster_name}\" role = module.iam_assumable_role_karpenter.iam_role_name policy = jsonencode({ Version = \"2012-10-17\" Statement = [ { Action = [ \"ec2:CreateLaunchTemplate\", \"ec2:CreateFleet\", \"ec2:RunInstances\", \"ec2:CreateTags\", \"iam:PassRole\", \"ec2:TerminateInstances\", \"ec2:DescribeLaunchTemplates\", \"ec2:DescribeInstances\", \"ec2:DescribeSecurityGroups\", \"ec2:DescribeSubnets\", \"ec2:DescribeInstanceTypes\", \"ec2:DescribeInstanceTypeOfferings\", \"ec2:DescribeAvailabilityZones\", \"ssm:GetParameter\" ] Effect = \"Allow\" Resource = \"*\" }, ] }) } Since we’ve added a new module, you’ll need to run terraform init again. Then, apply the changes.\nterraform init terraform apply -var cluster_name=$CLUSTER_NAME Install Karpenter Helm Chart Use helm to deploy Karpenter to the cluster. We are going to use the helm_release Terraform resource to do the deploy and pass in the cluster details and IAM role Karpenter needs to assume.\nresource \"helm_release\" \"karpenter\" { depends_on = [module.eks.kubeconfig] namespace = \"karpenter\" create_namespace = true name = \"karpenter\" repository = \"https://charts.karpenter.sh\" chart = \"karpenter\" version = \"0.5.4\" set { name = \"serviceAccount.annotations.eks\\\\.amazonaws\\\\.com/role-arn\" value = module.iam_assumable_role_karpenter.iam_role_arn } set { name = \"controller.clusterName\" value = var.cluster_name } set { name = \"controller.clusterEndpoint\" value = module.eks.cluster_endpoint } } Now, deploy Karpenter by applying the new Terraform config.\nterraform init terraform apply -var cluster_name=$CLUSTER_NAME Enable Debug Logging (optional) kubectl patch configmap config-logging -n karpenter --patch '{\"data\":{\"loglevel.controller\":\"debug\"}}' Provisioner A single Karpenter provisioner is capable of handling many different pod shapes. Karpenter makes scheduling and provisioning decisions based on pod attributes such as labels and affinity. In other words, Karpenter eliminates the need to manage many different node groups.\nCreate a default provisioner using the command below. This provisioner configures instances to connect to your cluster’s endpoint and discovers resources like subnets and security groups using the cluster’s name.\nThe ttlSecondsAfterEmpty value configures Karpenter to terminate empty nodes. This behavior can be disabled by leaving the value undefined.\nReview the provisioner CRD for more information. For example, ttlSecondsUntilExpired configures Karpenter to terminate nodes when a maximum age is reached.\nNote: This provisioner will create capacity as long as the sum of all created capacity is less than the specified limit.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\"] limits: resources: cpu: 1000 provider: instanceProfile: KarpenterNodeInstanceProfile-${CLUSTER_NAME} ttlSecondsAfterEmpty: 30 EOF First Use Karpenter is now active and ready to begin provisioning nodes. Create some pods using a deployment, and watch Karpenter provision nodes in response.\nAutomatic Node Provisioning This deployment uses the pause image and starts with zero replicas.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: inflate spec: replicas: 0 selector: matchLabels: app: inflate template: metadata: labels: app: inflate spec: terminationGracePeriodSeconds: 0 containers: - name: inflate image: public.ecr.aws/eks-distro/kubernetes/pause:3.2 resources: requests: cpu: 1 EOF kubectl scale deployment inflate --replicas 5 kubectl logs -f -n karpenter $(kubectl get pods -n karpenter -l karpenter=controller -o name) Automatic Node Termination Now, delete the deployment. After 30 seconds (ttlSecondsAfterEmpty), Karpenter should terminate the now empty nodes.\nkubectl delete deployment inflate kubectl logs -f -n karpenter $(kubectl get pods -n karpenter -l karpenter=controller -o name) Manual Node Termination If you delete a node with kubectl, Karpenter will gracefully cordon, drain, and shutdown the corresponding instance. Under the hood, Karpenter adds a finalizer to the node object, which blocks deletion until all pods are drained and the instance is terminated. Keep in mind, this only works for nodes provisioned by Karpenter.\nkubectl delete node $NODE_NAME Cleanup To avoid additional charges, remove the demo infrastructure from your AWS account. Since Karpenter is managing nodes outside of Terraform’s view, we need to remove the pods and node first (if you haven’t already). Once the node is removed, you can remove the rest of the infrastructure.\nkubectl delete deployment inflate kubectl delete node -l karpenter.sh/provisioner-name=default terraform destroy -var cluster_name=$CLUSTER_NAME ","categories":"","description":"","excerpt":"Karpenter automatically provisions new nodes in response to …","ref":"/preview/getting-started-with-terraform/","tags":"","title":"Getting Started with Terraform"},{"body":"Karpenter automatically provisions new nodes in response to unschedulable pods. Karpenter does this by observing events within the Kubernetes cluster, and then sending commands to the underlying cloud provider.\nIn this example, the cluster is running on Amazon Web Services (AWS) Elastic Kubernetes Service (EKS). Karpenter is designed to be cloud provider agnostic, but currently only supports AWS. Contributions are welcomed.\nThis guide should take less than 1 hour to complete, and cost less than $0.25. Follow the clean-up instructions to reduce any charges.\nInstall Karpenter is installed in clusters with a helm chart.\nKarpenter additionally requires IAM Roles for Service Accounts (IRSA). IRSA permits Karpenter (within the cluster) to make privileged requests to AWS (as the cloud provider).\nRequired Utilities Install these tools before proceeding:\n AWS CLI kubectl - the Kubernetes CLI terraform - infrastructure-as-code tool made by HashiCorp helm - the package manager for Kubernetes  Login to the AWS CLI with a user that has sufficient privileges to create a cluster.\nSetting up Variables After setting up the tools, set the following environment variables to store commonly used values.\nexport CLUSTER_NAME=$USER-karpenter-demo export AWS_DEFAULT_REGION=us-west-2 The first thing we need to do is create our main.tf file and place the following in it. This will let us pass in a cluster name that will be used throughout the remainder of our config.\nvariable \"cluster_name\" { description = \"The name of the cluster\" type = string } Create a Cluster We’re going to use two different Terraform modules to create our cluster - one to create the VPC and another for the cluster itself. The key part of this is that we need to tag the VPC subnets that we want to use for the worker nodes.\nPlace the following Terraform config into your main.tf file.\nmodule \"vpc\" { source = \"terraform-aws-modules/vpc/aws\" name = var.cluster_name cidr = \"10.0.0.0/16\" azs = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"] private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"] public_subnets = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"] enable_nat_gateway = true single_nat_gateway = true one_nat_gateway_per_az = false private_subnet_tags = { \"kubernetes.io/cluster/${var.cluster_name}\" = \"owned\" } } module \"eks\" { source = \"terraform-aws-modules/eks/aws\" version = \"\u003c18\" cluster_version = \"1.21\" cluster_name = var.cluster_name vpc_id = module.vpc.vpc_id subnets = module.vpc.private_subnets enable_irsa = true# Only need one node to get Karpenter up and running worker_groups = [ { instance_type = \"t3a.medium\" asg_max_size = 1 } ] } At this point, go ahead and apply what we’ve done to create the VPC and cluster. This may take some time.\nterraform init terraform apply -var cluster_name=$CLUSTER_NAME There’s a good chance it will fail when trying to configure the aws-auth ConfigMap. And that’s because we need to use the kubeconfig file that was generated during the cluster install. To use it, run the following. This will configure both your local CLI and Terraform to use the file. Then try the apply again.\nexport KUBECONFIG=${PWD}/kubeconfig_${CLUSTER_NAME} export KUBE_CONFIG_PATH=$KUBECONFIG terraform apply -var cluster_name=$CLUSTER_NAME Everything should apply successfully now!\nConfigure the KarpenterNode IAM Role The EKS module creates an IAM role for worker nodes. We’ll use that for Karpenter (so we don’t have to reconfigure the aws-auth ConfigMap), but we need to add one more policy and create an instance profile.\nPlace the following into your main.tf to add the policy and create an instance profile.\ndata \"aws_iam_policy\" \"ssm_managed_instance\" { arn = \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\" } resource \"aws_iam_role_policy_attachment\" \"karpenter_ssm_policy\" { role = module.eks.worker_iam_role_name policy_arn = data.aws_iam_policy.ssm_managed_instance.arn } resource \"aws_iam_instance_profile\" \"karpenter\" { name = \"KarpenterNodeInstanceProfile-${var.cluster_name}\" role = module.eks.worker_iam_role_name } Go ahead and apply the changes.\nterraform apply -var cluster_name=$CLUSTER_NAME Now, Karpenter can use this instance profile to launch new EC2 instances and those instances will be able to connect to your cluster.\nCreate the KarpenterController IAM Role Karpenter requires permissions like launching instances, which means it needs an IAM role that grants it access. The config below will create an AWS IAM Role, attach a policy, and authorize the Service Account to assume the role using IRSA. We will create the ServiceAccount and connect it to this role during the Helm chart install.\nmodule \"iam_assumable_role_karpenter\" { source = \"terraform-aws-modules/iam/aws//modules/iam-assumable-role-with-oidc\" version = \"4.7.0\" create_role = true role_name = \"karpenter-controller-${var.cluster_name}\" provider_url = module.eks.cluster_oidc_issuer_url oidc_fully_qualified_subjects = [\"system:serviceaccount:karpenter:karpenter\"] } resource \"aws_iam_role_policy\" \"karpenter_controller\" { name = \"karpenter-policy-${var.cluster_name}\" role = module.iam_assumable_role_karpenter.iam_role_name policy = jsonencode({ Version = \"2012-10-17\" Statement = [ { Action = [ \"ec2:CreateLaunchTemplate\", \"ec2:CreateFleet\", \"ec2:RunInstances\", \"ec2:CreateTags\", \"iam:PassRole\", \"ec2:TerminateInstances\", \"ec2:DescribeLaunchTemplates\", \"ec2:DescribeInstances\", \"ec2:DescribeSecurityGroups\", \"ec2:DescribeSubnets\", \"ec2:DescribeInstanceTypes\", \"ec2:DescribeInstanceTypeOfferings\", \"ec2:DescribeAvailabilityZones\", \"ssm:GetParameter\" ] Effect = \"Allow\" Resource = \"*\" }, ] }) } Since we’ve added a new module, you’ll need to run terraform init again. Then, apply the changes.\nterraform init terraform apply -var cluster_name=$CLUSTER_NAME Install Karpenter Helm Chart Use helm to deploy Karpenter to the cluster. We are going to use the helm_release Terraform resource to do the deploy and pass in the cluster details and IAM role Karpenter needs to assume.\nresource \"helm_release\" \"karpenter\" { depends_on = [module.eks.kubeconfig] namespace = \"karpenter\" create_namespace = true name = \"karpenter\" repository = \"https://charts.karpenter.sh\" chart = \"karpenter\" version = \"0.5.4\" set { name = \"serviceAccount.annotations.eks\\\\.amazonaws\\\\.com/role-arn\" value = module.iam_assumable_role_karpenter.iam_role_arn } set { name = \"controller.clusterName\" value = var.cluster_name } set { name = \"controller.clusterEndpoint\" value = module.eks.cluster_endpoint } } Now, deploy Karpenter by applying the new Terraform config.\nterraform init terraform apply -var cluster_name=$CLUSTER_NAME Enable Debug Logging (optional) kubectl patch configmap config-logging -n karpenter --patch '{\"data\":{\"loglevel.controller\":\"debug\"}}' Provisioner A single Karpenter provisioner is capable of handling many different pod shapes. Karpenter makes scheduling and provisioning decisions based on pod attributes such as labels and affinity. In other words, Karpenter eliminates the need to manage many different node groups.\nCreate a default provisioner using the command below. This provisioner configures instances to connect to your cluster’s endpoint and discovers resources like subnets and security groups using the cluster’s name.\nThe ttlSecondsAfterEmpty value configures Karpenter to terminate empty nodes. This behavior can be disabled by leaving the value undefined.\nReview the provisioner CRD for more information. For example, ttlSecondsUntilExpired configures Karpenter to terminate nodes when a maximum age is reached.\nNote: This provisioner will create capacity as long as the sum of all created capacity is less than the specified limit.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\"] limits: resources: cpu: 1000 provider: instanceProfile: KarpenterNodeInstanceProfile-${CLUSTER_NAME} ttlSecondsAfterEmpty: 30 EOF First Use Karpenter is now active and ready to begin provisioning nodes. Create some pods using a deployment, and watch Karpenter provision nodes in response.\nAutomatic Node Provisioning This deployment uses the pause image and starts with zero replicas.\ncat \u003c\u003cEOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: inflate spec: replicas: 0 selector: matchLabels: app: inflate template: metadata: labels: app: inflate spec: terminationGracePeriodSeconds: 0 containers: - name: inflate image: public.ecr.aws/eks-distro/kubernetes/pause:3.2 resources: requests: cpu: 1 EOF kubectl scale deployment inflate --replicas 5 kubectl logs -f -n karpenter $(kubectl get pods -n karpenter -l karpenter=controller -o name) Automatic Node Termination Now, delete the deployment. After 30 seconds (ttlSecondsAfterEmpty), Karpenter should terminate the now empty nodes.\nkubectl delete deployment inflate kubectl logs -f -n karpenter $(kubectl get pods -n karpenter -l karpenter=controller -o name) Manual Node Termination If you delete a node with kubectl, Karpenter will gracefully cordon, drain, and shutdown the corresponding instance. Under the hood, Karpenter adds a finalizer to the node object, which blocks deletion until all pods are drained and the instance is terminated. Keep in mind, this only works for nodes provisioned by Karpenter.\nkubectl delete node $NODE_NAME Cleanup To avoid additional charges, remove the demo infrastructure from your AWS account. Since Karpenter is managing nodes outside of Terraform’s view, we need to remove the pods and node first (if you haven’t already). Once the node is removed, you can remove the rest of the infrastructure.\nkubectl delete deployment inflate kubectl delete node -l karpenter.sh/provisioner-name=default terraform destroy -var cluster_name=$CLUSTER_NAME ","categories":"","description":"","excerpt":"Karpenter automatically provisions new nodes in response to …","ref":"/v0.5.3/getting-started-with-terraform/","tags":"","title":"Getting Started with Terraform"},{"body":"spec.provider This section covers parameters of the AWS Cloud Provider.\nReview these fields in the code.\nInstanceProfile An InstanceProfile is a way to pass a single IAM role to an EC2 instance.\nIt is required, and specified by name. A suitable KarpenterNodeRole is created in the getting started guide.\nspec:\rprovider:\rinstanceProfile: MyInstanceProfile\rLaunchTemplate A launch template is a set of configuration values sufficient for launching an EC2 instance (e.g., AMI, storage spec).\nA custom launch template is specified by name. If none is specified, Karpenter will automatically create a launch template.\nReview the Launch Template documentation to learn how to create a custom one.\nspec:\rprovider:\rlaunchTemplate: MyLaunchTemplate\rSubnetSelector Karpenter discovers subnets using AWS tags.\nSubnets may be specified by any AWS tag, including Name. Selecting tag values using wildcards (\"*\") is supported.\nWhen launching nodes, Karpenter automatically chooses a subnet that matches the desired zone. If multiple subnets exist for a zone, one is chosen randomly.\nExamples\nSelect all subnets with a specified tag:\n subnetSelector:\rkubernetes.io/cluster/MyCluster: '*'\rSelect subnets by name:\n subnetSelector:\rName: subnet-0fcd7006b3754e95e\rSelect subnets by an arbitrary AWS tag key/value pair:\n subnetSelector:\rMySubnetTag: value\rSelect subnets using wildcards:\n subnetSelector:\rName: *public*\rSecurityGroupSelector The security group of an instance is comparable to a set of firewall rules. If no security groups are explicitly listed, Karpenter discovers them using the tag “kubernetes.io/cluster/MyClusterName”, similar to subnet discovery.\nEKS creates at least two security groups by default, review the documentation for more info.\nSecurity groups may be specified by any AWS tag, including “name”. Selecting tags using wildcards (\"*\") is supported.\n‼️ When launching nodes, Karpenter uses all of the security groups that match the selector. If multiple security groups with the tag kubernetes.io/cluster/MyClusterName match the selector, this may result in failures using the AWS Load Balancer controller. The Load Balancer controller only supports a single security group having that tag key. See this issue for more details.\nTo verify if this restriction affects you, run the following commands.\nCLUSTER_VPC_ID=\"$(aws eks describe-cluster --name $CLUSTER_NAME --query cluster.resourcesVpcConfig.vpcId --output text)\" aws ec2 describe-security-groups --filters Name=vpc-id,Values=$CLUSTER_VPC_ID Name=tag-key,Values=kubernetes.io/cluster/$CLUSTER_NAME --query SecurityGroups[].[GroupName] --output text If multiple securityGroups are printed, you will need a more targeted securityGroupSelector.\nExamples\nSelect all security groups with a specified tag:\nspec:\rprovider:\rsecurityGroupSelector:\rkubernetes.io/cluster/MyKarpenterSecurityGroups: '*'\rSelect security groups by name, or another tag (all criteria must match):\n securityGroupSelector:\rName: sg-01077157b7cf4f5a8\rMySecurityTag: '' # matches all resources with the tag\rSelect security groups by name using a wildcard:\n securityGroupSelector:\rName: *public*\rTags Tags will be added to every EC2 Instance launched by this provisioner.\nspec:\rprovider:\rtags:\rInternalAccountingTag: 1234\rdev.corp.net/app: Calculator\rdev.corp.net/team: MyTeam\rNote: Karpenter will set the default AWS tags listed below, but these can be overridden in the tags section above.\nName: karpenter.sh/cluster/\u003ccluster-name\u003e/provisioner/\u003cprovisioner-name\u003e\rkarpenter.sh/cluster/\u003ccluster-name\u003e: owned\rkubernetes.io/cluster/\u003ccluster-name\u003e: owned\rOther Resources Accelerators, GPU Accelerator (e.g., GPU) values include\n nvidia.com/gpu amd.com/gpu aws.amazon.com/neuron  Karpenter supports accelerators, such as GPUs.\nAdditionally, include a resource requirement in the workload manifest. This will cause the GPU dependent pod will be scheduled onto the appropriate node.\nAccelerator resource in workload manifest (e.g., pod)\nspec:template:spec:containers:- resources:limits:nvidia.com/gpu:\"1\"","categories":"","description":"","excerpt":"spec.provider This section covers parameters of the AWS Cloud …","ref":"/preview/aws/provisioning/","tags":"","title":"Provisioning Configuration"},{"body":"spec.provider This section covers parameters of the AWS Cloud Provider.\nReview these fields in the code.\nInstanceProfile An InstanceProfile is a way to pass a single IAM role to an EC2 instance.\nIt is required, and specified by name. A suitable KarpenterNodeRole is created in the getting started guide.\nspec:\rprovider:\rinstanceProfile: MyInstanceProfile\rLaunchTemplate A launch template is a set of configuration values sufficient for launching an EC2 instance (e.g., AMI, storage spec).\nA custom launch template is specified by name. If none is specified, Karpenter will automatically create a launch template.\nReview the Launch Template documentation to learn how to create a custom one.\nspec:\rprovider:\rlaunchTemplate: MyLaunchTemplate\rSubnetSelector Karpenter discovers subnets using AWS tags.\nSubnets may be specified by any AWS tag, including Name. Selecting tag values using wildcards (\"*\") is supported.\nWhen launching nodes, Karpenter automatically chooses a subnet that matches the desired zone. If multiple subnets exist for a zone, one is chosen randomly.\nExamples\nSelect all subnets with a specified tag:\n subnetSelector:\rkubernetes.io/cluster/MyCluster: '*'\rSelect subnets by name:\n subnetSelector:\rName: subnet-0fcd7006b3754e95e\rSelect subnets by an arbitrary AWS tag key/value pair:\n subnetSelector:\rMySubnetTag: value\rSelect subnets using wildcards:\n subnetSelector:\rName: *public* SecurityGroupSelector The security group of an instance is comparable to a set of firewall rules. If no security groups are explicitly listed, Karpenter discovers them using the tag “kubernetes.io/cluster/MyClusterName”, similar to subnet discovery.\nEKS creates at least two security groups by default, review the documentation for more info.\nSecurity groups may be specified by any AWS tag, including “name”. Selecting tags using wildcards (\"*\") is supported.\n‼️ When launching nodes, Karpenter uses all of the security groups that match the selector. The only exception to this is security groups tagged with the label kubernetes.io/cluster/MyClusterName. The AWS Load Balancer controller requires that only a single security group with this tag may be attached to a node. In this case, Karpenter selects randomly.\nExamples\nSelect all security groups with a specified tag:\nspec:\rprovider:\rsecurityGroupSelector:\rkubernetes.io/cluster/MyKarpenterSecurityGroups: '*'\rSelect security groups by name, or another tag:\n securityGroupSelector:\rName: sg-01077157b7cf4f5a8\rMySecurityTag: '' # matches all resources with the tag\rSelect security groups by name using a wildcard:\n subnetSelector:\rName: *public*\rTags Tags will be added to every EC2 Instance launched by this provisioner.\nspec:\rprovider:\rtags:\rInternalAccountingTag: 1234\rdev.corp.net/app: Calculator\rdev.corp.net/team: MyTeam\rOther Resources Accelerators, GPU Accelerator (e.g., GPU) values include\n nvidia.com/gpu amd.com/gpu aws.amazon.com/neuron  Karpenter supports accelerators, such as GPUs.\nAdditionally, include a resource requirement in the workload manifest. This will cause the GPU dependent pod will be scheduled onto the appropriate node.\nAccelerator resource in workload manifest (e.g., pod)\nspec:template:spec:containers:- resources:limits:nvidia.com/gpu:\"1\"","categories":"","description":"","excerpt":"spec.provider This section covers parameters of the AWS Cloud …","ref":"/v0.5.0/aws/provisioning/","tags":"","title":"Provisioning Configuration"},{"body":"spec.provider This section covers parameters of the AWS Cloud Provider.\nReview these fields in the code.\nInstanceProfile An InstanceProfile is a way to pass a single IAM role to an EC2 instance.\nIt is required, and specified by name. A suitable KarpenterNodeRole is created in the getting started guide.\nspec:\rprovider:\rinstanceProfile: MyInstanceProfile\rLaunchTemplate A launch template is a set of configuration values sufficient for launching an EC2 instance (e.g., AMI, storage spec).\nA custom launch template is specified by name. If none is specified, Karpenter will automatically create a launch template.\nReview the Launch Template documentation to learn how to create a custom one.\nspec:\rprovider:\rlaunchTemplate: MyLaunchTemplate\rSubnetSelector Karpenter discovers subnets using AWS tags.\nSubnets may be specified by any AWS tag, including Name. Selecting tag values using wildcards (\"*\") is supported.\nWhen launching nodes, Karpenter automatically chooses a subnet that matches the desired zone. If multiple subnets exist for a zone, one is chosen randomly.\nExamples\nSelect all subnets with a specified tag:\n subnetSelector:\rkubernetes.io/cluster/MyCluster: '*'\rSelect subnets by name:\n subnetSelector:\rName: subnet-0fcd7006b3754e95e\rSelect subnets by an arbitrary AWS tag key/value pair:\n subnetSelector:\rMySubnetTag: value\rSelect subnets using wildcards:\n subnetSelector:\rName: *public* SecurityGroupSelector The security group of an instance is comparable to a set of firewall rules. If no security groups are explicitly listed, Karpenter discovers them using the tag “kubernetes.io/cluster/MyClusterName”, similar to subnet discovery.\nEKS creates at least two security groups by default, review the documentation for more info.\nSecurity groups may be specified by any AWS tag, including “name”. Selecting tags using wildcards (\"*\") is supported.\n‼️ When launching nodes, Karpenter uses all of the security groups that match the selector. The only exception to this is security groups tagged with the label kubernetes.io/cluster/MyClusterName. The AWS Load Balancer controller requires that only a single security group with this tag may be attached to a node. In this case, Karpenter selects randomly.\nExamples\nSelect all security groups with a specified tag:\nspec:\rprovider:\rsecurityGroupSelector:\rkubernetes.io/cluster/MyKarpenterSecurityGroups: '*'\rSelect security groups by name, or another tag:\n securityGroupSelector:\rName: sg-01077157b7cf4f5a8\rMySecurityTag: '' # matches all resources with the tag\rSelect security groups by name using a wildcard:\n subnetSelector:\rName: *public*\rTags Tags will be added to every EC2 Instance launched by this provisioner.\nspec:\rprovider:\rtags:\rInternalAccountingTag: 1234\rdev.corp.net/app: Calculator\rdev.corp.net/team: MyTeam\rOther Resources Accelerators, GPU Accelerator (e.g., GPU) values include\n nvidia.com/gpu amd.com/gpu aws.amazon.com/neuron  Karpenter supports accelerators, such as GPUs.\nAdditionally, include a resource requirement in the workload manifest. This will cause the GPU dependent pod will be scheduled onto the appropriate node.\nAccelerator resource in workload manifest (e.g., pod)\nspec:template:spec:containers:- resources:limits:nvidia.com/gpu:\"1\"","categories":"","description":"","excerpt":"spec.provider This section covers parameters of the AWS Cloud …","ref":"/v0.5.2/aws/provisioning/","tags":"","title":"Provisioning Configuration"},{"body":"spec.provider This section covers parameters of the AWS Cloud Provider.\nReview these fields in the code.\nInstanceProfile An InstanceProfile is a way to pass a single IAM role to an EC2 instance.\nIt is required, and specified by name. A suitable KarpenterNodeRole is created in the getting started guide.\nspec:\rprovider:\rinstanceProfile: MyInstanceProfile\rLaunchTemplate A launch template is a set of configuration values sufficient for launching an EC2 instance (e.g., AMI, storage spec).\nA custom launch template is specified by name. If none is specified, Karpenter will automatically create a launch template.\nReview the Launch Template documentation to learn how to create a custom one.\nspec:\rprovider:\rlaunchTemplate: MyLaunchTemplate\rSubnetSelector Karpenter discovers subnets using AWS tags.\nSubnets may be specified by any AWS tag, including Name. Selecting tag values using wildcards (\"*\") is supported.\nWhen launching nodes, Karpenter automatically chooses a subnet that matches the desired zone. If multiple subnets exist for a zone, one is chosen randomly.\nExamples\nSelect all subnets with a specified tag:\n subnetSelector:\rkubernetes.io/cluster/MyCluster: '*'\rSelect subnets by name:\n subnetSelector:\rName: subnet-0fcd7006b3754e95e\rSelect subnets by an arbitrary AWS tag key/value pair:\n subnetSelector:\rMySubnetTag: value\rSelect subnets using wildcards:\n subnetSelector:\rName: *public*\rSecurityGroupSelector The security group of an instance is comparable to a set of firewall rules. If no security groups are explicitly listed, Karpenter discovers them using the tag “kubernetes.io/cluster/MyClusterName”, similar to subnet discovery.\nEKS creates at least two security groups by default, review the documentation for more info.\nSecurity groups may be specified by any AWS tag, including “name”. Selecting tags using wildcards (\"*\") is supported.\n‼️ When launching nodes, Karpenter uses all of the security groups that match the selector. If multiple security groups with the tag kubernetes.io/cluster/MyClusterName match the selector, this may result in failures using the AWS Load Balancer controller. The Load Balancer controller only supports a single security group having that tag key. See this issue for more details.\nTo verify if this restriction affects you, run the following commands.\nCLUSTER_VPC_ID=\"$(aws eks describe-cluster --name $CLUSTER_NAME --query cluster.resourcesVpcConfig.vpcId --output text)\" aws ec2 describe-security-groups --filters Name=vpc-id,Values=$CLUSTER_VPC_ID Name=tag-key,Values=kubernetes.io/cluster/$CLUSTER_NAME --query SecurityGroups[].[GroupName] --output text If multiple securityGroups are printed, you will need a more targeted securityGroupSelector.\nExamples\nSelect all security groups with a specified tag:\nspec:\rprovider:\rsecurityGroupSelector:\rkubernetes.io/cluster/MyKarpenterSecurityGroups: '*'\rSelect security groups by name, or another tag:\n securityGroupSelector:\rName: sg-01077157b7cf4f5a8\rMySecurityTag: '' # matches all resources with the tag\rSelect security groups by name using a wildcard:\n securityGroupSelector:\rName: *public*\rTags Tags will be added to every EC2 Instance launched by this provisioner.\nspec:\rprovider:\rtags:\rInternalAccountingTag: 1234\rdev.corp.net/app: Calculator\rdev.corp.net/team: MyTeam\rNote: Karpenter will set the default AWS tags listed below, but these can be overridden in the tags section above.\nName: karpenter.sh/cluster/\u003ccluster-name\u003e/provisioner/\u003cprovisioner-name\u003e\rkarpenter.sh/cluster/\u003ccluster-name\u003e: owned\rkubernetes.io/cluster/\u003ccluster-name\u003e: owned\rOther Resources Accelerators, GPU Accelerator (e.g., GPU) values include\n nvidia.com/gpu amd.com/gpu aws.amazon.com/neuron  Karpenter supports accelerators, such as GPUs.\nAdditionally, include a resource requirement in the workload manifest. This will cause the GPU dependent pod will be scheduled onto the appropriate node.\nAccelerator resource in workload manifest (e.g., pod)\nspec:template:spec:containers:- resources:limits:nvidia.com/gpu:\"1\"","categories":"","description":"","excerpt":"spec.provider This section covers parameters of the AWS Cloud …","ref":"/v0.5.3/aws/provisioning/","tags":"","title":"Provisioning Configuration"},{"body":"If your pods have no requirements for how or where to run, you can let Karpenter choose nodes from the full range of available cloud provider resources. However, by taking advantage of Karpenter’s model of layered constraints, you can be sure that the precise type and amount of resources needed are available to your pods. Reasons for constraining where your pods run could include:\n Needing to run in zones where dependent applications or storage are available Requiring certain kinds of processors or other hardware Wanting to use techniques like topology spread to help insure high availability  Your Cloud Provider defines the first layer of constraints, including all instance types, architectures, zones, and purchase types available to its cloud. The cluster operator adds the next layer of constraints by creating one or more provisioners. The final layer comes from you adding specifications to your Kubernetes pod deployments. Pod scheduling constraints must fall within a provisioner’s constraints or the pods will not deploy. For example, if the provisioner sets limits that allow only a particular zone to be used, and a pod asks for a different zone, it will not be scheduled.\nConstraints you can request include:\n Resource requests: Request that certain amount of memory or CPU be available. Node selection: Choose to run on a node that is has a particular label (nodeSelector). Node affinity: Draws a pod to run on nodes with particular attributes (affinity). Topology spread: Use topology spread to help insure availability of the application.  Karpenter supports standard Kubernetes scheduling constraints. This allows you to define a single set of rules that apply to both existing and provisioned capacity. Pod affinity is a key exception to this rule.\nNote Karpenter supports specific Well-Known Labels, Annotations and Taints that are useful for scheduling.  Resource requests (resources) Within a Pod spec, you can both make requests and set limits on resources a pod needs, such as CPU and memory. For example:\napiVersion: v1 kind: Pod metadata: name: myapp spec: containers: - name: app image: myimage resources: requests: memory: \"128Mi\" cpu: \"500m\" limits: memory: \"256Mi\" cpu: \"1000m\" In this example, the container is requesting 128MiB of memory and .5 CPU. Its limits are set to 256MiB of memory and 1 CPU. Instance type selection math only uses requests, but limits may be configured to enable resource oversubscription.\nSee Managing Resources for Containers for details on resource types supported by Kubernetes, Specify a memory request and a memory limit for examples of memory requests, and Provisioning Configuration for a list of supported resources.\nSelecting nodes (nodeSelector and nodeAffinity) With nodeSelector you can ask for a node that matches selected key-value pairs. This can include well-known labels or custom labels you create yourself.\nWhile nodeSelector is like node affinity, it doesn’t have the same “and/or” matchExpressions that affinity has. So all key-value pairs must match if you use nodeSelector. Also, nodeSelector can do only do inclusions, while affinity can do inclusions and exclusions (In and NotIn).\nNode selector (nodeSelector) Here is an example of a nodeSelector for selecting nodes:\nnodeSelector: topology.kubernetes.io/zone: us-west-2a karpenter.sh/capacity-type: spot This example features a well-known label (topology.kubernetes.io/zone) and a label that is well known to Karpenter (karpenter.sh/capacity-type).\nIf you want to create a custom label, you should do that at the provisioner level. Then the pod can declare that custom label.\nSee nodeSelector in the Kubernetes documentation for details.\nNode affinity (nodeAffinity) Examples below illustrate how to use Node affinity to include (In) and exclude (NotIn) objects. See Node affinity for details. When setting rules, the following Node affinity types define how hard or soft each rule is:\n requiredDuringSchedulingIgnoredDuringExecution: This is a hard rule that must be met. preferredDuringSchedulingIgnoredDuringExecution: This is a preference, but the pod can run on a node where it is not guaranteed.  The IgnoredDuringExecution part of each tells the pod to keep running, even if conditions change on the node so the rules no longer matched. You can think of these concepts as required and preferred, since Kubernetes never implemented other variants of these rules.\nAll examples below assume that the provisioner doesn’t have constraints to prevent those zones from being used. The first constraint says you could use us-west-2a or us-west-2b, the second constraint makes it so only us-west-2b can be used.\n affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: \"topology.kubernetes.io/zone\" operator: \"In\" values: [\"us-west-2a, us-west-2b\"] - key: \"topology.kubernetes.io/zone\" operator: \"In\" values: [\"us-west-2b\"] Changing the second operator to NotIn would allow the pod to run in us-west-2a only:\n - key: \"topology.kubernetes.io/zone\" operator: \"In\" values: [\"us-west-2a, us-west-2b\"] - key: \"topology.kubernetes.io/zone\" operator: \"NotIn\" values: [\"us-west-2b\"] Continuing to add to the example, nodeAffinity lets you define terms so if one term doesn’t work it goes to the next one. Here, if us-west-2a is not available, the second term will cause the pod to run on a spot instance in us-west-2d.\n affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: # OR - key: \"topology.kubernetes.io/zone\" # AND operator: \"In\" values: [\"us-west-2a, us-west-2b\"] - key: \"topology.kubernetes.io/zone\" # AND operator: \"NotIn\" values: [\"us-west-2b\"] - matchExpressions: # OR - key: \"karpenter.sh/capacity-type\" # AND operator: \"In\" values: [\"spot\"] - key: \"topology.kubernetes.io/zone\" # AND operator: \"In\" values: [\"us-west-2d\"] In general, Karpenter will go through each of the nodeSelectorTerms in order and take the first one that works. However, if Karpenter fails to provision on the first nodeSelectorTerms, it will try again using the second one. If they all fail, Karpenter will fail to provision the pod. Karpenter will backoff and retry over time. So if capacity becomes available, it will schedule the pod without user intervention.\nTaints and tolerations Taints are the opposite of affinity. Setting a taint on a node tells the scheduler to not run a pod on it unless the pod has explicitly said it can tolerate that taint. This example shows a Provisioner that was set up with a taint for only running pods that require a GPU, such as the following:\napiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: gpu spec: requirements: - key: node.kubernetes.io/instance-type operator: In values: - p3.2xlarge - p3.8xlarge - p3.16xlarge taints: - key: nvidia.com/gpu value: true effect: “NoSchedule” For a pod to request to run on a node that has provisioner, it could set a toleration as follows:\napiVersion: v1 kind: Pod metadata: name: mygpupod spec: containers: - name: gpuapp resources: requests: nvidia.com/gpu: 1 limits: nvidia.com/gpu: 1 image: mygpucontainer tolerations: - key: \"nvidia.com/gpu\" operator: \"Exists\" effect: \"NoSchedule\" See Taints and Tolerations in the Kubernetes documentation for details.\nTopology spread (topologySpreadConstraints) By using the Kubernetes topologySpreadConstraints you can ask the provisioner to have pods push away from each other to limit the blast radius of an outage. Think of it as the Kubernetes evolution for pod affinity: it lets you relate pods with respect to nodes while still allowing spread. For example:\nspec: topologySpreadConstraints: - maxSkew: 1 topologyKey: \"topology.kubernetes.io/zone\" whenUnsatisfiable: ScheduleAnyway labelSelector: matchLabels: dev: jjones - maxSkew: 1 topologyKey: \"kubernetes.io/hostname\" whenUnsatisfiable: ScheduleAnyway labelSelector: matchLabels: dev: jjones Adding this to your podspec would result in:\n Pods being spread across both zones and hosts (topologyKey). The dev labelSelector will include all pods with the label of dev=jjones in topology calculations. It is recommended to use a selector to match all pods in a deployment. No more than one pod difference in the number of pods on each host (maxSkew). For example, if there were three nodes and five pods the pods could be spread 1, 2, 2 or 2, 1, 2 and so on. If instead the spread were 5, pods could be 5, 0, 0 or 3, 2, 0, or 2, 1, 2 and so on. Karpenter is always able to improve skew by launching new nodes in the right zones. Therefore, whenUnsatisfiable does not change provisioning behavior.  See Pod Topology Spread Constraints for details.\n","categories":"","description":"","excerpt":"If your pods have no requirements for how or where to run, you can let …","ref":"/v0.5.0/tasks/running-pods/","tags":"","title":"Running pods"},{"body":"If your pods have no requirements for how or where to run, you can let Karpenter choose nodes from the full range of available cloud provider resources. However, by taking advantage of Karpenter’s model of layered constraints, you can be sure that the precise type and amount of resources needed are available to your pods. Reasons for constraining where your pods run could include:\n Needing to run in zones where dependent applications or storage are available Requiring certain kinds of processors or other hardware Wanting to use techniques like topology spread to help insure high availability  Your Cloud Provider defines the first layer of constraints, including all instance types, architectures, zones, and purchase types available to its cloud. The cluster operator adds the next layer of constraints by creating one or more provisioners. The final layer comes from you adding specifications to your Kubernetes pod deployments. Pod scheduling constraints must fall within a provisioner’s constraints or the pods will not deploy. For example, if the provisioner sets limits that allow only a particular zone to be used, and a pod asks for a different zone, it will not be scheduled.\nConstraints you can request include:\n Resource requests: Request that certain amount of memory or CPU be available. Node selection: Choose to run on a node that is has a particular label (nodeSelector). Node affinity: Draws a pod to run on nodes with particular attributes (affinity). Topology spread: Use topology spread to help insure availability of the application.  Karpenter supports standard Kubernetes scheduling constraints. This allows you to define a single set of rules that apply to both existing and provisioned capacity. Pod affinity is a key exception to this rule.\nNote Karpenter supports specific Well-Known Labels, Annotations and Taints that are useful for scheduling.  Resource requests (resources) Within a Pod spec, you can both make requests and set limits on resources a pod needs, such as CPU and memory. For example:\napiVersion: v1 kind: Pod metadata: name: myapp spec: containers: - name: app image: myimage resources: requests: memory: \"128Mi\" cpu: \"500m\" limits: memory: \"256Mi\" cpu: \"1000m\" In this example, the container is requesting 128MiB of memory and .5 CPU. Its limits are set to 256MiB of memory and 1 CPU. Instance type selection math only uses requests, but limits may be configured to enable resource oversubscription.\nSee Managing Resources for Containers for details on resource types supported by Kubernetes, Specify a memory request and a memory limit for examples of memory requests, and Provisioning Configuration for a list of supported resources.\nSelecting nodes (nodeSelector and nodeAffinity) With nodeSelector you can ask for a node that matches selected key-value pairs. This can include well-known labels or custom labels you create yourself.\nWhile nodeSelector is like node affinity, it doesn’t have the same “and/or” matchExpressions that affinity has. So all key-value pairs must match if you use nodeSelector. Also, nodeSelector can do only do inclusions, while affinity can do inclusions and exclusions (In and NotIn).\nNode selector (nodeSelector) Here is an example of a nodeSelector for selecting nodes:\nnodeSelector: topology.kubernetes.io/zone: us-west-2a karpenter.sh/capacity-type: spot This example features a well-known label (topology.kubernetes.io/zone) and a label that is well known to Karpenter (karpenter.sh/capacity-type).\nIf you want to create a custom label, you should do that at the provisioner level. Then the pod can declare that custom label.\nSee nodeSelector in the Kubernetes documentation for details.\nNode affinity (nodeAffinity) Examples below illustrate how to use Node affinity to include (In) and exclude (NotIn) objects. See Node affinity for details. When setting rules, the following Node affinity types define how hard or soft each rule is:\n requiredDuringSchedulingIgnoredDuringExecution: This is a hard rule that must be met. preferredDuringSchedulingIgnoredDuringExecution: This is a preference, but the pod can run on a node where it is not guaranteed.  The IgnoredDuringExecution part of each tells the pod to keep running, even if conditions change on the node so the rules no longer matched. You can think of these concepts as required and preferred, since Kubernetes never implemented other variants of these rules.\nAll examples below assume that the provisioner doesn’t have constraints to prevent those zones from being used. The first constraint says you could use us-west-2a or us-west-2b, the second constraint makes it so only us-west-2b can be used.\n affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: \"topology.kubernetes.io/zone\" operator: \"In\" values: [\"us-west-2a, us-west-2b\"] - key: \"topology.kubernetes.io/zone\" operator: \"In\" values: [\"us-west-2b\"] Changing the second operator to NotIn would allow the pod to run in us-west-2a only:\n - key: \"topology.kubernetes.io/zone\" operator: \"In\" values: [\"us-west-2a, us-west-2b\"] - key: \"topology.kubernetes.io/zone\" operator: \"NotIn\" values: [\"us-west-2b\"] Continuing to add to the example, nodeAffinity lets you define terms so if one term doesn’t work it goes to the next one. Here, if us-west-2a is not available, the second term will cause the pod to run on a spot instance in us-west-2d.\n affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: # OR - key: \"topology.kubernetes.io/zone\" # AND operator: \"In\" values: [\"us-west-2a, us-west-2b\"] - key: \"topology.kubernetes.io/zone\" # AND operator: \"NotIn\" values: [\"us-west-2b\"] - matchExpressions: # OR - key: \"karpenter.sh/capacity-type\" # AND operator: \"In\" values: [\"spot\"] - key: \"topology.kubernetes.io/zone\" # AND operator: \"In\" values: [\"us-west-2d\"] In general, Karpenter will go through each of the nodeSelectorTerms in order and take the first one that works. However, if Karpenter fails to provision on the first nodeSelectorTerms, it will try again using the second one. If they all fail, Karpenter will fail to provision the pod. Karpenter will backoff and retry over time. So if capacity becomes available, it will schedule the pod without user intervention.\nTaints and tolerations Taints are the opposite of affinity. Setting a taint on a node tells the scheduler to not run a pod on it unless the pod has explicitly said it can tolerate that taint. This example shows a Provisioner that was set up with a taint for only running pods that require a GPU, such as the following:\napiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: gpu spec: requirements: - key: node.kubernetes.io/instance-type operator: In values: - p3.2xlarge - p3.8xlarge - p3.16xlarge taints: - key: nvidia.com/gpu value: true effect: “NoSchedule” For a pod to request to run on a node that has provisioner, it could set a toleration as follows:\napiVersion: v1 kind: Pod metadata: name: mygpupod spec: containers: - name: gpuapp resources: requests: nvidia.com/gpu: 1 limits: nvidia.com/gpu: 1 image: mygpucontainer tolerations: - key: \"nvidia.com/gpu\" operator: \"Exists\" effect: \"NoSchedule\" See Taints and Tolerations in the Kubernetes documentation for details.\nTopology spread (topologySpreadConstraints) By using the Kubernetes topologySpreadConstraints you can ask the provisioner to have pods push away from each other to limit the blast radius of an outage. Think of it as the Kubernetes evolution for pod affinity: it lets you relate pods with respect to nodes while still allowing spread. For example:\nspec: topologySpreadConstraints: - maxSkew: 1 topologyKey: \"topology.kubernetes.io/zone\" whenUnsatisfiable: ScheduleAnyway labelSelector: matchLabels: dev: jjones - maxSkew: 1 topologyKey: \"kubernetes.io/hostname\" whenUnsatisfiable: ScheduleAnyway labelSelector: matchLabels: dev: jjones Adding this to your podspec would result in:\n Pods being spread across both zones and hosts (topologyKey). The dev labelSelector will include all pods with the label of dev=jjones in topology calculations. It is recommended to use a selector to match all pods in a deployment. No more than one pod difference in the number of pods on each host (maxSkew). For example, if there were three nodes and five pods the pods could be spread 1, 2, 2 or 2, 1, 2 and so on. If instead the spread were 5, pods could be 5, 0, 0 or 3, 2, 0, or 2, 1, 2 and so on. Karpenter is always able to improve skew by launching new nodes in the right zones. Therefore, whenUnsatisfiable does not change provisioning behavior.  See Pod Topology Spread Constraints for details.\n","categories":"","description":"","excerpt":"If your pods have no requirements for how or where to run, you can let …","ref":"/v0.5.2/tasks/running-pods/","tags":"","title":"Running pods"},{"body":"If your pods have no requirements for how or where to run, you can let Karpenter choose nodes from the full range of available cloud provider resources. However, by taking advantage of Karpenter’s model of layered constraints, you can be sure that the precise type and amount of resources needed are available to your pods. Reasons for constraining where your pods run could include:\n Needing to run in zones where dependent applications or storage are available Requiring certain kinds of processors or other hardware Wanting to use techniques like topology spread to help insure high availability  Your Cloud Provider defines the first layer of constraints, including all instance types, architectures, zones, and purchase types available to its cloud. The cluster operator adds the next layer of constraints by creating one or more provisioners. The final layer comes from you adding specifications to your Kubernetes pod deployments. Pod scheduling constraints must fall within a provisioner’s constraints or the pods will not deploy. For example, if the provisioner sets limits that allow only a particular zone to be used, and a pod asks for a different zone, it will not be scheduled.\nConstraints you can request include:\n Resource requests: Request that certain amount of memory or CPU be available. Node selection: Choose to run on a node that is has a particular label (nodeSelector). Node affinity: Draws a pod to run on nodes with particular attributes (affinity). Topology spread: Use topology spread to help insure availability of the application.  Karpenter supports standard Kubernetes scheduling constraints. This allows you to define a single set of rules that apply to both existing and provisioned capacity. Pod affinity is a key exception to this rule.\nNote Karpenter supports specific Well-Known Labels, Annotations and Taints that are useful for scheduling.  Resource requests (resources) Within a Pod spec, you can both make requests and set limits on resources a pod needs, such as CPU and memory. For example:\napiVersion: v1 kind: Pod metadata: name: myapp spec: containers: - name: app image: myimage resources: requests: memory: \"128Mi\" cpu: \"500m\" limits: memory: \"256Mi\" cpu: \"1000m\" In this example, the container is requesting 128MiB of memory and .5 CPU. Its limits are set to 256MiB of memory and 1 CPU. Instance type selection math only uses requests, but limits may be configured to enable resource oversubscription.\nSee Managing Resources for Containers for details on resource types supported by Kubernetes, Specify a memory request and a memory limit for examples of memory requests, and Provisioning Configuration for a list of supported resources.\nSelecting nodes (nodeSelector and nodeAffinity) With nodeSelector you can ask for a node that matches selected key-value pairs. This can include well-known labels or custom labels you create yourself.\nWhile nodeSelector is like node affinity, it doesn’t have the same “and/or” matchExpressions that affinity has. So all key-value pairs must match if you use nodeSelector. Also, nodeSelector can do only do inclusions, while affinity can do inclusions and exclusions (In and NotIn).\nNode selector (nodeSelector) Here is an example of a nodeSelector for selecting nodes:\nnodeSelector: topology.kubernetes.io/zone: us-west-2a karpenter.sh/capacity-type: spot This example features a well-known label (topology.kubernetes.io/zone) and a label that is well known to Karpenter (karpenter.sh/capacity-type).\nIf you want to create a custom label, you should do that at the provisioner level. Then the pod can declare that custom label.\nSee nodeSelector in the Kubernetes documentation for details.\nNode affinity (nodeAffinity) Examples below illustrate how to use Node affinity to include (In) and exclude (NotIn) objects. See Node affinity for details. When setting rules, the following Node affinity types define how hard or soft each rule is:\n requiredDuringSchedulingIgnoredDuringExecution: This is a hard rule that must be met. preferredDuringSchedulingIgnoredDuringExecution: This is a preference, but the pod can run on a node where it is not guaranteed.  The IgnoredDuringExecution part of each tells the pod to keep running, even if conditions change on the node so the rules no longer matched. You can think of these concepts as required and preferred, since Kubernetes never implemented other variants of these rules.\nAll examples below assume that the provisioner doesn’t have constraints to prevent those zones from being used. The first constraint says you could use us-west-2a or us-west-2b, the second constraint makes it so only us-west-2b can be used.\n affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: \"topology.kubernetes.io/zone\" operator: \"In\" values: [\"us-west-2a, us-west-2b\"] - key: \"topology.kubernetes.io/zone\" operator: \"In\" values: [\"us-west-2b\"] Changing the second operator to NotIn would allow the pod to run in us-west-2a only:\n - key: \"topology.kubernetes.io/zone\" operator: \"In\" values: [\"us-west-2a, us-west-2b\"] - key: \"topology.kubernetes.io/zone\" operator: \"NotIn\" values: [\"us-west-2b\"] Continuing to add to the example, nodeAffinity lets you define terms so if one term doesn’t work it goes to the next one. Here, if us-west-2a is not available, the second term will cause the pod to run on a spot instance in us-west-2d.\n affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: # OR - key: \"topology.kubernetes.io/zone\" # AND operator: \"In\" values: [\"us-west-2a, us-west-2b\"] - key: \"topology.kubernetes.io/zone\" # AND operator: \"NotIn\" values: [\"us-west-2b\"] - matchExpressions: # OR - key: \"karpenter.sh/capacity-type\" # AND operator: \"In\" values: [\"spot\"] - key: \"topology.kubernetes.io/zone\" # AND operator: \"In\" values: [\"us-west-2d\"] In general, Karpenter will go through each of the nodeSelectorTerms in order and take the first one that works. However, if Karpenter fails to provision on the first nodeSelectorTerms, it will try again using the second one. If they all fail, Karpenter will fail to provision the pod. Karpenter will backoff and retry over time. So if capacity becomes available, it will schedule the pod without user intervention.\nTaints and tolerations Taints are the opposite of affinity. Setting a taint on a node tells the scheduler to not run a pod on it unless the pod has explicitly said it can tolerate that taint. This example shows a Provisioner that was set up with a taint for only running pods that require a GPU, such as the following:\napiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: gpu spec: requirements: - key: node.kubernetes.io/instance-type operator: In values: - p3.2xlarge - p3.8xlarge - p3.16xlarge taints: - key: nvidia.com/gpu value: true effect: “NoSchedule” For a pod to request to run on a node that has provisioner, it could set a toleration as follows:\napiVersion: v1 kind: Pod metadata: name: mygpupod spec: containers: - name: gpuapp resources: requests: nvidia.com/gpu: 1 limits: nvidia.com/gpu: 1 image: mygpucontainer tolerations: - key: \"nvidia.com/gpu\" operator: \"Exists\" effect: \"NoSchedule\" See Taints and Tolerations in the Kubernetes documentation for details.\nTopology spread (topologySpreadConstraints) By using the Kubernetes topologySpreadConstraints you can ask the provisioner to have pods push away from each other to limit the blast radius of an outage. Think of it as the Kubernetes evolution for pod affinity: it lets you relate pods with respect to nodes while still allowing spread. For example:\nspec: topologySpreadConstraints: - maxSkew: 1 topologyKey: \"topology.kubernetes.io/zone\" whenUnsatisfiable: ScheduleAnyway labelSelector: matchLabels: dev: jjones - maxSkew: 1 topologyKey: \"kubernetes.io/hostname\" whenUnsatisfiable: ScheduleAnyway labelSelector: matchLabels: dev: jjones Adding this to your podspec would result in:\n Pods being spread across both zones and hosts (topologyKey). The dev labelSelector will include all pods with the label of dev=jjones in topology calculations. It is recommended to use a selector to match all pods in a deployment. No more than one pod difference in the number of pods on each host (maxSkew). For example, if there were three nodes and five pods the pods could be spread 1, 2, 2 or 2, 1, 2 and so on. If instead the spread were 5, pods could be 5, 0, 0 or 3, 2, 0, or 2, 1, 2 and so on. Karpenter is always able to improve skew by launching new nodes in the right zones. Therefore, whenUnsatisfiable does not change provisioning behavior.  See Pod Topology Spread Constraints for details.\n","categories":"","description":"","excerpt":"If your pods have no requirements for how or where to run, you can let …","ref":"/v0.5.3/tasks/running-pods/","tags":"","title":"Running pods"},{"body":"The Provisioner CRD supports defining node properties like instance type and zone. For certain well-known labels (documented below), Karpenter will provision nodes accordingly. For example, in response to a label of topology.kubernetes.io/zone=us-east-1c, Karpenter will provision nodes in that availability zone.\nInstance Types Karpenter supports specifying AWS instance type.\nThe default value includes all instance types with the exclusion of metal (non-virtualized), non-HVM, and GPU instances.\nIf necessary, Karpenter supports defining a limited list of default instance types.\nIf more than one type is listed, Karpenter will determine the instance type to minimize the number of new nodes.\nView the full list of instance types with aws ec2 describe-instance-types.\nExample\nSet Default with provisioner.yaml\nspec:requirements:- key:node.kubernetes.io/instance-typeoperator:Invalues:[\"m5.large\",\"m5.2xlarge\"]Override with workload manifest (e.g., pod)\nspec:template:spec:nodeSelector:node.kubernetes.io/instance-type:m5.largeAvailability Zones topology.kubernetes.io/zone=us-east-1c\n key: topology.kubernetes.io/zone value example: us-east-1c value list: aws ec2 describe-availability-zones --region \u003cregion-name\u003e  Karpenter can be configured to create nodes in a particular zone. Note that the Availability Zone us-east-1a for your AWS account might not have the same location as us-east-1a for another AWS account.\nLearn more about Availability Zone IDs.\nCapacity Type  key: karpenter.sh/capacity-type values  on-demand (default) spot    Karpenter supports specifying capacity type, which is analogous to EC2 usage classes (aka “market types”) and defaults to on-demand.\nSpecify this value on the provisioner to enable spot instances. Spot instances may be preempted, and should not be used for critical workloads that do not tolerate interruptions.\nExample\nSet Default with provisioner.yaml\nspec:requirements:- key:karpenter.sh/capacity-typeoperator:Invalues:[\"spot\",\"on-demand\"]Override with workload manifest (e.g., pod)\nspec:template:spec:nodeSelector:karpenter.sh/capacity-type:spotArchitecture  key: kubernetes.io/arch values  amd64 (default) arm64    Karpenter supports amd64 nodes, and arm64 nodes.\nExample\nSet Default with provisioner.yaml\nspec:requirements:- key:kubernetes.io/archoperator:Invalues:[\"arm64\",\"amd64\"]Override with workload manifest (e.g., pod)\nspec:template:spec:nodeSelector:kubernetes.io/arch:amd64Operating System  key: kubernetes.io/os values  linux (default)    At this time, Karpenter only supports Linux OS nodes.\nAccelerators, GPU Accelerator (e.g., GPU) values include\n nvidia.com/gpu amd.com/gpu aws.amazon.com/neuron  Karpenter supports accelerators, such as GPUs.\nTo enable instances with accelerators, use the instance type well known label selector.\nAdditionally, include a resource requirement in the workload manifest. Thus, accelerator dependent pod will be scheduled onto the appropriate node.\naccelerator resource in workload manifest (e.g., pod)\nspec:template:spec:containers:- resources:limits:nvidia.com/gpu:\"1\"","categories":"","description":"","excerpt":"The Provisioner CRD supports defining node properties like instance …","ref":"/v0.4.3/cloud-providers/aws/aws-spec-fields/","tags":"","title":"Specifying Values to Control AWS Provisioning"},{"body":"If your pods have no requirements for how or where to run, you can let Karpenter choose nodes from the full range of available cloud provider resources. However, by taking advantage of Karpenter’s model of layered constraints, you can be sure that the precise type and amount of resources needed are available to your pods. Reasons for constraining where your pods run could include:\n Needing to run in zones where dependent applications or storage are available Requiring certain kinds of processors or other hardware Wanting to use techniques like topology spread to help insure high availability  Your Cloud Provider defines the first layer of constraints, including all instance types, architectures, zones, and purchase types available to its cloud. The cluster administrator adds the next layer of constraints by creating one or more provisioners. The final layer comes from you adding specifications to your Kubernetes pod deployments. Pod scheduling constraints must fall within a provisioner’s constraints or the pods will not deploy. For example, if the provisioner sets limits that allow only a particular zone to be used, and a pod asks for a different zone, it will not be scheduled.\nConstraints you can request include:\n Resource requests: Request that certain amount of memory or CPU be available. Node selection: Choose to run on a node that is has a particular label (nodeSelector). Node affinity: Draws a pod to run on nodes with particular attributes (affinity). Topology spread: Use topology spread to help insure availability of the application.  Karpenter supports standard Kubernetes scheduling constraints. This allows you to define a single set of rules that apply to both existing and provisioned capacity. Pod affinity is a key exception to this rule.\nNote Karpenter supports specific Well-Known Labels, Annotations and Taints that are useful for scheduling.  Resource requests Within a Pod spec, you can both make requests and set limits on resources a pod needs, such as CPU and memory. For example:\napiVersion: v1 kind: Pod metadata: name: myapp spec: containers: - name: app image: myimage resources: requests: memory: \"128Mi\" cpu: \"500m\" limits: memory: \"256Mi\" cpu: \"1000m\" In this example, the container is requesting 128MiB of memory and .5 CPU. Its limits are set to 256MiB of memory and 1 CPU. Instance type selection math only uses requests, but limits may be configured to enable resource oversubscription.\nSee Managing Resources for Containers for details on resource types supported by Kubernetes, Specify a memory request and a memory limit for examples of memory requests, and Provisioning Configuration for a list of supported resources.\nSelecting nodes With nodeSelector you can ask for a node that matches selected key-value pairs. This can include well-known labels or custom labels you create yourself.\nWhile nodeSelector is like node affinity, it doesn’t have the same “and/or” matchExpressions that affinity has. So all key-value pairs must match if you use nodeSelector. Also, nodeSelector can do only do inclusions, while affinity can do inclusions and exclusions (In and NotIn).\nNode selectors Here is an example of a nodeSelector for selecting nodes:\nnodeSelector: topology.kubernetes.io/zone: us-west-2a karpenter.sh/capacity-type: spot This example features a well-known label (topology.kubernetes.io/zone) and a label that is well known to Karpenter (karpenter.sh/capacity-type).\nIf you want to create a custom label, you should do that at the provisioner level. Then the pod can declare that custom label.\nSee nodeSelector in the Kubernetes documentation for details.\nNode affinity Examples below illustrate how to use Node affinity to include (In) and exclude (NotIn) objects. See Node affinity for details. When setting rules, the following Node affinity types define how hard or soft each rule is:\n requiredDuringSchedulingIgnoredDuringExecution: This is a hard rule that must be met. preferredDuringSchedulingIgnoredDuringExecution: This is a preference, but the pod can run on a node where it is not guaranteed.  The IgnoredDuringExecution part of each tells the pod to keep running, even if conditions change on the node so the rules no longer matched. You can think of these concepts as required and preferred, since Kubernetes never implemented other variants of these rules.\nAll examples below assume that the provisioner doesn’t have constraints to prevent those zones from being used. The first constraint says you could use us-west-2a or us-west-2b, the second constraint makes it so only us-west-2b can be used.\n affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: \"topology.kubernetes.io/zone\" operator: \"In\" values: [\"us-west-2a, us-west-2b\"] - key: \"topology.kubernetes.io/zone\" operator: \"In\" values: [\"us-west-2b\"] Changing the second operator to NotIn would allow the pod to run in us-west-2a only:\n - key: \"topology.kubernetes.io/zone\" operator: \"In\" values: [\"us-west-2a, us-west-2b\"] - key: \"topology.kubernetes.io/zone\" operator: \"NotIn\" values: [\"us-west-2b\"] Continuing to add to the example, nodeAffinity lets you define terms so if one term doesn’t work it goes to the next one. Here, if us-west-2a is not available, the second term will cause the pod to run on a spot instance in us-west-2d.\n affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: # OR - key: \"topology.kubernetes.io/zone\" # AND operator: \"In\" values: [\"us-west-2a, us-west-2b\"] - key: \"topology.kubernetes.io/zone\" # AND operator: \"NotIn\" values: [\"us-west-2b\"] - matchExpressions: # OR - key: \"karpenter.sh/capacity-type\" # AND operator: \"In\" values: [\"spot\"] - key: \"topology.kubernetes.io/zone\" # AND operator: \"In\" values: [\"us-west-2d\"] In general, Karpenter will go through each of the nodeSelectorTerms in order and take the first one that works. However, if Karpenter fails to provision on the first nodeSelectorTerms, it will try again using the second one. If they all fail, Karpenter will fail to provision the pod. Karpenter will backoff and retry over time. So if capacity becomes available, it will schedule the pod without user intervention.\nTaints and tolerations Taints are the opposite of affinity. Setting a taint on a node tells the scheduler to not run a pod on it unless the pod has explicitly said it can tolerate that taint. This example shows a Provisioner that was set up with a taint for only running pods that require a GPU, such as the following:\napiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: gpu spec: requirements: - key: node.kubernetes.io/instance-type operator: In values: - p3.2xlarge - p3.8xlarge - p3.16xlarge taints: - key: nvidia.com/gpu value: true effect: “NoSchedule” For a pod to request to run on a node that has provisioner, it could set a toleration as follows:\napiVersion: v1 kind: Pod metadata: name: mygpupod spec: containers: - name: gpuapp resources: requests: nvidia.com/gpu: 1 limits: nvidia.com/gpu: 1 image: mygpucontainer tolerations: - key: \"nvidia.com/gpu\" operator: \"Exists\" effect: \"NoSchedule\" See Taints and Tolerations in the Kubernetes documentation for details.\nTopology Spread By using the Kubernetes topologySpreadConstraints you can ask the provisioner to have pods push away from each other to limit the blast radius of an outage. Think of it as the Kubernetes evolution for pod affinity: it lets you relate pods with respect to nodes while still allowing spread. For example:\nspec: topologySpreadConstraints: - maxSkew: 1 topologyKey: \"topology.kubernetes.io/zone\" whenUnsatisfiable: ScheduleAnyway labelSelector: matchLabels: dev: jjones - maxSkew: 1 topologyKey: \"kubernetes.io/hostname\" whenUnsatisfiable: ScheduleAnyway labelSelector: matchLabels: dev: jjones Adding this to your podspec would result in:\n Pods being spread across both zones and hosts (topologyKey). The dev labelSelector will include all pods with the label of dev=jjones in topology calculations. It is recommended to use a selector to match all pods in a deployment. No more than one pod difference in the number of pods on each host (maxSkew). For example, if there were three nodes and five pods the pods could be spread 1, 2, 2 or 2, 1, 2 and so on. If instead the spread were 5, pods could be 5, 0, 0 or 3, 2, 0, or 2, 1, 2 and so on. Karpenter is always able to improve skew by launching new nodes in the right zones. Therefore, whenUnsatisfiable does not change provisioning behavior.  See Pod Topology Spread Constraints for details.\nPersistent Volume Topology Karpenter automatically detects storage scheduling requirements and includes them in node launch decisions.\nIn the following example, the StorageClass defines zonal topologies for us-west-2a and us-west-2b and binding mode WaitForFirstConsumer. When the pod is created, Karpenter follows references from the Pod to PersistentVolumeClaim to StorageClass and identifies that this pod requires storage in us-west-2a and us-west-2b. It randomly selects us-west-2a, provisions a node in that zone, and binds the pod to the node. The CSI driver creates a PersistentVolume according to the PersistentVolumeClaim and gives it a node affinity rule for us-west-2a.\nLater on, the pod is deleted and a new pod is created that requests the same claim. This time, Karpenter identifies that a PersistentVolume already exists for the PersistentVolumeClaim, and includes its zone us-west-2a in the pod’s scheduling requirements.\napiVersion:v1kind:Podmetadata:name:appspec:containers:...volumes:- name:storagepersistentVolumeClaim:claimName:ebs-claim---kind:StorageClassapiVersion:storage.k8s.io/v1metadata:name:ebsprovisioner:ebs.csi.aws.comvolumeBindingMode:WaitForFirstConsumerallowedTopologies:- matchLabelExpressions:- key:topology.ebs.csi.aws.com/zonevalues:[\"us-west-2a\",\"us-west-2b\"]---apiVersion:v1kind:PersistentVolumeClaimmetadata:name:ebs-claimspec:accessModes:- ReadWriteOncestorageClassName:ebsresources:requests:storage:4Gi Note ☁️ AWS Specific\nThe EBS CSI driver uses topology.ebs.csi.aws.com/zone instead of the standard topology.kubernetes.io/zone label. Karpenter is aware of label aliasing and translates this label into topology.kubernetes.io/zone in memory. When configuring a StorageClass for the EBS CSI Driver, you must use topology.ebs.csi.aws.com/zone.\n Note The topology key topology.kubernetes.io/region is not supported. Legacy in-tree CSI providers specify this label. Instead, install an out-of-tree CSI provider. Learn more about moving to CSI providers.  ","categories":"","description":"","excerpt":"If your pods have no requirements for how or where to run, you can let …","ref":"/preview/tasks/scheduling-pods/","tags":"","title":"Scheduling"},{"body":"Karpenter is an open-source node provisioning project built for Kubernetes. Adding Karpenter to a Kubernetes cluster can dramatically improve the efficiency and cost of running workloads on that cluster. Karpenter is tightly integrated with Kubernetes features to make sure that the right types and amounts of compute resources are available to pods as they are needed. Karpenter works by:\n Watching for pods that the Kubernetes scheduler has marked as unschedulable Evaluating scheduling constraints (resource requests, nodeselectors, affinities, tolerations, and topology spread constraints) requested by the pods Provisioning nodes that meet the requirements of the pods Scheduling the pods to run on the new nodes Removing the nodes when the nodes are no longer needed  As a cluster administrator, you can configure an unconstrained Karpenter provisioner when it is first installed and not change it again. Other times, you might continue to tweak the provisioner or create multiple provisioners for a cluster used by different teams. On-going cluster administrator tasks include upgrading and decommissioning nodes.\nAs an application developer, you can make specific requests for capacity and features you want from the nodes running your pods. Karpenter is designed to quickly create the best possible nodes to meet those needs and schedule the pods to run on them.\nLearn more about Karpenter and how to get started below.\n","categories":"","description":"","excerpt":"Karpenter is an open-source node provisioning project built for …","ref":"/preview/","tags":"","title":"Documentation"},{"body":"Karpenter is an open-source node provisioning project built for Kubernetes. Adding Karpenter to a Kubernetes cluster can dramatically improve the efficiency and cost of running workloads on that cluster. Karpenter is tightly integrated with Kubernetes features to make sure that the right types and amounts of compute resources are available to pods as they are needed. Karpenter works by:\n Watching for pods that the Kubernetes scheduler has marked as unschedulable Evaluating scheduling constraints (resource requests, nodeselectors, affinities, tolerations, and topology spread constraints) requested by the pods Provisioning nodes that meet the requirements of the pods Scheduling the pods to run on the new nodes Removing the nodes when the nodes are no longer needed  As a cluster operator, you can configure an unconstrained Karpenter provisioner when it is first installed and not change it again. Other times, you might continue to tweak the provisioner or create multiple provisioners for a cluster used by different teams. On-going cluster operator tasks include upgrading and decommissioning nodes.\nAs an application developer, you can make specific requests for capacity and features you want from the nodes running your pods. Karpenter is designed to quickly create the best possible nodes to meet those needs and schedule the pods to run on them.\nLearn more about Karpenter and how to get started below.\n","categories":"","description":"","excerpt":"Karpenter is an open-source node provisioning project built for …","ref":"/v0.4.3/","tags":"","title":"Documentation"},{"body":"Karpenter is an open-source node provisioning project built for Kubernetes. Adding Karpenter to a Kubernetes cluster can dramatically improve the efficiency and cost of running workloads on that cluster. Karpenter is tightly integrated with Kubernetes features to make sure that the right types and amounts of compute resources are available to pods as they are needed. Karpenter works by:\n Watching for pods that the Kubernetes scheduler has marked as unschedulable Evaluating scheduling constraints (resource requests, nodeselectors, affinities, tolerations, and topology spread constraints) requested by the pods Provisioning nodes that meet the requirements of the pods Scheduling the pods to run on the new nodes Removing the nodes when the nodes are no longer needed  As a cluster operator, you can configure an unconstrained Karpenter provisioner when it is first installed and not change it again. Other times, you might continue to tweak the provisioner or create multiple provisioners for a cluster used by different teams. On-going cluster operator tasks include upgrading and decommissioning nodes.\nAs an application developer, you can make specific requests for capacity and features you want from the nodes running your pods. Karpenter is designed to quickly create the best possible nodes to meet those needs and schedule the pods to run on them.\nLearn more about Karpenter and how to get started below.\n","categories":"","description":"","excerpt":"Karpenter is an open-source node provisioning project built for …","ref":"/v0.5.0/","tags":"","title":"Documentation"},{"body":"Karpenter is an open-source node provisioning project built for Kubernetes. Adding Karpenter to a Kubernetes cluster can dramatically improve the efficiency and cost of running workloads on that cluster. Karpenter is tightly integrated with Kubernetes features to make sure that the right types and amounts of compute resources are available to pods as they are needed. Karpenter works by:\n Watching for pods that the Kubernetes scheduler has marked as unschedulable Evaluating scheduling constraints (resource requests, nodeselectors, affinities, tolerations, and topology spread constraints) requested by the pods Provisioning nodes that meet the requirements of the pods Scheduling the pods to run on the new nodes Removing the nodes when the nodes are no longer needed  As a cluster operator, you can configure an unconstrained Karpenter provisioner when it is first installed and not change it again. Other times, you might continue to tweak the provisioner or create multiple provisioners for a cluster used by different teams. On-going cluster operator tasks include upgrading and decommissioning nodes.\nAs an application developer, you can make specific requests for capacity and features you want from the nodes running your pods. Karpenter is designed to quickly create the best possible nodes to meet those needs and schedule the pods to run on them.\nLearn more about Karpenter and how to get started below.\n","categories":"","description":"","excerpt":"Karpenter is an open-source node provisioning project built for …","ref":"/v0.5.2/","tags":"","title":"Documentation"},{"body":"Karpenter is an open-source node provisioning project built for Kubernetes. Adding Karpenter to a Kubernetes cluster can dramatically improve the efficiency and cost of running workloads on that cluster. Karpenter is tightly integrated with Kubernetes features to make sure that the right types and amounts of compute resources are available to pods as they are needed. Karpenter works by:\n Watching for pods that the Kubernetes scheduler has marked as unschedulable Evaluating scheduling constraints (resource requests, nodeselectors, affinities, tolerations, and topology spread constraints) requested by the pods Provisioning nodes that meet the requirements of the pods Scheduling the pods to run on the new nodes Removing the nodes when the nodes are no longer needed  As a cluster operator, you can configure an unconstrained Karpenter provisioner when it is first installed and not change it again. Other times, you might continue to tweak the provisioner or create multiple provisioners for a cluster used by different teams. On-going cluster operator tasks include upgrading and decommissioning nodes.\nAs an application developer, you can make specific requests for capacity and features you want from the nodes running your pods. Karpenter is designed to quickly create the best possible nodes to meet those needs and schedule the pods to run on them.\nLearn more about Karpenter and how to get started below.\n","categories":"","description":"","excerpt":"Karpenter is an open-source node provisioning project built for …","ref":"/v0.5.3/","tags":"","title":"Documentation"},{"body":"Deletion Workflow Finalizer Karpenter adds a finalizer to provisioned nodes. Review how finalizers work.\nDrain Nodes Review how to safely drain a node.\nDelete Node Karpenter changes the behavior of kubectl delete node. Nodes will be drained, and then the underlying instance will be deleted.\nDisruption Budget Karpenter respects Pod Disruption Budgets. Review what disruptions are, and how to configure them.\nGenerally, pod workloads may be configured with .spec.minAvailable and/or .spec.maxUnavailable. Karpenter provisions nodes to accommodate these constraints.\nEmptiness Karpenter will delete nodes (and the instance) that are considered empty of pods. Daemonset pods are not included in this calculation.\nExpiry Nodes may be configured to expire. That is, a maximum lifetime in seconds starting with the node joining the cluster. Review the ttlSecondsUntilExpired field of the provisioner API.\nNote that newly created nodes have a Kubernetes version matching the control plane. One use case for node expiry is to handle node upgrades. Old nodes (with a potentially outdated Kubernetes version) are deleted, and replaced with nodes on the current version.\n","categories":"","description":"","excerpt":"Deletion Workflow Finalizer Karpenter adds a finalizer to provisioned …","ref":"/v0.5.0/tasks/deprov-nodes/","tags":"","title":"Deprovisioning nodes"},{"body":"Deletion Workflow Finalizer Karpenter adds a finalizer to provisioned nodes. Review how finalizers work.\nDrain Nodes Review how to safely drain a node.\nDelete Node Karpenter changes the behavior of kubectl delete node. Nodes will be drained, and then the underlying instance will be deleted.\nDisruption Budget Karpenter respects Pod Disruption Budgets. Review what disruptions are, and how to configure them.\nGenerally, pod workloads may be configured with .spec.minAvailable and/or .spec.maxUnavailable. Karpenter provisions nodes to accommodate these constraints.\nEmptiness Karpenter will delete nodes (and the instance) that are considered empty of pods. Daemonset pods are not included in this calculation.\nExpiry Nodes may be configured to expire. That is, a maximum lifetime in seconds starting with the node joining the cluster. Review the ttlSecondsUntilExpired field of the provisioner API.\nNote that newly created nodes have a Kubernetes version matching the control plane. One use case for node expiry is to handle node upgrades. Old nodes (with a potentially outdated Kubernetes version) are deleted, and replaced with nodes on the current version.\n","categories":"","description":"","excerpt":"Deletion Workflow Finalizer Karpenter adds a finalizer to provisioned …","ref":"/v0.5.2/tasks/deprov-nodes/","tags":"","title":"Deprovisioning nodes"},{"body":"Deletion Workflow Finalizer Karpenter adds a finalizer to provisioned nodes. Review how finalizers work.\nDrain Nodes Review how to safely drain a node.\nDelete Node Karpenter changes the behavior of kubectl delete node. Nodes will be drained, and then the underlying instance will be deleted.\nDisruption Budget Karpenter respects Pod Disruption Budgets. Review what disruptions are, and how to configure them.\nGenerally, pod workloads may be configured with .spec.minAvailable and/or .spec.maxUnavailable. Karpenter provisions nodes to accommodate these constraints.\nEmptiness Karpenter will delete nodes (and the instance) that are considered empty of pods. Daemonset pods are not included in this calculation.\nExpiry Nodes may be configured to expire. That is, a maximum lifetime in seconds starting with the node joining the cluster. Review the ttlSecondsUntilExpired field of the provisioner API.\nNote that newly created nodes have a Kubernetes version matching the control plane. One use case for node expiry is to handle node upgrades. Old nodes (with a potentially outdated Kubernetes version) are deleted, and replaced with nodes on the current version.\n","categories":"","description":"","excerpt":"Deletion Workflow Finalizer Karpenter adds a finalizer to provisioned …","ref":"/v0.5.3/tasks/deprov-nodes/","tags":"","title":"Deprovisioning nodes"},{"body":"General How does a Provisioner decide to manage a particular node? Karpenter will only take action on nodes that it provisions. All nodes launched by Karpenter will be labeled with karpenter.sh/provisioner-name.\nCompatibility Which Kubernetes versions does Karpenter support? Karpenter releases on a similar cadence to upstream Kubernetes releases. Currently, Karpenter is compatible with Kubernetes versions v1.19+. However, this may change in the future as Karpenter takes dependencies on new Kubernetes features.\nCan I use Karpenter alongside another node management solution? Provisioners are designed to work alongside static capacity management solutions like EKS Managed Node Groups and EC2 Auto Scaling Groups. Some users may choose to (1) manage the entirety of their capacity using Provisioners, others may prefer (2) a mixed model with both dynamic and statically managed capacity, some may prefer (3) a fully static approach. We anticipate that most users will fall into bucket (2) in the short term, and (1) in the long term.\nCan I use Karpenter with the Kubernetes Cluster Autoscaler? Yes, with side effects. Karpenter is a Cluster Autoscaler replacement. Both systems scale up nodes in response to unschedulable pods. If configured together, both systems will race to launch new instances for these pods. Since Karpenter makes binding decisions, Karpenter will typically win the scheduling race. In this case, the Cluster Autoscaler will eventually scale down the unnecessary capacity. If the Cluster Autoscaler is configured with Node Groups that support scheduling constraints that aren’t supported by any Provisioner, its behavior will continue unimpeded.\nDoes Karpenter replace the Kube Scheduler? No. Provisioners work in tandem with the Kube Scheduler. When capacity is unconstrained, the Kube Scheduler will schedule pods as usual. It may schedule pods to nodes managed by Provisioners or other types of capacity in the cluster. Provisioners only attempt to schedule pods when type=PodScheduled,reason=Unschedulable. In this case, Karpenter will make a provisioning decision, launch new capacity, and bind pods to the provisioned nodes. Unlike the Cluster Autoscaler, Karpenter does not wait for the Kube Scheduler to make a scheduling decision, as the decision is already made during the provisioning decision. It’s possible that a node from another management solution, like the Cluster Autoscaler, could create a race between the kube-scheduler and Karpenter. In this case, the first binding call will win, although Karpenter will often win these race conditions due to its performance characteristics. If Karpenter loses this race, the node will eventually be cleaned up.\nProvisioning How should I define scheduling constraints? Karpenter takes a layered approach to scheduling constraints. Karpenter comes with a set of global defaults, which may be overridden by Provisioner-level defaults. Further, these may be overridden by pod scheduling constraints. This model requires minimal configuration for most use cases, and supports diverse workloads using a single Provisioner.\nDoes Karpenter support node selectors? Yes. Node selectors are an opt-in mechanism which allow users to specify the nodes on which a pod can scheduled. Karpenter recognizes well-known node selectors on unschedulable pods and uses them to constrain the nodes it provisions. You can read more about the well-known node selectors supported by Karpenter in the Concepts documentation. For example, node.kubernetes.io/instance-type, topology.kubernetes.io/zone, kubernetes.io/os, kubernetes.io/arch, karpenter.sh/capacity-type are supported, and will ensure that provisioned nodes are constrained accordingly. Additionally, users may specify arbitrary labels, which will be automatically applied to every node launched by the Provisioner.\nDoes Karpenter support taints? Yes. Taints are an opt-out mechanism which allows users to specify the nodes on which a pod cannot be scheduled. Unlike node selectors, Karpenter does not automatically taint nodes in response to pod tolerations. Similar to node selectors, users may specify taints on their Provisioner, which will be automatically added to every node it provisions. This means that if a Provisioner is configured with taints, any incoming pods will not be scheduled unless the taints are tolerated.\nDoes Karpenter support topology spread constraints? Not yet. Karpenter plans to respect pod.spec.topologySpreadConstraints by v0.4.0.\nDoes Karpenter support node affinity? Not yet. Karpenter plans to respect pod.spec.nodeAffinity by v0.4.0.\nDoes Karpenter support custom resource like accelerators or HPC? Yes. Support for specific custom resources may be implemented by cloud providers. The AWS Cloud Provider supports nvidia.com/gpu, amd.com/gpu, aws.amazon.com/neuron.\nDoes Karpenter support daemonsets? Yes. Karpenter factors in daemonset overhead into all provisioning calculations. Daemonsets are only included in calculations if their scheduling constraints are applicable to the provisioned node.\nDoes Karpenter support multiple Provisioners? Each Provisioner is capable of defining heterogenous nodes across multiple availability zones, instance types, and capacity types. This flexibility reduces the need for a large number of Provisioners. However, users may find multiple Provisioners to be useful for more advanced use cases, such as defining multiple sets of provisioning defaults in a single cluster.\nIf multiple Provisioners are defined, which will my pod use? By default, pods will use the rules defined by a Provisioner named default. This is analogous to the default scheduler. To select an alternative provisioner, use the node selector karpenter.sh/provisioner-name: alternative-provisioner. You must either define a default provisioner or explicitly specify karpenter.sh/provisioner-name node selector.\nDeprovisioning How does Karpenter decide which nodes it can terminate? Karpenter will only terminate nodes that it manages. Nodes will be considered for termination due to expiry or emptiness (see below).\nWhen does Karpenter terminate empty nodes? Nodes are considered empty when they do not have any pods scheduled to them. Daemonsets pods and Failed pods are ignored. Karpenter will send a deletion request to the Kubernetes API, and graceful termination will be handled by termination finalizer. Karpenter will wait for the duration of ttlSecondsAfterUnderutilized to terminate an empty node. If ttlSecondsAfterUnderutilized is unset, which it is by default, Karpenter will not terminate nodes once they are empty.\nWhen does Karpenter terminate expired nodes? Nodes are considered expired when the current time exceeds their creation time plus ttlSecondsUntilExpired. Karpenter will send a deletion request to the Kubernetes API, and graceful termination will be handled by termination finalizer. If ttlSecondsUntilExpired is unset, which it is by default, Karpenter will not terminate any nodes due to expiry.\nHow does Karpenter terminate nodes? Karpenter cordons nodes to be terminated and uses the Kubernetes Eviction API to evict all non-daemonset pods. After successful eviction of all non-daemonset pods, the node is terminated. If all the pods cannot be evicted, Karpenter won’t forcibly terminate them and keep on trying to evict them. Karpenter respects Pod Disruption Budgets (PDB) by using the Kubernetes Eviction API.\nDoes Karpenter support scale to zero? Yes. Karpenter only launches or terminates nodes as necessary based on aggregate pod resource requests. Karpenter will only retain nodes in your cluster as long as there are pods using them.\n","categories":"","description":"","excerpt":"General How does a Provisioner decide to manage a particular node? …","ref":"/v0.4.3/faqs/","tags":"","title":"FAQs"},{"body":"Users fall under two basic roles: Kubernetes cluster administrators and application developers. This document describes Karpenter concepts through the lens of those two types of users.\nCluster administrator As a Kubernetes cluster administrator, you can engage with Karpenter to:\n Install Karpenter Configure provisioners to set constraints and other features for managing nodes Deprovision nodes Upgrade nodes  Concepts associated with this role are described below.\nInstalling Karpenter Karpenter is designed to run on a node in your Kubernetes cluster. As part of the installation process, you need credentials from the underlying cloud provider to allow nodes to be started up and added to the cluster as they are needed.\nGetting Started with Karpenter on AWS describes the process of installing Karpenter on an AWS cloud provider. Because requests to add and delete nodes and schedule pods are made through Kubernetes, AWS IAM Roles for Service Accounts (IRSA) are needed by your Kubernetes cluster to make privileged requests to AWS. For example, Karpenter uses AWS IRSA roles to grant the permissions needed to describe EC2 instance types and create EC2 instances.\nOnce privileges are in place, Karpenter is deployed with a Helm chart.\nConfiguring provisioners Karpenter’s job is to add nodes to handle unschedulable pods, schedule pods on those nodes, and remove the nodes when they are not needed. To configure Karpenter, you create provisioners that define how Karpenter manages unschedulable pods and expires nodes. Here are some things to know about the Karpenter provisioner:\n  Unschedulable pods: Karpenter only attempts to schedule pods that have a status condition Unschedulable=True, which the kube scheduler sets when it fails to schedule the pod to existing capacity.\n  Provisioner CR: Karpenter defines a Custom Resource called a Provisioner to specify provisioning configuration. Each provisioner manages a distinct set of nodes, but pods can be scheduled to any provisioner that supports its scheduling constraints. A provisioner contains constraints that impact the nodes that can be provisioned and attributes of those nodes (such timers for removing nodes). See Provisioner API for a description of settings and the Provisioning task for provisioner examples.\n  Well-known labels: The provisioner can use well-known Kubernetes labels to allow pods to request only certain instance types, architectures, operating systems, or other attributes when creating nodes. See Well-Known Labels, Annotations and Taints for details. Keep in mind that only a subset of these labels are supported in Karpenter, as described later.\n  Deprovisioning nodes: A provisioner can also include time-to-live values to indicate when nodes should be deprovisioned after a set amount of time from when they were created or after they becomes empty of deployed pods.\n  Multiple provisioners: Multiple provisioners can be configured on the same cluster. For example, you might want to configure different teams on the same cluster to run on completely separate capacity. One team could run on nodes nodes using BottleRocket, while another uses EKSOptimizedAMI.\n  Although most use cases are addressed with a single provisioner for multiple teams, multiple provisioners are useful to isolate nodes for billing, use different node constraints (such as no GPUs for a team), or use different deprovisioning settings.\nDeprovisioning nodes Karpenter deletes nodes when they are no longer needed.\n Finalizer: Karpenter places a finalizer bit on each node it creates. When a request comes in to delete one of those nodes (such as a TTL or a manual kubectl delete node), Karpenter will cordon the node, drain all the pods, terminate the EC2 instance, and delete the node object. Karpenter handles all clean-up work needed to properly delete the node. Node Expiry: If a node expiry time-to-live value (ttlSecondsUntilExpired) is reached, that node is drained of pods and deleted (even if it is still running workloads). Empty nodes: When the last workload pod running on a Karpenter-managed node is gone, the node is annotated with an emptiness timestamp. Once that “node empty” time-to-live (ttlSecondsAfterEmpty) is reached, finalization is triggered.  For more details on how Karpenter deletes nodes, see Deprovisioning nodes for details.\nUpgrading nodes A straight-forward way to upgrade nodes is to set ttlSecondsUntilExpired. Nodes will be terminated after a set period of time and will be replaced with newer nodes.\nUnderstanding the following concepts will help you in carrying out the tasks just described.\nConstraints The concept of layered constraints is key to using Karpenter. With no constraints defined in provisioners and none requested from pods being deployed, Karpenter chooses from the entire universe of features available to your cloud provider. Nodes can be created using any instance type and run in any zones.\nAn application developer can tighten the constraints defined in a provisioner by the cluster administrator by defining additional scheduling constraints in their pod spec. Refer to the description of Karpenter constraints in the Application Developer section below for details.\nScheduling Karpenter schedules pods that the Kubernetes scheduler has marked unschedulable. After solving scheduling constraints and launching capacity, Karpenter optimistically creates the Node object and binds the pod. This stateless approach helps to avoid race conditions and improves performance. If something is wrong with the launched node, Kubernetes will automatically migrate the pods to a new node.\nOnce Karpenter brings up a node, that node is available for the Kubernetes scheduler to schedule pods on it as well. This is useful if there is additional room in the node due to imperfect packing shape or because workloads finish over time.\nCloud provider Karpenter makes requests to provision new nodes to the associated cloud provider. The first supported cloud provider is AWS, although Karpenter is designed to work with other cloud providers. Separating Kubernetes and AWS-specific settings allows Karpenter a clean path to integrating with other cloud providers.\nWhile using Kubernetes well-known labels, the provisioner can set some values that are specific to the cloud provider. So, for example, to include a certain instance type, you could use the Kubernetes label node.kubernetes.io/instance-type, but set its value to an AWS instance type (such as m5.large or m5.2xlarge).\nKubernetes cluster autoscaler Like Karpenter, Kubernetes Cluster Autoscaler is designed to add nodes when requests come in to run pods that cannot be met by current capacity. Cluster autoscaler is part of the Kubernetes project, with implementations by most major Kubernetes cloud providers. By taking a fresh look at provisioning, Karpenter offers the following improvements:\n  Designed to handle the full flexibility of the cloud: Karpenter has the ability to efficiently address the full range of instance types available through AWS. Cluster autoscaler was not originally built with the flexibility to handle hundreds of instance types, zones, and purchase options.\n  Group-less node provisioning: Karpenter manages each instance directly, without use of additional orchestration mechanisms like node groups. This enables it to retry in milliseconds instead of minutes when capacity is unavailable. It also allows Karpenter to leverage diverse instance types, availability zones, and purchase options without the creation of hundreds of node groups.\n  Scheduling enforcement: Cluster autoscaler doesn’t bind pods to the nodes it creates. Instead, it relies on the kube-scheduler to make the same scheduling decision after the node has come online. A node that Karpenter launches has its pods bound immediately. The kubelet doesn’t have to wait for the scheduler or for the node to become ready. It can start preparing the container runtime immediately, including pre-pulling the image. This can shave seconds off of node startup latency.\n  Application developer As someone deploying pods that might be evaluated by Karpenter, you should know how to request the properties that your pods need of its compute resources. Karpenter’s job is to efficiently assess and choose compute assets based on requests from pod deployments. These can include basic Kubernetes features or features that are specific to the cloud provider (such as AWS).\nLayered constraints are applied when a pod makes requests for compute resources that cannot be met by current capacity. A pod can specify nodeAffinity (to run in a particular zone or instance type) or a topologySpreadConstraints spread (to cause a set of pods to be balanced across multiple nodes). The pod can specify a nodeSelector to run only on nodes with a particular label and resource.requests to ensure that the node has enough available memory.\nThe Kubernetes scheduler tries to match those constraints with available nodes. If the pod is unschedulable, Karpenter creates compute resources that match its needs. When Karpenter tries to provision a node, it analyzes scheduling constraints before choosing the node to create.\nAs long as the requests are not outside of the provisioner’s constraints, Karpenter will look to best match the request, comparing the same well-known labels defined by the pod’s scheduling constraints. Note that if the constraints are such that a match is not possible, the pod will remain unscheduled.\nSo, what constraints can you use as an application developer deploying pods that could be managed by Karpenter?\nKubernetes features that Karpenter supports for scheduling pods include nodeAffinity and nodeSelector. It also supports PodDisruptionBudget and topologySpreadConstraints.\nFrom the Kubernetes Well-Known Labels, Annotations and Taints page, you can see a full list of Kubernetes labels, annotations and taints that determine scheduling. Those that are implemented in Karpenter include:\n kubernetes.io/arch: For example, kubernetes.io/arch=amd64 node.kubernetes.io/instance-type: For example, node.kubernetes.io/instance-type=m3.medium topology.kubernetes.io/zone: For example, topology.kubernetes.io/zone=us-east-1c  Note Don’t use podAffinity and podAntiAffinity to schedule pods on the same or different nodes as other pods. Kubernetes SIG scalability recommends against these features due to their negative performance impact on the Kubernetes Scheduler (see KEP 895) and Karpenter doesn’t support them for the moment (you can follow their consideration by subscribing to the issue).\". Instead, the Karpenter project recommends topologySpreadConstraints to reduce blast radius and nodeSelectors and taints to implement colocation.  For more on how, as a developer, you can add constraints to your pod deployment, see Running pods for details.\n","categories":"","description":"","excerpt":"Users fall under two basic roles: Kubernetes cluster administrators …","ref":"/preview/concepts/","tags":"","title":"Concepts"},{"body":"Users fall under two basic roles: Kubernetes cluster operators and application developers. This document describes Karpenter concepts through the lens of those two types of users.\nCluster operator As a Kubernetes cluster operator, you can engage with Karpenter to:\n Install Karpenter Configure provisioners to set constraints and other features for managing nodes Deprovision nodes Upgrade nodes  Concepts associated with this role are described below.\nInstalling Karpenter Karpenter is designed to run on a node in your Kubernetes cluster. As part of the installation process, you need credentials from the underlying cloud provider to allow nodes to be started up and added to the cluster as they are needed.\nGetting Started with Karpenter on AWS describes the process of installing Karpenter on an AWS cloud provider. Because requests to add and delete nodes and schedule pods are made through Kubernetes, AWS IAM Roles for Service Accounts (IRSA) are needed by your Kubernetes cluster to make privileged requests to AWS. For example, Karpenter uses AWS IRSA roles to grant the permissions needed to describe EC2 instance types and create EC2 instances.\nOnce privileges are in place, Karpenter is deployed with a Helm chart.\nConfiguring provisioners Karpenter’s job is to add nodes to handle unschedulable pods, schedule pods on those nodes, and remove the nodes when they are not needed. To configure Karpenter, you create provisioners that define how Karpenter manages unschedulable pods and expires nodes. Here are some things to know about the Karpenter provisioner:\n  Unschedulable pods: Karpenter only attempts to schedule pods that have a status condition Unschedulable=True, which the kube scheduler sets when it fails to schedule the pod to existing capacity.\n  Provisioner CR: Karpenter defines a Custom Resource called a Provisioner to specify provisioning configuration. Each provisioner manages a distinct set of nodes, but pods can be scheduled to any provisioner that supports its scheduling constraints. A provisioner contains constraints that impact the nodes that can be provisioned and attributes of those nodes (such timers for removing nodes). See Provisioner for a description of settings and the Provisioning task for of provisioner examples.\n  Well-known labels: The provisioner can use well-known Kubernetes labels to allow pods to request only certain instance types, architectures, operating systems, or other attributes when creating nodes. See Well-Known Labels, Annotations and Taints for details. Keep in mind that only a subset of these labels are supported in Karpenter, as described later.\n  Deprovisioning nodes: A provisioner can also include time-to-live values to indicate when nodes should be deprovisioned after a set amount of time from when they were created or after they becomes empty of deployed pods.\n  Multiple provisioners: Multiple provisioners can be configured on the same cluster. For example, you might want to configure different teams on the same cluster to run on completely separate capacity. One team could run on nodes nodes using BottleRocket, while another uses EKSOptimizedAMI.\n  Although most use cases are addressed with a single provisioner for multiple teams, multiple provisioners are useful to isolate nodes for billing, use different node constraints (such as no GPUs for a team), or use different deprovisioning settings.\nDeprovisioning nodes Karpenter deletes nodes when they are no longer needed.\n Finalizer: Karpenter places a finalizer bit on each node it creates. When a request comes in to delete one of those nodes (such as a TTL or a manual kubectl delete node), Karpenter will cordon the node, drain all the pods, terminate the EC2 instance, and delete the node object. Karpenter handles all clean-up work needed to properly delete the node. Node Expiry: If a node expiry time-to-live value (ttlSecondsUntilExpired) is reached, that node is drained of pods and deleted (even if it is still running workloads). Empty nodes: When the last workload pod running on a Karpenter-managed node is gone, the node is annotated with an emptiness timestamp. Once that “node empty” time-to-live (ttlSecondsAfterEmpty) is reached, finalization is triggered.  For more details on how Karpenter deletes nodes, see Deleting nodes with Karpenter for details.\nUpgrading nodes A straight-forward way to upgrade nodes is to set ttlSecondsUntilExpired. Nodes will be terminated after a set period of time and will be replaced with newer nodes.\nFor details on upgrading nodes with Karpenter, see Upgrading nodes with Karpenter for details.\nUnderstanding the following concepts will help you in carrying out the tasks just described.\nConstraints The concept of layered constraints is key to using Karpenter. With no constraints defined in provisioners and none requested from pods being deployed, Karpenter chooses from the entire universe of features available to your cloud provider. Nodes can be created using any instance type and run in any zones.\nAn application developer can tighten the constraints defined in a provisioner by the cluster operator by defining additional scheduling constraints in their pod spec. Refer to the description of Karpenter constraints in the Application Developer section below for details.\nScheduling Karpenter schedules pods that the Kubernetes scheduler has marked unschedulable. After solving scheduling constraints and launching capacity, Karpenter optimistically creates the Node object and binds the pod. This stateless approach helps to avoid race conditions and improves performance. If something is wrong with the launched node, Kubernetes will automatically migrate the pods to a new node.\nOnce Karpenter brings up a node, that node is available for the Kubernetes scheduler to schedule pods on it as well. This is useful if there is additional room in the node due to imperfect packing shape or because workloads finish over time.\nCloud provider Karpenter makes requests to provision new nodes to the associated cloud provider. The first supported cloud provider is AWS, although Karpenter is designed to work with other cloud providers. Separating Kubernetes and AWS-specific settings allows Karpenter a clean path to integrating with other cloud providers.\nWhile using Kubernetes well-known labels, the provisioner can set some values that are specific to the cloud provider. So, for example, to include a certain instance type, you could use the Kubernetes label node.kubernetes.io/instance-type, but set its value to an AWS instance type (such as m5.large or m5.2xlarge).\nKubernetes cluster autoscaler Like Karpenter, Kubernetes Cluster Autoscaler is designed to add nodes when requests come in to run pods that cannot be met by current capacity. Cluster autoscaler is part of the Kubernetes project, with implementations by most major Kubernetes cloud providers. By taking a fresh look at provisioning, Karpenter offers the following improvements:\n  Designed to handle the full flexibility of the cloud: Karpenter has the ability to efficiently address the full range of instance types available through AWS. Cluster autoscaler was not originally built with the flexibility to handle hundreds of instance types, zones, and purchase options.\n  Group-less node provisioning: Karpenter manages each instance directly, without use of additional orchestration mechanisms like node groups. This enables it to retry in milliseconds instead of minutes when capacity is unavailable. It also allows Karpenter to leverage diverse instance types, availability zones, and purchase options without the creation of hundreds of node groups.\n  Scheduling enforcement: Cluster autoscaler doesn’t bind pods to the nodes it creates. Instead, it relies on the kube-scheduler to make the same scheduling decision after the node has come online. A node that Karpenter launches has its pods bound immediately. The kubelet doesn’t have to wait for the scheduler or for the node to become ready. It can start preparing the container runtime immediately, including pre-pulling the image. This can shave seconds off of node startup latency.\n  Application developer As someone deploying pods that might be evaluated by Karpenter, you should know how to request the properties that your pods need of its compute resources. Karpenter’s job is to efficiently assess and choose compute assets based on requests from pod deployments. These can include basic Kubernetes features or features that are specific to the cloud provider (such as AWS).\nLayered constraints are applied when a pod makes requests for compute resources that cannot be met by current capacity. A pod can specify nodeAffinity (to run in a particular zone or instance type) or a topologySpreadConstraints spread (to cause a set of pods be balanced across multiple nodes). The pod can specify a nodeSelector to run only on nodes with a particular label and resource.requests to ensure that the node has enough available memory.\nThe Kubernetes scheduler tries to match those constraints with available nodes. If the pod is unschedulable, Karpenter created compute resources that match its needs. When Karpenter tries to provision a node, it analyzes scheduling constraints before choosing the node to create.\nAs long as the requests are not outside of the provisioner’s constraints, Karpenter will look to best match the request, comparing the same well-known labels defined by the pod’s scheduling constraints. Note that if the constraints are such that a match is not possible, the pod will remain unscheduled.\nSo, what constraints can you use as an application developer deploying pods that could be managed by Karpenter?\nKubernetes features that Karpenter supports for scheduling pods include nodeAffinity and nodeSelector. It also supports PodDisruptionBudget and topologySpreadConstraints.\nFrom the Kubernetes Well-Known Labels, Annotations and Taints page, you can see a full list of Kubernetes labels, annotations and taints that determine scheduling. Only a small set of them are implemented in Karpenter, including:\n kubernetes.io/arch: For example, kubernetes.io/arch=amd64 node.kubernetes.io/instance-type: For example, node.kubernetes.io/instance-type=m3.medium topology.kubernetes.io/zone: For example, topology.kubernetes.io/zone=us-east-1c  Note Don’t use podAffinity and podAntiAffinity to schedule pods on the same or different nodes as other pods. Kubernetes SIG scalability recommends against these features and Karpenter doesn’t support them. Instead, the Karpenter project recommends topologySpreadConstraints to reduce blast radius and nodeSelectors and taints to implement colocation.  For more on how, as a developer, you can add constraints to your pod deployment, see Running pods for details.\n","categories":"","description":"","excerpt":"Users fall under two basic roles: Kubernetes cluster operators and …","ref":"/v0.4.3/concepts/","tags":"","title":"Concepts"},{"body":"Users fall under two basic roles: Kubernetes cluster operators and application developers. This document describes Karpenter concepts through the lens of those two types of users.\nCluster operator As a Kubernetes cluster operator, you can engage with Karpenter to:\n Install Karpenter Configure provisioners to set constraints and other features for managing nodes Deprovision nodes Upgrade nodes  Concepts associated with this role are described below.\nInstalling Karpenter Karpenter is designed to run on a node in your Kubernetes cluster. As part of the installation process, you need credentials from the underlying cloud provider to allow nodes to be started up and added to the cluster as they are needed.\nGetting Started with Karpenter on AWS describes the process of installing Karpenter on an AWS cloud provider. Because requests to add and delete nodes and schedule pods are made through Kubernetes, AWS IAM Roles for Service Accounts (IRSA) are needed by your Kubernetes cluster to make privileged requests to AWS. For example, Karpenter uses AWS IRSA roles to grant the permissions needed to describe EC2 instance types and create EC2 instances.\nOnce privileges are in place, Karpenter is deployed with a Helm chart.\nConfiguring provisioners Karpenter’s job is to add nodes to handle unschedulable pods, schedule pods on those nodes, and remove the nodes when they are not needed. To configure Karpenter, you create provisioners that define how Karpenter manages unschedulable pods and expires nodes. Here are some things to know about the Karpenter provisioner:\n  Unschedulable pods: Karpenter only attempts to schedule pods that have a status condition Unschedulable=True, which the kube scheduler sets when it fails to schedule the pod to existing capacity.\n  Provisioner CR: Karpenter defines a Custom Resource called a Provisioner to specify provisioning configuration. Each provisioner manages a distinct set of nodes, but pods can be scheduled to any provisioner that supports its scheduling constraints. A provisioner contains constraints that impact the nodes that can be provisioned and attributes of those nodes (such timers for removing nodes). See Provisioner API for a description of settings and the Provisioning task for provisioner examples.\n  Well-known labels: The provisioner can use well-known Kubernetes labels to allow pods to request only certain instance types, architectures, operating systems, or other attributes when creating nodes. See Well-Known Labels, Annotations and Taints for details. Keep in mind that only a subset of these labels are supported in Karpenter, as described later.\n  Deprovisioning nodes: A provisioner can also include time-to-live values to indicate when nodes should be deprovisioned after a set amount of time from when they were created or after they becomes empty of deployed pods.\n  Multiple provisioners: Multiple provisioners can be configured on the same cluster. For example, you might want to configure different teams on the same cluster to run on completely separate capacity. One team could run on nodes nodes using BottleRocket, while another uses EKSOptimizedAMI.\n  Although most use cases are addressed with a single provisioner for multiple teams, multiple provisioners are useful to isolate nodes for billing, use different node constraints (such as no GPUs for a team), or use different deprovisioning settings.\nDeprovisioning nodes Karpenter deletes nodes when they are no longer needed.\n Finalizer: Karpenter places a finalizer bit on each node it creates. When a request comes in to delete one of those nodes (such as a TTL or a manual kubectl delete node), Karpenter will cordon the node, drain all the pods, terminate the EC2 instance, and delete the node object. Karpenter handles all clean-up work needed to properly delete the node. Node Expiry: If a node expiry time-to-live value (ttlSecondsUntilExpired) is reached, that node is drained of pods and deleted (even if it is still running workloads). Empty nodes: When the last workload pod running on a Karpenter-managed node is gone, the node is annotated with an emptiness timestamp. Once that “node empty” time-to-live (ttlSecondsAfterEmpty) is reached, finalization is triggered.  For more details on how Karpenter deletes nodes, see Deprovisioning nodes for details.\nUpgrading nodes A straight-forward way to upgrade nodes is to set ttlSecondsUntilExpired. Nodes will be terminated after a set period of time and will be replaced with newer nodes.\nUnderstanding the following concepts will help you in carrying out the tasks just described.\nConstraints The concept of layered constraints is key to using Karpenter. With no constraints defined in provisioners and none requested from pods being deployed, Karpenter chooses from the entire universe of features available to your cloud provider. Nodes can be created using any instance type and run in any zones.\nAn application developer can tighten the constraints defined in a provisioner by the cluster operator by defining additional scheduling constraints in their pod spec. Refer to the description of Karpenter constraints in the Application Developer section below for details.\nScheduling Karpenter schedules pods that the Kubernetes scheduler has marked unschedulable. After solving scheduling constraints and launching capacity, Karpenter optimistically creates the Node object and binds the pod. This stateless approach helps to avoid race conditions and improves performance. If something is wrong with the launched node, Kubernetes will automatically migrate the pods to a new node.\nOnce Karpenter brings up a node, that node is available for the Kubernetes scheduler to schedule pods on it as well. This is useful if there is additional room in the node due to imperfect packing shape or because workloads finish over time.\nCloud provider Karpenter makes requests to provision new nodes to the associated cloud provider. The first supported cloud provider is AWS, although Karpenter is designed to work with other cloud providers. Separating Kubernetes and AWS-specific settings allows Karpenter a clean path to integrating with other cloud providers.\nWhile using Kubernetes well-known labels, the provisioner can set some values that are specific to the cloud provider. So, for example, to include a certain instance type, you could use the Kubernetes label node.kubernetes.io/instance-type, but set its value to an AWS instance type (such as m5.large or m5.2xlarge).\nKubernetes cluster autoscaler Like Karpenter, Kubernetes Cluster Autoscaler is designed to add nodes when requests come in to run pods that cannot be met by current capacity. Cluster autoscaler is part of the Kubernetes project, with implementations by most major Kubernetes cloud providers. By taking a fresh look at provisioning, Karpenter offers the following improvements:\n  Designed to handle the full flexibility of the cloud: Karpenter has the ability to efficiently address the full range of instance types available through AWS. Cluster autoscaler was not originally built with the flexibility to handle hundreds of instance types, zones, and purchase options.\n  Group-less node provisioning: Karpenter manages each instance directly, without use of additional orchestration mechanisms like node groups. This enables it to retry in milliseconds instead of minutes when capacity is unavailable. It also allows Karpenter to leverage diverse instance types, availability zones, and purchase options without the creation of hundreds of node groups.\n  Scheduling enforcement: Cluster autoscaler doesn’t bind pods to the nodes it creates. Instead, it relies on the kube-scheduler to make the same scheduling decision after the node has come online. A node that Karpenter launches has its pods bound immediately. The kubelet doesn’t have to wait for the scheduler or for the node to become ready. It can start preparing the container runtime immediately, including pre-pulling the image. This can shave seconds off of node startup latency.\n  Application developer As someone deploying pods that might be evaluated by Karpenter, you should know how to request the properties that your pods need of its compute resources. Karpenter’s job is to efficiently assess and choose compute assets based on requests from pod deployments. These can include basic Kubernetes features or features that are specific to the cloud provider (such as AWS).\nLayered constraints are applied when a pod makes requests for compute resources that cannot be met by current capacity. A pod can specify nodeAffinity (to run in a particular zone or instance type) or a topologySpreadConstraints spread (to cause a set of pods to be balanced across multiple nodes). The pod can specify a nodeSelector to run only on nodes with a particular label and resource.requests to ensure that the node has enough available memory.\nThe Kubernetes scheduler tries to match those constraints with available nodes. If the pod is unschedulable, Karpenter creates compute resources that match its needs. When Karpenter tries to provision a node, it analyzes scheduling constraints before choosing the node to create.\nAs long as the requests are not outside of the provisioner’s constraints, Karpenter will look to best match the request, comparing the same well-known labels defined by the pod’s scheduling constraints. Note that if the constraints are such that a match is not possible, the pod will remain unscheduled.\nSo, what constraints can you use as an application developer deploying pods that could be managed by Karpenter?\nKubernetes features that Karpenter supports for scheduling pods include nodeAffinity and nodeSelector. It also supports PodDisruptionBudget and topologySpreadConstraints.\nFrom the Kubernetes Well-Known Labels, Annotations and Taints page, you can see a full list of Kubernetes labels, annotations and taints that determine scheduling. Those that are implemented in Karpenter include:\n kubernetes.io/arch: For example, kubernetes.io/arch=amd64 node.kubernetes.io/instance-type: For example, node.kubernetes.io/instance-type=m3.medium topology.kubernetes.io/zone: For example, topology.kubernetes.io/zone=us-east-1c  Note Don’t use podAffinity and podAntiAffinity to schedule pods on the same or different nodes as other pods. Kubernetes SIG scalability recommends against these features due to their negative performance impact on the Kubernetes Scheduler (see KEP 895) and Karpenter doesn’t support them for the moment (you can follow their consideration by subscribing to the issue).\". Instead, the Karpenter project recommends topologySpreadConstraints to reduce blast radius and nodeSelectors and taints to implement colocation.  For more on how, as a developer, you can add constraints to your pod deployment, see Running pods for details.\n","categories":"","description":"","excerpt":"Users fall under two basic roles: Kubernetes cluster operators and …","ref":"/v0.5.0/concepts/","tags":"","title":"Concepts"},{"body":"Users fall under two basic roles: Kubernetes cluster operators and application developers. This document describes Karpenter concepts through the lens of those two types of users.\nCluster operator As a Kubernetes cluster operator, you can engage with Karpenter to:\n Install Karpenter Configure provisioners to set constraints and other features for managing nodes Deprovision nodes Upgrade nodes  Concepts associated with this role are described below.\nInstalling Karpenter Karpenter is designed to run on a node in your Kubernetes cluster. As part of the installation process, you need credentials from the underlying cloud provider to allow nodes to be started up and added to the cluster as they are needed.\nGetting Started with Karpenter on AWS describes the process of installing Karpenter on an AWS cloud provider. Because requests to add and delete nodes and schedule pods are made through Kubernetes, AWS IAM Roles for Service Accounts (IRSA) are needed by your Kubernetes cluster to make privileged requests to AWS. For example, Karpenter uses AWS IRSA roles to grant the permissions needed to describe EC2 instance types and create EC2 instances.\nOnce privileges are in place, Karpenter is deployed with a Helm chart.\nConfiguring provisioners Karpenter’s job is to add nodes to handle unschedulable pods, schedule pods on those nodes, and remove the nodes when they are not needed. To configure Karpenter, you create provisioners that define how Karpenter manages unschedulable pods and expires nodes. Here are some things to know about the Karpenter provisioner:\n  Unschedulable pods: Karpenter only attempts to schedule pods that have a status condition Unschedulable=True, which the kube scheduler sets when it fails to schedule the pod to existing capacity.\n  Provisioner CR: Karpenter defines a Custom Resource called a Provisioner to specify provisioning configuration. Each provisioner manages a distinct set of nodes, but pods can be scheduled to any provisioner that supports its scheduling constraints. A provisioner contains constraints that impact the nodes that can be provisioned and attributes of those nodes (such timers for removing nodes). See Provisioner API for a description of settings and the Provisioning task for provisioner examples.\n  Well-known labels: The provisioner can use well-known Kubernetes labels to allow pods to request only certain instance types, architectures, operating systems, or other attributes when creating nodes. See Well-Known Labels, Annotations and Taints for details. Keep in mind that only a subset of these labels are supported in Karpenter, as described later.\n  Deprovisioning nodes: A provisioner can also include time-to-live values to indicate when nodes should be deprovisioned after a set amount of time from when they were created or after they becomes empty of deployed pods.\n  Multiple provisioners: Multiple provisioners can be configured on the same cluster. For example, you might want to configure different teams on the same cluster to run on completely separate capacity. One team could run on nodes nodes using BottleRocket, while another uses EKSOptimizedAMI.\n  Although most use cases are addressed with a single provisioner for multiple teams, multiple provisioners are useful to isolate nodes for billing, use different node constraints (such as no GPUs for a team), or use different deprovisioning settings.\nDeprovisioning nodes Karpenter deletes nodes when they are no longer needed.\n Finalizer: Karpenter places a finalizer bit on each node it creates. When a request comes in to delete one of those nodes (such as a TTL or a manual kubectl delete node), Karpenter will cordon the node, drain all the pods, terminate the EC2 instance, and delete the node object. Karpenter handles all clean-up work needed to properly delete the node. Node Expiry: If a node expiry time-to-live value (ttlSecondsUntilExpired) is reached, that node is drained of pods and deleted (even if it is still running workloads). Empty nodes: When the last workload pod running on a Karpenter-managed node is gone, the node is annotated with an emptiness timestamp. Once that “node empty” time-to-live (ttlSecondsAfterEmpty) is reached, finalization is triggered.  For more details on how Karpenter deletes nodes, see Deprovisioning nodes for details.\nUpgrading nodes A straight-forward way to upgrade nodes is to set ttlSecondsUntilExpired. Nodes will be terminated after a set period of time and will be replaced with newer nodes.\nUnderstanding the following concepts will help you in carrying out the tasks just described.\nConstraints The concept of layered constraints is key to using Karpenter. With no constraints defined in provisioners and none requested from pods being deployed, Karpenter chooses from the entire universe of features available to your cloud provider. Nodes can be created using any instance type and run in any zones.\nAn application developer can tighten the constraints defined in a provisioner by the cluster operator by defining additional scheduling constraints in their pod spec. Refer to the description of Karpenter constraints in the Application Developer section below for details.\nScheduling Karpenter schedules pods that the Kubernetes scheduler has marked unschedulable. After solving scheduling constraints and launching capacity, Karpenter optimistically creates the Node object and binds the pod. This stateless approach helps to avoid race conditions and improves performance. If something is wrong with the launched node, Kubernetes will automatically migrate the pods to a new node.\nOnce Karpenter brings up a node, that node is available for the Kubernetes scheduler to schedule pods on it as well. This is useful if there is additional room in the node due to imperfect packing shape or because workloads finish over time.\nCloud provider Karpenter makes requests to provision new nodes to the associated cloud provider. The first supported cloud provider is AWS, although Karpenter is designed to work with other cloud providers. Separating Kubernetes and AWS-specific settings allows Karpenter a clean path to integrating with other cloud providers.\nWhile using Kubernetes well-known labels, the provisioner can set some values that are specific to the cloud provider. So, for example, to include a certain instance type, you could use the Kubernetes label node.kubernetes.io/instance-type, but set its value to an AWS instance type (such as m5.large or m5.2xlarge).\nKubernetes cluster autoscaler Like Karpenter, Kubernetes Cluster Autoscaler is designed to add nodes when requests come in to run pods that cannot be met by current capacity. Cluster autoscaler is part of the Kubernetes project, with implementations by most major Kubernetes cloud providers. By taking a fresh look at provisioning, Karpenter offers the following improvements:\n  Designed to handle the full flexibility of the cloud: Karpenter has the ability to efficiently address the full range of instance types available through AWS. Cluster autoscaler was not originally built with the flexibility to handle hundreds of instance types, zones, and purchase options.\n  Group-less node provisioning: Karpenter manages each instance directly, without use of additional orchestration mechanisms like node groups. This enables it to retry in milliseconds instead of minutes when capacity is unavailable. It also allows Karpenter to leverage diverse instance types, availability zones, and purchase options without the creation of hundreds of node groups.\n  Scheduling enforcement: Cluster autoscaler doesn’t bind pods to the nodes it creates. Instead, it relies on the kube-scheduler to make the same scheduling decision after the node has come online. A node that Karpenter launches has its pods bound immediately. The kubelet doesn’t have to wait for the scheduler or for the node to become ready. It can start preparing the container runtime immediately, including pre-pulling the image. This can shave seconds off of node startup latency.\n  Application developer As someone deploying pods that might be evaluated by Karpenter, you should know how to request the properties that your pods need of its compute resources. Karpenter’s job is to efficiently assess and choose compute assets based on requests from pod deployments. These can include basic Kubernetes features or features that are specific to the cloud provider (such as AWS).\nLayered constraints are applied when a pod makes requests for compute resources that cannot be met by current capacity. A pod can specify nodeAffinity (to run in a particular zone or instance type) or a topologySpreadConstraints spread (to cause a set of pods to be balanced across multiple nodes). The pod can specify a nodeSelector to run only on nodes with a particular label and resource.requests to ensure that the node has enough available memory.\nThe Kubernetes scheduler tries to match those constraints with available nodes. If the pod is unschedulable, Karpenter creates compute resources that match its needs. When Karpenter tries to provision a node, it analyzes scheduling constraints before choosing the node to create.\nAs long as the requests are not outside of the provisioner’s constraints, Karpenter will look to best match the request, comparing the same well-known labels defined by the pod’s scheduling constraints. Note that if the constraints are such that a match is not possible, the pod will remain unscheduled.\nSo, what constraints can you use as an application developer deploying pods that could be managed by Karpenter?\nKubernetes features that Karpenter supports for scheduling pods include nodeAffinity and nodeSelector. It also supports PodDisruptionBudget and topologySpreadConstraints.\nFrom the Kubernetes Well-Known Labels, Annotations and Taints page, you can see a full list of Kubernetes labels, annotations and taints that determine scheduling. Those that are implemented in Karpenter include:\n kubernetes.io/arch: For example, kubernetes.io/arch=amd64 node.kubernetes.io/instance-type: For example, node.kubernetes.io/instance-type=m3.medium topology.kubernetes.io/zone: For example, topology.kubernetes.io/zone=us-east-1c  Note Don’t use podAffinity and podAntiAffinity to schedule pods on the same or different nodes as other pods. Kubernetes SIG scalability recommends against these features due to their negative performance impact on the Kubernetes Scheduler (see KEP 895) and Karpenter doesn’t support them for the moment (you can follow their consideration by subscribing to the issue).\". Instead, the Karpenter project recommends topologySpreadConstraints to reduce blast radius and nodeSelectors and taints to implement colocation.  For more on how, as a developer, you can add constraints to your pod deployment, see Running pods for details.\n","categories":"","description":"","excerpt":"Users fall under two basic roles: Kubernetes cluster operators and …","ref":"/v0.5.2/concepts/","tags":"","title":"Concepts"},{"body":"Users fall under two basic roles: Kubernetes cluster operators and application developers. This document describes Karpenter concepts through the lens of those two types of users.\nCluster operator As a Kubernetes cluster operator, you can engage with Karpenter to:\n Install Karpenter Configure provisioners to set constraints and other features for managing nodes Deprovision nodes Upgrade nodes  Concepts associated with this role are described below.\nInstalling Karpenter Karpenter is designed to run on a node in your Kubernetes cluster. As part of the installation process, you need credentials from the underlying cloud provider to allow nodes to be started up and added to the cluster as they are needed.\nGetting Started with Karpenter on AWS describes the process of installing Karpenter on an AWS cloud provider. Because requests to add and delete nodes and schedule pods are made through Kubernetes, AWS IAM Roles for Service Accounts (IRSA) are needed by your Kubernetes cluster to make privileged requests to AWS. For example, Karpenter uses AWS IRSA roles to grant the permissions needed to describe EC2 instance types and create EC2 instances.\nOnce privileges are in place, Karpenter is deployed with a Helm chart.\nConfiguring provisioners Karpenter’s job is to add nodes to handle unschedulable pods, schedule pods on those nodes, and remove the nodes when they are not needed. To configure Karpenter, you create provisioners that define how Karpenter manages unschedulable pods and expires nodes. Here are some things to know about the Karpenter provisioner:\n  Unschedulable pods: Karpenter only attempts to schedule pods that have a status condition Unschedulable=True, which the kube scheduler sets when it fails to schedule the pod to existing capacity.\n  Provisioner CR: Karpenter defines a Custom Resource called a Provisioner to specify provisioning configuration. Each provisioner manages a distinct set of nodes, but pods can be scheduled to any provisioner that supports its scheduling constraints. A provisioner contains constraints that impact the nodes that can be provisioned and attributes of those nodes (such timers for removing nodes). See Provisioner API for a description of settings and the Provisioning task for provisioner examples.\n  Well-known labels: The provisioner can use well-known Kubernetes labels to allow pods to request only certain instance types, architectures, operating systems, or other attributes when creating nodes. See Well-Known Labels, Annotations and Taints for details. Keep in mind that only a subset of these labels are supported in Karpenter, as described later.\n  Deprovisioning nodes: A provisioner can also include time-to-live values to indicate when nodes should be deprovisioned after a set amount of time from when they were created or after they becomes empty of deployed pods.\n  Multiple provisioners: Multiple provisioners can be configured on the same cluster. For example, you might want to configure different teams on the same cluster to run on completely separate capacity. One team could run on nodes nodes using BottleRocket, while another uses EKSOptimizedAMI.\n  Although most use cases are addressed with a single provisioner for multiple teams, multiple provisioners are useful to isolate nodes for billing, use different node constraints (such as no GPUs for a team), or use different deprovisioning settings.\nDeprovisioning nodes Karpenter deletes nodes when they are no longer needed.\n Finalizer: Karpenter places a finalizer bit on each node it creates. When a request comes in to delete one of those nodes (such as a TTL or a manual kubectl delete node), Karpenter will cordon the node, drain all the pods, terminate the EC2 instance, and delete the node object. Karpenter handles all clean-up work needed to properly delete the node. Node Expiry: If a node expiry time-to-live value (ttlSecondsUntilExpired) is reached, that node is drained of pods and deleted (even if it is still running workloads). Empty nodes: When the last workload pod running on a Karpenter-managed node is gone, the node is annotated with an emptiness timestamp. Once that “node empty” time-to-live (ttlSecondsAfterEmpty) is reached, finalization is triggered.  For more details on how Karpenter deletes nodes, see Deprovisioning nodes for details.\nUpgrading nodes A straight-forward way to upgrade nodes is to set ttlSecondsUntilExpired. Nodes will be terminated after a set period of time and will be replaced with newer nodes.\nUnderstanding the following concepts will help you in carrying out the tasks just described.\nConstraints The concept of layered constraints is key to using Karpenter. With no constraints defined in provisioners and none requested from pods being deployed, Karpenter chooses from the entire universe of features available to your cloud provider. Nodes can be created using any instance type and run in any zones.\nAn application developer can tighten the constraints defined in a provisioner by the cluster operator by defining additional scheduling constraints in their pod spec. Refer to the description of Karpenter constraints in the Application Developer section below for details.\nScheduling Karpenter schedules pods that the Kubernetes scheduler has marked unschedulable. After solving scheduling constraints and launching capacity, Karpenter optimistically creates the Node object and binds the pod. This stateless approach helps to avoid race conditions and improves performance. If something is wrong with the launched node, Kubernetes will automatically migrate the pods to a new node.\nOnce Karpenter brings up a node, that node is available for the Kubernetes scheduler to schedule pods on it as well. This is useful if there is additional room in the node due to imperfect packing shape or because workloads finish over time.\nCloud provider Karpenter makes requests to provision new nodes to the associated cloud provider. The first supported cloud provider is AWS, although Karpenter is designed to work with other cloud providers. Separating Kubernetes and AWS-specific settings allows Karpenter a clean path to integrating with other cloud providers.\nWhile using Kubernetes well-known labels, the provisioner can set some values that are specific to the cloud provider. So, for example, to include a certain instance type, you could use the Kubernetes label node.kubernetes.io/instance-type, but set its value to an AWS instance type (such as m5.large or m5.2xlarge).\nKubernetes cluster autoscaler Like Karpenter, Kubernetes Cluster Autoscaler is designed to add nodes when requests come in to run pods that cannot be met by current capacity. Cluster autoscaler is part of the Kubernetes project, with implementations by most major Kubernetes cloud providers. By taking a fresh look at provisioning, Karpenter offers the following improvements:\n  Designed to handle the full flexibility of the cloud: Karpenter has the ability to efficiently address the full range of instance types available through AWS. Cluster autoscaler was not originally built with the flexibility to handle hundreds of instance types, zones, and purchase options.\n  Group-less node provisioning: Karpenter manages each instance directly, without use of additional orchestration mechanisms like node groups. This enables it to retry in milliseconds instead of minutes when capacity is unavailable. It also allows Karpenter to leverage diverse instance types, availability zones, and purchase options without the creation of hundreds of node groups.\n  Scheduling enforcement: Cluster autoscaler doesn’t bind pods to the nodes it creates. Instead, it relies on the kube-scheduler to make the same scheduling decision after the node has come online. A node that Karpenter launches has its pods bound immediately. The kubelet doesn’t have to wait for the scheduler or for the node to become ready. It can start preparing the container runtime immediately, including pre-pulling the image. This can shave seconds off of node startup latency.\n  Application developer As someone deploying pods that might be evaluated by Karpenter, you should know how to request the properties that your pods need of its compute resources. Karpenter’s job is to efficiently assess and choose compute assets based on requests from pod deployments. These can include basic Kubernetes features or features that are specific to the cloud provider (such as AWS).\nLayered constraints are applied when a pod makes requests for compute resources that cannot be met by current capacity. A pod can specify nodeAffinity (to run in a particular zone or instance type) or a topologySpreadConstraints spread (to cause a set of pods to be balanced across multiple nodes). The pod can specify a nodeSelector to run only on nodes with a particular label and resource.requests to ensure that the node has enough available memory.\nThe Kubernetes scheduler tries to match those constraints with available nodes. If the pod is unschedulable, Karpenter creates compute resources that match its needs. When Karpenter tries to provision a node, it analyzes scheduling constraints before choosing the node to create.\nAs long as the requests are not outside of the provisioner’s constraints, Karpenter will look to best match the request, comparing the same well-known labels defined by the pod’s scheduling constraints. Note that if the constraints are such that a match is not possible, the pod will remain unscheduled.\nSo, what constraints can you use as an application developer deploying pods that could be managed by Karpenter?\nKubernetes features that Karpenter supports for scheduling pods include nodeAffinity and nodeSelector. It also supports PodDisruptionBudget and topologySpreadConstraints.\nFrom the Kubernetes Well-Known Labels, Annotations and Taints page, you can see a full list of Kubernetes labels, annotations and taints that determine scheduling. Those that are implemented in Karpenter include:\n kubernetes.io/arch: For example, kubernetes.io/arch=amd64 node.kubernetes.io/instance-type: For example, node.kubernetes.io/instance-type=m3.medium topology.kubernetes.io/zone: For example, topology.kubernetes.io/zone=us-east-1c  Note Don’t use podAffinity and podAntiAffinity to schedule pods on the same or different nodes as other pods. Kubernetes SIG scalability recommends against these features due to their negative performance impact on the Kubernetes Scheduler (see KEP 895) and Karpenter doesn’t support them for the moment (you can follow their consideration by subscribing to the issue).\". Instead, the Karpenter project recommends topologySpreadConstraints to reduce blast radius and nodeSelectors and taints to implement colocation.  For more on how, as a developer, you can add constraints to your pod deployment, see Running pods for details.\n","categories":"","description":"","excerpt":"Users fall under two basic roles: Kubernetes cluster operators and …","ref":"/v0.5.3/concepts/","tags":"","title":"Concepts"},{"body":"Example Provisioner Resource apiVersion:karpenter.sh/v1alpha5kind:Provisionermetadata:name:defaultspec:# If nil, the feature is disabled, nodes will never expirettlSecondsUntilExpired:2592000# 30 Days = 60 * 60 * 24 * 30 Seconds;# If nil, the feature is disabled, nodes will never scale down due to low utilizationttlSecondsAfterEmpty:30# Provisioned nodes will have these taints# Taints may prevent pods from scheduling if they are not toleratedtaints:- key:example.com/special-tainteffect:NoSchedule# Labels are arbitrary key-values that are applied to all nodeslabels:billing-team:my-team# Requirements that constrain the parameters of provisioned nodes.# These requirements are combined with pod.spec.affinity.nodeAffinity rules.# Operators { In, NotIn } are supported to enable including or excluding valuesrequirements:- key:\"node.kubernetes.io/instance-type\"# If not included, all instance types are consideredoperator:Invalues:[\"m5.large\",\"m5.2xlarge\"]- key:\"topology.kubernetes.io/zone\"# If not included, all zones are consideredoperator:Invalues:[\"us-west-2a\",\"us-west-2b\"]- key:\"kubernetes.io/arch\"# If not included, all architectures are consideredoperator:Invalues:[\"arm64\",\"amd64\"]- key:\"kubernetes.io/os\"# If not included, all operating systems are consideredoperator:Invalues:[\"linux\"]- key:\"karpenter.sh/capacity-type\"# If not included, the webhook for the AWS cloud provider will default to on-demandoperator:Invalues:[\"spot\",\"on-demand\"]# These fields vary per cloud provider, see your cloud provider specific documentationprovider:{}","categories":"","description":"","excerpt":"Example Provisioner Resource …","ref":"/v0.4.3/provisioner-crd/","tags":"","title":"Provisioner CRD"},{"body":"Karpenter tasks can be divided into those for a cluster administrator who is managing the cluster itself and application developers who are deploying pod workloads on a cluster.\n","categories":"","description":"","excerpt":"Karpenter tasks can be divided into those for a cluster administrator …","ref":"/preview/tasks/","tags":"","title":"Tasks"},{"body":"Karpenter tasks can be divided into those for a cluster operator who is managing the cluster itself and application developers who are deploying pod workloads on a cluster.\n","categories":"","description":"","excerpt":"Karpenter tasks can be divided into those for a cluster operator who …","ref":"/v0.5.0/tasks/","tags":"","title":"Tasks"},{"body":"Karpenter tasks can be divided into those for a cluster operator who is managing the cluster itself and application developers who are deploying pod workloads on a cluster.\n","categories":"","description":"","excerpt":"Karpenter tasks can be divided into those for a cluster operator who …","ref":"/v0.5.2/tasks/","tags":"","title":"Tasks"},{"body":"Karpenter tasks can be divided into those for a cluster operator who is managing the cluster itself and application developers who are deploying pod workloads on a cluster.\n","categories":"","description":"","excerpt":"Karpenter tasks can be divided into those for a cluster operator who …","ref":"/v0.5.3/tasks/","tags":"","title":"Tasks"},{"body":"Example Provisioner Resource apiVersion:karpenter.sh/v1alpha5kind:Provisionermetadata:name:defaultspec:# If nil, the feature is disabled, nodes will never expirettlSecondsUntilExpired:2592000# 30 Days = 60 * 60 * 24 * 30 Seconds;# If nil, the feature is disabled, nodes will never scale down due to low utilizationttlSecondsAfterEmpty:30# Provisioned nodes will have these taints# Taints may prevent pods from scheduling if they are not toleratedtaints:- key:example.com/special-tainteffect:NoSchedule# Labels are arbitrary key-values that are applied to all nodeslabels:billing-team:my-team# Requirements that constrain the parameters of provisioned nodes.# These requirements are combined with pod.spec.affinity.nodeAffinity rules.# Operators { In, NotIn } are supported to enable including or excluding valuesrequirements:- key:\"node.kubernetes.io/instance-type\"operator:Invalues:[\"m5.large\",\"m5.2xlarge\"]- key:\"topology.kubernetes.io/zone\"operator:Invalues:[\"us-west-2a\",\"us-west-2b\"]- key:\"kubernetes.io/arch\"operator:Invalues:[\"arm64\",\"amd64\"]- key:\"karpenter.sh/capacity-type\"# If not included, the webhook for the AWS cloud provider will default to on-demandoperator:Invalues:[\"spot\",\"on-demand\"]# These fields vary per cloud provider, see your cloud provider specific documentationprovider:{}spec.requirements Kubernetes defines the following Well-Known Labels, and cloud providers (e.g., AWS) implement them. They are defined at the “spec.requirements” section of the Provisioner API.\nThese well known labels may be specified at the provisioner level, or in a workload definition (e.g., nodeSelector on a pod.spec). Nodes are chosen using the both the provisioner’s and pod’s requirements. If there is no overlap, nodes will not be launched. In other words, a pod’s requirements must be within the provisioner’s requirements. If a requirement is not defined for a well known label, any value available to the cloud provider may be chosen.\nFor example, an instance type may be specified using a nodeSelector in a pod spec. If the instance type requested is not included in the provisioner list and the provisioner has instance type requirements, Karpenter will not create a node or schedule the pod.\n📝 None of these values are required.\nInstance Types  key: node.kubernetes.io/instance-type  Generally, instance types should be a list and not a single value. Leaving this field undefined is recommended, as it maximizes choices for efficiently placing pods.\n☁️ AWS\nReview AWS instance types.\nThe default value includes all instance types with the exclusion of metal (non-virtualized), non-HVM, and GPU instances.\nView the full list of instance types with aws ec2 describe-instance-types.\nExample\nSet Default with provisioner.yaml\nspec:requirements:- key:node.kubernetes.io/instance-typeoperator:Invalues:[\"m5.large\",\"m5.2xlarge\"]Override with workload manifest (e.g., pod)\nspec:template:spec:nodeSelector:node.kubernetes.io/instance-type:m5.largeAvailability Zones  key: topology.kubernetes.io/zone value example: us-east-1c  ☁️ AWS\n value list: aws ec2 describe-availability-zones --region \u003cregion-name\u003e  Karpenter can be configured to create nodes in a particular zone. Note that the Availability Zone us-east-1a for your AWS account might not have the same location as us-east-1a for another AWS account.\nLearn more about Availability Zone IDs.\nArchitecture  key: kubernetes.io/arch values  amd64 (default) arm64    Karpenter supports amd64 nodes, and arm64 nodes.\nCapacity Type  key: karpenter.sh/capacity-type  ☁️ AWS\n values  spot (default) on-demand    Karpenter supports specifying capacity type, which is analogous to EC2 purchase options.\nspec.kubeletConfiguration Karpenter provides the ability to specify a few additional Kubelet args. These are all optional and provide support for additional customization and use cases. Adjust these only if you know you need to do so.\nspec:kubeletConfiguration:clusterDNS:[\"10.0.1.100\"]spec.provider This section is cloud provider specific. Reference the appropriate documentation:\n AWS  ","categories":"","description":"Provisioner API reference page\n","excerpt":"Provisioner API reference page\n","ref":"/preview/provisioner/","tags":"","title":"Provisioner API"},{"body":"Example Provisioner Resource apiVersion:karpenter.sh/v1alpha5kind:Provisionermetadata:name:defaultspec:# If nil, the feature is disabled, nodes will never expirettlSecondsUntilExpired:2592000# 30 Days = 60 * 60 * 24 * 30 Seconds;# If nil, the feature is disabled, nodes will never scale down due to low utilizationttlSecondsAfterEmpty:30# Provisioned nodes will have these taints# Taints may prevent pods from scheduling if they are not toleratedtaints:- key:example.com/special-tainteffect:NoSchedule# Labels are arbitrary key-values that are applied to all nodeslabels:billing-team:my-team# Requirements that constrain the parameters of provisioned nodes.# These requirements are combined with pod.spec.affinity.nodeAffinity rules.# Operators { In, NotIn } are supported to enable including or excluding valuesrequirements:- key:\"node.kubernetes.io/instance-type\"operator:Invalues:[\"m5.large\",\"m5.2xlarge\"]- key:\"topology.kubernetes.io/zone\"operator:Invalues:[\"us-west-2a\",\"us-west-2b\"]- key:\"kubernetes.io/arch\"operator:Invalues:[\"arm64\",\"amd64\"]- key:\"karpenter.sh/capacity-type\"# If not included, the webhook for the AWS cloud provider will default to on-demandoperator:Invalues:[\"spot\",\"on-demand\"]# These fields vary per cloud provider, see your cloud provider specific documentationprovider:{}spec.requirements Kubernetes defines the following Well-Known Labels, and cloud providers (e.g., AWS) implement them. They are defined at the “spec.requirements” section of the Provisioner API.\nThese well known labels may be specified at the provisioner level, or in a workload definition (e.g., nodeSelector on a pod.spec). Nodes are chosen using the both the provisioner’s and pod’s requirements. If there is no overlap, nodes will not be launched. In other words, a pod’s requirements must be within the provisioner’s requirements. If a requirement is not defined for a well known label, any value available to the cloud provider may be chosen.\nFor example, an instance type may be specified using a nodeSelector in a pod spec. If the instance type requested is not included in the provisioner list and the provisioner has instance type requirements, Karpenter will not create a node or schedule the pod.\n📝 None of these values are required.\nInstance Types  key: node.kubernetes.io/instance-type  Generally, instance types should be a list and not a single value. Leaving this field undefined is recommended, as it maximizes choices for efficiently placing pods.\n☁️ AWS\nReview AWS instance types.\nThe default value includes all instance types with the exclusion of metal (non-virtualized), non-HVM, and GPU instances.\nView the full list of instance types with aws ec2 describe-instance-types.\nExample\nSet Default with provisioner.yaml\nspec:requirements:- key:node.kubernetes.io/instance-typeoperator:Invalues:[\"m5.large\",\"m5.2xlarge\"]Override with workload manifest (e.g., pod)\nspec:template:spec:nodeSelector:node.kubernetes.io/instance-type:m5.largeAvailability Zones  key: topology.kubernetes.io/zone value example: us-east-1c  ☁️ AWS\n value list: aws ec2 describe-availability-zones --region \u003cregion-name\u003e  Karpenter can be configured to create nodes in a particular zone. Note that the Availability Zone us-east-1a for your AWS account might not have the same location as us-east-1a for another AWS account.\nLearn more about Availability Zone IDs.\nArchitecture  key: kubernetes.io/arch values  amd64 (default) arm64    Karpenter supports amd64 nodes, and arm64 nodes.\nCapacity Type  key: karpenter.sh/capacity-type  ☁️ AWS\n values  spot (default) on-demand    Karpenter supports specifying capacity type, which is analogous to EC2 purchase options.\nspec.provider This section is cloud provider specific. Reference the appropriate documentation:\n AWS  ","categories":"","description":"Provisioner API reference page\n","excerpt":"Provisioner API reference page\n","ref":"/v0.5.0/provisioner/","tags":"","title":"Provisioner API"},{"body":"Example Provisioner Resource apiVersion:karpenter.sh/v1alpha5kind:Provisionermetadata:name:defaultspec:# If nil, the feature is disabled, nodes will never expirettlSecondsUntilExpired:2592000# 30 Days = 60 * 60 * 24 * 30 Seconds;# If nil, the feature is disabled, nodes will never scale down due to low utilizationttlSecondsAfterEmpty:30# Provisioned nodes will have these taints# Taints may prevent pods from scheduling if they are not toleratedtaints:- key:example.com/special-tainteffect:NoSchedule# Labels are arbitrary key-values that are applied to all nodeslabels:billing-team:my-team# Requirements that constrain the parameters of provisioned nodes.# These requirements are combined with pod.spec.affinity.nodeAffinity rules.# Operators { In, NotIn } are supported to enable including or excluding valuesrequirements:- key:\"node.kubernetes.io/instance-type\"operator:Invalues:[\"m5.large\",\"m5.2xlarge\"]- key:\"topology.kubernetes.io/zone\"operator:Invalues:[\"us-west-2a\",\"us-west-2b\"]- key:\"kubernetes.io/arch\"operator:Invalues:[\"arm64\",\"amd64\"]- key:\"karpenter.sh/capacity-type\"# If not included, the webhook for the AWS cloud provider will default to on-demandoperator:Invalues:[\"spot\",\"on-demand\"]# These fields vary per cloud provider, see your cloud provider specific documentationprovider:{}spec.requirements Kubernetes defines the following Well-Known Labels, and cloud providers (e.g., AWS) implement them. They are defined at the “spec.requirements” section of the Provisioner API.\nThese well known labels may be specified at the provisioner level, or in a workload definition (e.g., nodeSelector on a pod.spec). Nodes are chosen using the both the provisioner’s and pod’s requirements. If there is no overlap, nodes will not be launched. In other words, a pod’s requirements must be within the provisioner’s requirements. If a requirement is not defined for a well known label, any value available to the cloud provider may be chosen.\nFor example, an instance type may be specified using a nodeSelector in a pod spec. If the instance type requested is not included in the provisioner list and the provisioner has instance type requirements, Karpenter will not create a node or schedule the pod.\n📝 None of these values are required.\nInstance Types  key: node.kubernetes.io/instance-type  Generally, instance types should be a list and not a single value. Leaving this field undefined is recommended, as it maximizes choices for efficiently placing pods.\n☁️ AWS\nReview AWS instance types.\nThe default value includes all instance types with the exclusion of metal (non-virtualized), non-HVM, and GPU instances.\nView the full list of instance types with aws ec2 describe-instance-types.\nExample\nSet Default with provisioner.yaml\nspec:requirements:- key:node.kubernetes.io/instance-typeoperator:Invalues:[\"m5.large\",\"m5.2xlarge\"]Override with workload manifest (e.g., pod)\nspec:template:spec:nodeSelector:node.kubernetes.io/instance-type:m5.largeAvailability Zones  key: topology.kubernetes.io/zone value example: us-east-1c  ☁️ AWS\n value list: aws ec2 describe-availability-zones --region \u003cregion-name\u003e  Karpenter can be configured to create nodes in a particular zone. Note that the Availability Zone us-east-1a for your AWS account might not have the same location as us-east-1a for another AWS account.\nLearn more about Availability Zone IDs.\nArchitecture  key: kubernetes.io/arch values  amd64 (default) arm64    Karpenter supports amd64 nodes, and arm64 nodes.\nCapacity Type  key: karpenter.sh/capacity-type  ☁️ AWS\n values  spot (default) on-demand    Karpenter supports specifying capacity type, which is analogous to EC2 purchase options.\nspec.provider This section is cloud provider specific. Reference the appropriate documentation:\n AWS  ","categories":"","description":"Provisioner API reference page\n","excerpt":"Provisioner API reference page\n","ref":"/v0.5.2/provisioner/","tags":"","title":"Provisioner API"},{"body":"Example Provisioner Resource apiVersion:karpenter.sh/v1alpha5kind:Provisionermetadata:name:defaultspec:# If nil, the feature is disabled, nodes will never expirettlSecondsUntilExpired:2592000# 30 Days = 60 * 60 * 24 * 30 Seconds;# If nil, the feature is disabled, nodes will never scale down due to low utilizationttlSecondsAfterEmpty:30# Provisioned nodes will have these taints# Taints may prevent pods from scheduling if they are not toleratedtaints:- key:example.com/special-tainteffect:NoSchedule# Labels are arbitrary key-values that are applied to all nodeslabels:billing-team:my-team# Requirements that constrain the parameters of provisioned nodes.# These requirements are combined with pod.spec.affinity.nodeAffinity rules.# Operators { In, NotIn } are supported to enable including or excluding valuesrequirements:- key:\"node.kubernetes.io/instance-type\"operator:Invalues:[\"m5.large\",\"m5.2xlarge\"]- key:\"topology.kubernetes.io/zone\"operator:Invalues:[\"us-west-2a\",\"us-west-2b\"]- key:\"kubernetes.io/arch\"operator:Invalues:[\"arm64\",\"amd64\"]- key:\"karpenter.sh/capacity-type\"# If not included, the webhook for the AWS cloud provider will default to on-demandoperator:Invalues:[\"spot\",\"on-demand\"]# These fields vary per cloud provider, see your cloud provider specific documentationprovider:{}spec.requirements Kubernetes defines the following Well-Known Labels, and cloud providers (e.g., AWS) implement them. They are defined at the “spec.requirements” section of the Provisioner API.\nThese well known labels may be specified at the provisioner level, or in a workload definition (e.g., nodeSelector on a pod.spec). Nodes are chosen using the both the provisioner’s and pod’s requirements. If there is no overlap, nodes will not be launched. In other words, a pod’s requirements must be within the provisioner’s requirements. If a requirement is not defined for a well known label, any value available to the cloud provider may be chosen.\nFor example, an instance type may be specified using a nodeSelector in a pod spec. If the instance type requested is not included in the provisioner list and the provisioner has instance type requirements, Karpenter will not create a node or schedule the pod.\n📝 None of these values are required.\nInstance Types  key: node.kubernetes.io/instance-type  Generally, instance types should be a list and not a single value. Leaving this field undefined is recommended, as it maximizes choices for efficiently placing pods.\n☁️ AWS\nReview AWS instance types.\nThe default value includes all instance types with the exclusion of metal (non-virtualized), non-HVM, and GPU instances.\nView the full list of instance types with aws ec2 describe-instance-types.\nExample\nSet Default with provisioner.yaml\nspec:requirements:- key:node.kubernetes.io/instance-typeoperator:Invalues:[\"m5.large\",\"m5.2xlarge\"]Override with workload manifest (e.g., pod)\nspec:template:spec:nodeSelector:node.kubernetes.io/instance-type:m5.largeAvailability Zones  key: topology.kubernetes.io/zone value example: us-east-1c  ☁️ AWS\n value list: aws ec2 describe-availability-zones --region \u003cregion-name\u003e  Karpenter can be configured to create nodes in a particular zone. Note that the Availability Zone us-east-1a for your AWS account might not have the same location as us-east-1a for another AWS account.\nLearn more about Availability Zone IDs.\nArchitecture  key: kubernetes.io/arch values  amd64 (default) arm64    Karpenter supports amd64 nodes, and arm64 nodes.\nCapacity Type  key: karpenter.sh/capacity-type  ☁️ AWS\n values  spot (default) on-demand    Karpenter supports specifying capacity type, which is analogous to EC2 purchase options.\nspec.kubeletConfiguration Karpenter provides the ability to specify a few additional Kubelet args. These are all optional and provide support for additional customization and use cases. Adjust these only if you know you need to do so.\nspec:kubeletConfiguration:clusterDNS:[\"10.0.1.100\"]spec.provider This section is cloud provider specific. Reference the appropriate documentation:\n AWS  ","categories":"","description":"Provisioner API reference page\n","excerpt":"Provisioner API reference page\n","ref":"/v0.5.3/provisioner/","tags":"","title":"Provisioner API"},{"body":"","categories":"","description":"","excerpt":"","ref":"/preview/aws/","tags":"","title":"AWS"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v0.4.3/cloud-providers/aws/","tags":"","title":"AWS"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v0.5.0/aws/","tags":"","title":"AWS"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v0.5.2/aws/","tags":"","title":"AWS"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v0.5.3/aws/","tags":"","title":"AWS"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v0.4.3/cloud-providers/","tags":"","title":"Cloud Providers"},{"body":"Dependencies The following tools are required for contributing to the Karpenter project.\n   Package Version Install     go v1.15.3+ Instructions   kubectl  brew install kubectl   helm  brew install helm   Other tools  make toolchain    Developing Setup / Teardown Based on how you are running your Kubernetes cluster, follow the Environment specific setup to configure your environment before you continue. Once you have your environment set up, to install Karpenter in the Kubernetes cluster specified in your ~/.kube/config run the following commands.\nCLOUD_PROVIDER=\u003cYOUR_PROVIDER\u003e make apply # Install Karpenter make delete # Uninstall Karpenter Developer Loop  Make sure dependencies are installed  Run make codegen to make sure yaml manifests are generated Run make toolchain to install cli tools for building and testing the project   You will need a personal development image repository (e.g. ECR)  Make sure you have valid credentials to your development repository. $KO_DOCKER_REPO must point to your development repository Your cluster must have permissions to read from the repository   If you created your cluster on version 1.19 or above, you may need to tag your subnets as mentioned here. This is a temporary problem with our subnet discovery system, and is being tracked here. It’s also a good idea to persist $CLOUD_PROVIDER in your environment variables to simplify the make apply command.  Build and Deploy Note: these commands do not rely on each other and may be executed independently\nmake apply # quickly deploy changes to your cluster make dev # run codegen, lint, and tests Testing make test # E2e correctness tests make battletest # More rigorous tests run in CI environment Verbose Logging kubectl patch configmap config-logging -n karpenter --patch '{\"data\":{\"loglevel.controller\":\"debug\"}}' Debugging Metrics OSX:\nopen http://localhost:8080/metrics \u0026\u0026 kubectl port-forward service/karpenter-metrics -n karpenter 8080 Linux:\ngio open http://localhost:8080/metrics \u0026\u0026 kubectl port-forward service/karpenter-metrics -n karpenter 8080 Tailing Logs While you can tail Karpenter’s logs with kubectl, there’s a number of tools out there that enhance the experience. We recommend Stern:\nstern -l karpenter=controller -n karpenter Environment specific setup AWS Set the CLOUD_PROVIDER environment variable to build cloud provider specific packages of Karpenter.\nexport CLOUD_PROVIDER=aws For local development on Karpenter you will need a Docker repo which can manage your images for Karpenter components. You can use the following command to provision an ECR repository.\naws ecr create-repository \\  --repository-name karpenter/controller \\  --image-scanning-configuration scanOnPush=true \\  --region ${AWS_DEFAULT_REGION} aws ecr create-repository \\  --repository-name karpenter/webhook \\  --image-scanning-configuration scanOnPush=true \\  --region ${AWS_DEFAULT_REGION} Once you have your ECR repository provisioned, configure your Docker daemon to authenticate with your newly created repository.\nexport KO_DOCKER_REPO=\"${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/karpenter\" aws ecr get-login-password --region ${AWS_DEFAULT_REGION} | docker login --username AWS --password-stdin $KO_DOCKER_REPO ","categories":"","description":"","excerpt":"Dependencies The following tools are required for contributing to the …","ref":"/preview/development-guide/","tags":"","title":"Development Guide"},{"body":"Dependencies The following tools are required for contributing to the Karpenter project.\n   Package Version Install     go v1.15.3+ Instructions   kubectl  brew install kubectl   helm  brew install helm   Other tools  make toolchain    Developing Setup / Teardown Based on how you are running your Kubernetes cluster, follow the Environment specific setup to configure your environment before you continue. Once you have your environment set up, to install Karpenter in the Kubernetes cluster specified in your ~/.kube/config run the following commands.\nCLOUD_PROVIDER=\u003cYOUR_PROVIDER\u003e make apply # Install Karpenter make delete # Uninstall Karpenter Developer Loop  Make sure dependencies are installed  Run make codegen to make sure yaml manifests are generated Run make toolchain to install cli tools for building and testing the project   You will need a personal development image repository (e.g. ECR)  Make sure you have valid credentials to your development repository. $KO_DOCKER_REPO must point to your development repository Your cluster must have permissions to read from the repository   If you created your cluster on version 1.19 or above, you may need to tag your subnets as mentioned here. This is a temporary problem with our subnet discovery system, and is being tracked here. It’s also a good idea to persist $CLOUD_PROVIDER in your environment variables to simplify the make apply command.  Build and Deploy make dev # build and test code kubectl create namespace karpenter # create target namespace for deployment CLOUD_PROVIDER=\u003cYOUR_PROVIDER\u003e make apply # deploy for your cloud provider Testing make test # E2e correctness tests make battletest # More rigorous tests run in CI environment Verbose Logging kubectl patch configmap config-logging -n karpenter --patch '{\"data\":{\"loglevel.controller\":\"debug\"}}' Debugging Metrics OSX:\nopen http://localhost:8080/metrics \u0026\u0026 kubectl port-forward service/karpenter-metrics -n karpenter 8080 Linux:\ngio open http://localhost:8080/metrics \u0026\u0026 kubectl port-forward service/karpenter-metrics -n karpenter 8080 Tailing Logs While you can tail Karpenter’s logs with kubectl, there’s a number of tools out there that enhance the experience. We recommend Stern:\nstern -l karpenter=controller -n karpenter Environment specific setup AWS Set the CLOUD_PROVIDER environment variable to build cloud provider specific packages of Karpenter.\nexport CLOUD_PROVIDER=aws For local development on Karpenter you will need a Docker repo which can manage your images for Karpenter components. You can use the following command to provision an ECR repository.\naws ecr create-repository \\ --repository-name karpenter/controller \\ --image-scanning-configuration scanOnPush=true \\ --region ${AWS_DEFAULT_REGION} aws ecr create-repository \\ --repository-name karpenter/webhook \\ --image-scanning-configuration scanOnPush=true \\ --region ${AWS_DEFAULT_REGION} Once you have your ECR repository provisioned, configure your Docker daemon to authenticate with your newly created repository.\nexport KO_DOCKER_REPO=\"${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/karpenter\" aws ecr get-login-password --region ${AWS_DEFAULT_REGION} | docker login --username AWS --password-stdin $KO_DOCKER_REPO ","categories":"","description":"","excerpt":"Dependencies The following tools are required for contributing to the …","ref":"/v0.4.3/development-guide/","tags":"","title":"Development Guide"},{"body":"Dependencies The following tools are required for contributing to the Karpenter project.\n   Package Version Install     go v1.15.3+ Instructions   kubectl  brew install kubectl   helm  brew install helm   Other tools  make toolchain    Developing Setup / Teardown Based on how you are running your Kubernetes cluster, follow the Environment specific setup to configure your environment before you continue. Once you have your environment set up, to install Karpenter in the Kubernetes cluster specified in your ~/.kube/config run the following commands.\nCLOUD_PROVIDER=\u003cYOUR_PROVIDER\u003e make apply # Install Karpenter make delete # Uninstall Karpenter Developer Loop  Make sure dependencies are installed  Run make codegen to make sure yaml manifests are generated Run make toolchain to install cli tools for building and testing the project   You will need a personal development image repository (e.g. ECR)  Make sure you have valid credentials to your development repository. $KO_DOCKER_REPO must point to your development repository Your cluster must have permissions to read from the repository   If you created your cluster on version 1.19 or above, you may need to tag your subnets as mentioned here. This is a temporary problem with our subnet discovery system, and is being tracked here. It’s also a good idea to persist $CLOUD_PROVIDER in your environment variables to simplify the make apply command.  Build and Deploy Note: these commands do not rely on each other and may be executed independently\nmake apply # quickly deploy changes to your cluster make dev # run codegen, lint, and tests Testing make test # E2e correctness tests make battletest # More rigorous tests run in CI environment Verbose Logging kubectl patch configmap config-logging -n karpenter --patch '{\"data\":{\"loglevel.controller\":\"debug\"}}' Debugging Metrics OSX:\nopen http://localhost:8080/metrics \u0026\u0026 kubectl port-forward service/karpenter-metrics -n karpenter 8080 Linux:\ngio open http://localhost:8080/metrics \u0026\u0026 kubectl port-forward service/karpenter-metrics -n karpenter 8080 Tailing Logs While you can tail Karpenter’s logs with kubectl, there’s a number of tools out there that enhance the experience. We recommend Stern:\nstern -l karpenter=controller -n karpenter Environment specific setup AWS Set the CLOUD_PROVIDER environment variable to build cloud provider specific packages of Karpenter.\nexport CLOUD_PROVIDER=aws For local development on Karpenter you will need a Docker repo which can manage your images for Karpenter components. You can use the following command to provision an ECR repository.\naws ecr create-repository \\  --repository-name karpenter/controller \\  --image-scanning-configuration scanOnPush=true \\  --region ${AWS_DEFAULT_REGION} aws ecr create-repository \\  --repository-name karpenter/webhook \\  --image-scanning-configuration scanOnPush=true \\  --region ${AWS_DEFAULT_REGION} Once you have your ECR repository provisioned, configure your Docker daemon to authenticate with your newly created repository.\nexport KO_DOCKER_REPO=\"${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/karpenter\" aws ecr get-login-password --region ${AWS_DEFAULT_REGION} | docker login --username AWS --password-stdin $KO_DOCKER_REPO ","categories":"","description":"","excerpt":"Dependencies The following tools are required for contributing to the …","ref":"/v0.5.0/development-guide/","tags":"","title":"Development Guide"},{"body":"Dependencies The following tools are required for contributing to the Karpenter project.\n   Package Version Install     go v1.15.3+ Instructions   kubectl  brew install kubectl   helm  brew install helm   Other tools  make toolchain    Developing Setup / Teardown Based on how you are running your Kubernetes cluster, follow the Environment specific setup to configure your environment before you continue. Once you have your environment set up, to install Karpenter in the Kubernetes cluster specified in your ~/.kube/config run the following commands.\nCLOUD_PROVIDER=\u003cYOUR_PROVIDER\u003e make apply # Install Karpenter make delete # Uninstall Karpenter Developer Loop  Make sure dependencies are installed  Run make codegen to make sure yaml manifests are generated Run make toolchain to install cli tools for building and testing the project   You will need a personal development image repository (e.g. ECR)  Make sure you have valid credentials to your development repository. $KO_DOCKER_REPO must point to your development repository Your cluster must have permissions to read from the repository   If you created your cluster on version 1.19 or above, you may need to tag your subnets as mentioned here. This is a temporary problem with our subnet discovery system, and is being tracked here. It’s also a good idea to persist $CLOUD_PROVIDER in your environment variables to simplify the make apply command.  Build and Deploy Note: these commands do not rely on each other and may be executed independently\nmake apply # quickly deploy changes to your cluster make dev # run codegen, lint, and tests Testing make test # E2e correctness tests make battletest # More rigorous tests run in CI environment Verbose Logging kubectl patch configmap config-logging -n karpenter --patch '{\"data\":{\"loglevel.controller\":\"debug\"}}' Debugging Metrics OSX:\nopen http://localhost:8080/metrics \u0026\u0026 kubectl port-forward service/karpenter-metrics -n karpenter 8080 Linux:\ngio open http://localhost:8080/metrics \u0026\u0026 kubectl port-forward service/karpenter-metrics -n karpenter 8080 Tailing Logs While you can tail Karpenter’s logs with kubectl, there’s a number of tools out there that enhance the experience. We recommend Stern:\nstern -l karpenter=controller -n karpenter Environment specific setup AWS Set the CLOUD_PROVIDER environment variable to build cloud provider specific packages of Karpenter.\nexport CLOUD_PROVIDER=aws For local development on Karpenter you will need a Docker repo which can manage your images for Karpenter components. You can use the following command to provision an ECR repository.\naws ecr create-repository \\  --repository-name karpenter/controller \\  --image-scanning-configuration scanOnPush=true \\  --region ${AWS_DEFAULT_REGION} aws ecr create-repository \\  --repository-name karpenter/webhook \\  --image-scanning-configuration scanOnPush=true \\  --region ${AWS_DEFAULT_REGION} Once you have your ECR repository provisioned, configure your Docker daemon to authenticate with your newly created repository.\nexport KO_DOCKER_REPO=\"${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/karpenter\" aws ecr get-login-password --region ${AWS_DEFAULT_REGION} | docker login --username AWS --password-stdin $KO_DOCKER_REPO ","categories":"","description":"","excerpt":"Dependencies The following tools are required for contributing to the …","ref":"/v0.5.2/development-guide/","tags":"","title":"Development Guide"},{"body":"Dependencies The following tools are required for contributing to the Karpenter project.\n   Package Version Install     go v1.15.3+ Instructions   kubectl  brew install kubectl   helm  brew install helm   Other tools  make toolchain    Developing Setup / Teardown Based on how you are running your Kubernetes cluster, follow the Environment specific setup to configure your environment before you continue. Once you have your environment set up, to install Karpenter in the Kubernetes cluster specified in your ~/.kube/config run the following commands.\nCLOUD_PROVIDER=\u003cYOUR_PROVIDER\u003e make apply # Install Karpenter make delete # Uninstall Karpenter Developer Loop  Make sure dependencies are installed  Run make codegen to make sure yaml manifests are generated Run make toolchain to install cli tools for building and testing the project   You will need a personal development image repository (e.g. ECR)  Make sure you have valid credentials to your development repository. $KO_DOCKER_REPO must point to your development repository Your cluster must have permissions to read from the repository   If you created your cluster on version 1.19 or above, you may need to tag your subnets as mentioned here. This is a temporary problem with our subnet discovery system, and is being tracked here. It’s also a good idea to persist $CLOUD_PROVIDER in your environment variables to simplify the make apply command.  Build and Deploy Note: these commands do not rely on each other and may be executed independently\nmake apply # quickly deploy changes to your cluster make dev # run codegen, lint, and tests Testing make test # E2e correctness tests make battletest # More rigorous tests run in CI environment Verbose Logging kubectl patch configmap config-logging -n karpenter --patch '{\"data\":{\"loglevel.controller\":\"debug\"}}' Debugging Metrics OSX:\nopen http://localhost:8080/metrics \u0026\u0026 kubectl port-forward service/karpenter-metrics -n karpenter 8080 Linux:\ngio open http://localhost:8080/metrics \u0026\u0026 kubectl port-forward service/karpenter-metrics -n karpenter 8080 Tailing Logs While you can tail Karpenter’s logs with kubectl, there’s a number of tools out there that enhance the experience. We recommend Stern:\nstern -l karpenter=controller -n karpenter Environment specific setup AWS Set the CLOUD_PROVIDER environment variable to build cloud provider specific packages of Karpenter.\nexport CLOUD_PROVIDER=aws For local development on Karpenter you will need a Docker repo which can manage your images for Karpenter components. You can use the following command to provision an ECR repository.\naws ecr create-repository \\  --repository-name karpenter/controller \\  --image-scanning-configuration scanOnPush=true \\  --region ${AWS_DEFAULT_REGION} aws ecr create-repository \\  --repository-name karpenter/webhook \\  --image-scanning-configuration scanOnPush=true \\  --region ${AWS_DEFAULT_REGION} Once you have your ECR repository provisioned, configure your Docker daemon to authenticate with your newly created repository.\nexport KO_DOCKER_REPO=\"${AWS_ACCOUNT_ID}.dkr.ecr.${AWS_DEFAULT_REGION}.amazonaws.com/karpenter\" aws ecr get-login-password --region ${AWS_DEFAULT_REGION} | docker login --username AWS --password-stdin $KO_DOCKER_REPO ","categories":"","description":"","excerpt":"Dependencies The following tools are required for contributing to the …","ref":"/v0.5.3/development-guide/","tags":"","title":"Development Guide"},{"body":"By default, Karpenter generates launch templates that use EKS Optimized AMI for nodes. Often, users need to customize the node image to integrate with existing infrastructure or meet compliance requirements. Karpenter supports custom node images through Launch Templates. If you need to customize the node, then you need a custom launch template.\nNote: By customizing the image, you are taking responsibility for maintaining the image, including security updates. In the default configuration, Karpenter will use the latest version of the EKS optimized AMI, which is maintained by AWS.\nIntroduction Karpenter follows existing AWS patterns for customizing the base image of instances. More specifically, Karpenter uses EC2 launch templates. Launch templates may specify many values. The pivotal value is the base image (AMI). Launch templates further specify many different parameters related to networking, authorization, instance type, and more.\nLaunch Templates and AMIs are unique to AWS regions, similar to EKS clusters. IAM resources are global.\nKarpenter only implements a subset of launch template fields, and some fields should not be set.\nThis guide describes requirements for using launch templates with Karpenter, and later an example procedure.\nLaunch Template Requirements The Launch Template resource includes a large number of fields. AWS accepts launch templates with any subset of these fields defined.\nCertain fields are obviously critical, such as AMI and User Data. Some fields are useful for particular workloads, such as storage and IAM Instance Profile.\nFinally, the majority of Launch Template fields should not be set (or will have no effect), such as network interfaces and instance type.\nImportant Fields When creating a custom launch template, the AMI and User Data are the defining characteristics. Instance Profile (IAM Role) and Security Group (firewall rules) are also important for Karpenter.\nAMI AMI (Amazon Machine Image), is the base image/VM for a launch template.\nReview the instructions for importing a VM to AWS. Note the AMI id generated by this process, such as, ami-074cce78125f09d61.\nUser Data - Autoconfigure Importantly, the AMI must support automatically connecting to a cluster based on “user data”, or a base64 encoded string passed to the instance at startup. The syntax and purpose of the user data varies between images. The Karpenter default OS, Amazon Linux 2 (AL2), accepts shell scripts (bash commands).\nAWS calls data passed to an instance at launch time “user data”.\nIn the default configuration, Karpenter uses an EKS optimized version of AL2 and passes the hostname of the Kubernetes API server, and a certificate. The EKS Optimized AMI includes a bootstrap.sh script which connects the instance to the cluster, based on the passed data.\nAlternatively, you may reference AWS’s bootstrap.sh file when building a custom base image.\n#!/bin/bash\r/etc/eks/bootstrap.sh \u003cmy-cluster-name\u003e \\\r--kubelet-extra-args \u003c'--max-pods=40'\u003e \\\r--b64-cluster-ca \u003ccertificateAuthority\u003e \\\r--apiserver-endpoint \u003cendpoint\u003e \\\r--dns-cluster-ip \u003cserviceIpv4Cidr\u003e \\\r--use-max-pods false\rNote, you must populate this command with live values. Karpenter will not change the user data in the launch template.\nEncode using yaml function !Base64 yaml function or cat userdata.sh | base64 \u003e userdata-encoded.txt shell command.\nBootstrap Script Parameters\nThe sample bootstrap script requires information to join the cluster.\nThese values may be found using:\naws eks describe-cluster --name MyKarpenterCluster\rKubelet Arguments\nSpecifying max-pods can break Karpenter’s binpacking logic (it has no way to know what this setting is). If Karpenter attempts to pack more than this number of pods, the instance may be oversized, and additional pods will reschedule.\nSituational Fields Configure these values in response to a particular use case, such as nodes interacting with another AWS service, or using EBS storage on the node.\nInstance Profile - IAM The launch template must include an “instance profile” – an IAM role.\nThe instance profile must include at least the permissions of the default Karpenter node instance profile. See the default role, KarpenterNodeRole, in the full example below for more information.\nSee also, the managed policy “AmazonEKSWorkerNodePolicy” which includes permission to describe clusters and subnets.\nStorage Karpenter expects nothing of node storage. Configure as needed for your base image.\nSecurity Groups - Firewall The launch template may include a security group (i.e., instance firewall rules) and the security group must be associated with the virtual private cloud (VPC) of the EKS cluster. If none is specified, the default security group of the cluster VPC is used.\nThe security group must permit communication with EKS control plane. Outbound access should be permitted for at least: HTTPS on port 443, DNS (UDP and TCP) on port 53, and your subnet’s network access control list (network ACL).\nFields with Undefined Behavior Resources referenced by these fields are controlled by EKS/Karpenter, and not the launch template.\nInstance Type The instance type should not be specified in the launch template. Karpenter will determine the launch template at run time.\nNetwork Interfaces The AWS CNI will configure the network interfaces. Do not configure network instances in the launch template.\nCreating the Launch Template Launch Templates may be created via the web console, the AWS CLI, or CloudFormation.\nCloudFormation The procedure, in summary, is to:\n Create an AMI as described in the EC2 documentation. Write a EC2 Launch Template specification including the AMI. Push the specification to AWS with CloudFormation. Update the Provisioner CRD to specify the new Launch Template.  An example yaml cloudformation definition of a launch template for Karpenter is provided below.\nCloudFormation yaml is suited for the moderately high configuration density of launch templates, and creating the unusual InstanceProfile resource.\nYou must manually replace these values in the template:\n SecurityGroupID  list all security groups with aws ec2 describe-security-groups   Parameters in UserData AMI  AWSTemplateFormatVersion:'2010-09-09'Resources:# create InstanceProfile wrapper on NodeRoleKarpenterNodeInstanceProfile:Type:\"AWS::IAM::InstanceProfile\"Properties:InstanceProfileName:\"KarpenterNodeInstanceProfile\"Path:\"/\"Roles:- Ref:\"KarpenterNodeRole\"# create role with basic permissions for EKS nodeKarpenterNodeRole:Type:\"AWS::IAM::Role\"Properties:RoleName:\"KarpenterNodeRole\"Path:/AssumeRolePolicyDocument:Version:\"2012-10-17\"Statement:- Effect:AllowPrincipal:Service:!Sub \"ec2.${AWS::URLSuffix}\"Action:- \"sts:AssumeRole\"ManagedPolicyArns:- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEKSWorkerNodePolicy\"- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEKS_CNI_Policy\"- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore\"MyLaunchTemplate:Type:AWS::EC2::LaunchTemplateProperties:LaunchTemplateData:IamInstanceProfile:# Get ARN of InstanceProfile defined aboveArn:!GetAtt- KarpenterNodeInstanceProfile- ArnImageId:ami-074cce78125f09d61# UserData is Base64 EncodedUserData:!Base64 \u003e#!/bin/bash/etc/eks/bootstrap.sh 'MyClusterName' \\--kubelet-extra-args '--node-labels=node.k8s.aws/capacity-type=spot' \\--b64-cluster-ca 'LS0t....0tCg==' \\--apiserver-endpoint 'https://B0385BE29EA792E811CB5866D23C856E.gr7.us-east-2.eks.amazonaws.com'BlockDeviceMappings:- Ebs:VolumeSize:80VolumeType:gp3DeviceName:/dev/xvda# The SecurityGroup must be associated with the cluster VPCSecurityGroupIds:- sg-a69adfdbLaunchTemplateName:KarpenterCustomLaunchTemplateCreate the Launch Template by uploading the CloudFormation yaml file. The sample yaml creates an IAM Object (InstanceProfile), so --capabilities\rCAPABILITY_NAMED_IAM must be indicated.\naws cloudformation create-stack \\\r--stack-name KarpenterLaunchTemplateStack \\\r--template-body file://$(pwd)/lt-cfn-demo.yaml \\\r--capabilities CAPABILITY_NAMED_IAM\rDefine LaunchTemplate for Provisioner The LaunchTemplate is ready to be used. Specify it by name in the Provisioner CRD. Karpenter will use this template when creating new instances.\napiVersion:karpenter.sh/v1alpha5kind:Provisionerspec:provider:launchTemplate:CustomKarpenterLaunchTemplateDemo","categories":"","description":"","excerpt":"By default, Karpenter generates launch templates that use EKS …","ref":"/preview/aws/launch-templates/","tags":"","title":"Launch Templates and Custom Images"},{"body":"By default, Karpenter generates launch templates that use EKS Optimized AMI for nodes. Often, users need to customize the node image to integrate with existing infrastructure or meet compliance requirements. Karpenter supports custom node images through Launch Templates. If you need to customize the node, then you need a custom launch template.\nNote: By customizing the image, you are taking responsibility for maintaining the image, including security updates. In the default configuration, Karpenter will use the latest version of the EKS optimized AMI, which is maintained by AWS.\nIntroduction Karpenter follows existing AWS patterns for customizing the base image of instances. More specifically, Karpenter uses EC2 launch templates. Launch templates may specify many values. The pivotal value is the base image (AMI). Launch templates further specify many different parameters related to networking, authorization, instance type, and more.\nLaunch Templates and AMIs are unique to AWS regions, similar to EKS clusters. IAM resources are global.\nKarpenter only implements a subset of launch template fields, and some fields should not be set.\nThis guide describes requirements for using launch templates with Karpenter, and later an example procedure.\nLaunch Template Requirements The Launch Template resource includes a large number of fields. AWS accepts launch templates with any subset of these fields defined.\nCertain fields are obviously critical, such as AMI and User Data. Some fields are useful for particular workloads, such as storage and IAM Instance Profile.\nFinally, the majority of Launch Template fields should not be set (or will have no effect), such as network interfaces and instance type.\nImportant Fields When creating a custom launch template, the AMI and User Data are the defining characteristics. Instance Profile (IAM Role) and Security Group (firewall rules) are also important for Karpenter.\nAMI AMI (Amazon Machine Image), is the base image/VM for a launch template.\nReview the instructions for importing a VM to AWS. Note the AMI id generated by this process, such as, ami-074cce78125f09d61.\nUser Data - Autoconfigure Importantly, the AMI must support automatically connecting to a cluster based on “user data”, or a base64 encoded string passed to the instance at startup. The syntax and purpose of the user data varies between images. The Karpenter default OS, Amazon Linux 2 (AL2), accepts shell scripts (bash commands).\nAWS calls data passed to an instance at launch time “user data”.\nIn the default configuration, Karpenter uses an EKS optimized version of AL2 and passes the hostname of the Kubernetes API server, and a certificate. The EKS Optimized AMI includes a bootstrap.sh script which connects the instance to the cluster, based on the passed data.\nAlternatively, you may reference AWS’s bootstrap.sh file when building a custom base image.\n#!/bin/bash\r/etc/eks/bootstrap.sh \u003cmy-cluster-name\u003e \\\r--kubelet-extra-args \u003c'--max-pods=40'\u003e \\\r--b64-cluster-ca \u003ccertificateAuthority\u003e \\\r--apiserver-endpoint \u003cendpoint\u003e \\\r--dns-cluster-ip \u003cserviceIpv4Cidr\u003e \\\r--use-max-pods false\rNote, you must populate this command with live values. Karpenter will not change the user data in the launch template.\nEncode using yaml function !Base64 yaml function or cat userdata.sh | base64 \u003e userdata-encoded.txt shell command.\nBootstrap Script Parameters\nThe sample bootstrap script requires information to join the cluster.\nThese values may be found using:\naws eks describe-cluster --name MyKarpenterCluster\rKubelet Arguments\nSpecifying max-pods can break Karpenter’s binpacking logic (it has no way to know what this setting is). If Karpenter attempts to pack more than this number of pods, the instance may be oversized, and additional pods will reschedule.\nSituational Fields Configure these values in response to a particular use case, such as nodes interacting with another AWS service, or using EBS storage on the node.\nInstance Profile - IAM The launch template must include an “instance profile” – an IAM role.\nThe instance profile must include at least the permissions of the default Karpenter node instance profile. See the default role, KarpenterNodeRole, in the full example below for more information.\nSee also, the managed policy “AmazonEKSWorkerNodePolicy” which includes permission to describe clusters and subnets.\nStorage Karpenter expects nothing of node storage. Configure as needed for your base image.\nSecurity Groups - Firewall The launch template may include a security group (i.e., instance firewall rules) and the security group must be associated with the virtual private cloud (VPC) of the EKS cluster. If none is specified, the default security group of the cluster VPC is used.\nThe security group must permit communication with EKS control plane. Outbound access should be permitted for at least: HTTPS on port 443, DNS (UDP and TCP) on port 53, and your subnet’s network access control list (network ACL).\nFields with Undefined Behavior Resources referenced by these fields are controlled by EKS/Karpenter, and not the launch template.\nInstance Type The instance type should not be specified in the launch template. Karpenter will determine the launch template at run time.\nNetwork Interfaces The AWS CNI will configure the network interfaces. Do not configure network instances in the launch template.\nCreating the Launch Template Launch Templates may be created via the web console, the AWS CLI, or CloudFormation.\nCloudFormation The procedure, in summary, is to:\n Create an AMI as described in the EC2 documentation. Write a EC2 Launch Template specification including the AMI. Push the specification to AWS with CloudFormation. Update the Provisioner CRD to specify the new Launch Template.  An example yaml cloudformation definition of a launch template for Karpenter is provided below.\nCloudFormation yaml is suited for the moderately high configuration density of launch templates, and creating the unusual InstanceProfile resource.\nYou must manually replace these values in the template:\n SecurityGroupID  list all security groups with aws ec2 describe-security-groups   Parameters in UserData AMI  AWSTemplateFormatVersion:'2010-09-09'Resources:# create InstanceProfile wrapper on NodeRoleKarpenterNodeInstanceProfile:Type:\"AWS::IAM::InstanceProfile\"Properties:InstanceProfileName:\"KarpenterNodeInstanceProfile\"Path:\"/\"Roles:- Ref:\"KarpenterNodeRole\"# create role with basic permissions for EKS nodeKarpenterNodeRole:Type:\"AWS::IAM::Role\"Properties:RoleName:\"KarpenterNodeRole\"Path:/AssumeRolePolicyDocument:Version:\"2012-10-17\"Statement:- Effect:AllowPrincipal:Service:!Sub \"ec2.${AWS::URLSuffix}\"Action:- \"sts:AssumeRole\"ManagedPolicyArns:- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEKSWorkerNodePolicy\"- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEKS_CNI_Policy\"- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore\"MyLaunchTemplate:Type:AWS::EC2::LaunchTemplateProperties:LaunchTemplateData:IamInstanceProfile:# Get ARN of InstanceProfile defined aboveArn:!GetAtt- KarpenterNodeInstanceProfile- ArnImageId:ami-074cce78125f09d61# UserData is Base64 EncodedUserData:!Base64 \u003e#!/bin/bash/etc/eks/bootstrap.sh 'MyClusterName' \\--kubelet-extra-args '--node-labels=node.k8s.aws/capacity-type=spot' \\--b64-cluster-ca 'LS0t....0tCg==' \\--apiserver-endpoint 'https://B0385BE29EA792E811CB5866D23C856E.gr7.us-east-2.eks.amazonaws.com'BlockDeviceMappings:- Ebs:VolumeSize:80VolumeType:gp3DeviceName:/dev/xvda# The SecurityGroup must be associated with the cluster VPCSecurityGroupIds:- sg-a69adfdbLaunchTemplateName:KarpenterCustomLaunchTemplateCreate the Launch Template by uploading the CloudFormation yaml file. The sample yaml creates an IAM Object (InstanceProfile), so --capabilities\rCAPABILITY_NAMED_IAM must be indicated.\naws cloudformation create-stack \\\r--stack-name KarpenterLaunchTemplateStack \\\r--template-body file://$(pwd)/lt-cfn-demo.yaml \\\r--capabilities CAPABILITY_NAMED_IAM\rDefine LaunchTemplate for Provisioner The LaunchTemplate is ready to be used. Specify it by name in the Provisioner CRD. Karpenter will use this template when creating new instances.\napiVersion:karpenter.sh/v1alpha5kind:Provisionerspec:provider:launchTemplate:CustomKarpenterLaunchTemplateDemo","categories":"","description":"","excerpt":"By default, Karpenter generates launch templates that use EKS …","ref":"/v0.4.3/cloud-providers/aws/launch-templates/","tags":"","title":"Launch Templates and Custom Images"},{"body":"By default, Karpenter generates launch templates that use EKS Optimized AMI for nodes. Often, users need to customize the node image to integrate with existing infrastructure or meet compliance requirements. Karpenter supports custom node images through Launch Templates. If you need to customize the node, then you need a custom launch template.\nNote: By customizing the image, you are taking responsibility for maintaining the image, including security updates. In the default configuration, Karpenter will use the latest version of the EKS optimized AMI, which is maintained by AWS.\nIntroduction Karpenter follows existing AWS patterns for customizing the base image of instances. More specifically, Karpenter uses EC2 launch templates. Launch templates may specify many values. The pivotal value is the base image (AMI). Launch templates further specify many different parameters related to networking, authorization, instance type, and more.\nLaunch Templates and AMIs are unique to AWS regions, similar to EKS clusters. IAM resources are global.\nKarpenter only implements a subset of launch template fields, and some fields should not be set.\nThis guide describes requirements for using launch templates with Karpenter, and later an example procedure.\nLaunch Template Requirements The Launch Template resource includes a large number of fields. AWS accepts launch templates with any subset of these fields defined.\nCertain fields are obviously critical, such as AMI and User Data. Some fields are useful for particular workloads, such as storage and IAM Instance Profile.\nFinally, the majority of Launch Template fields should not be set (or will have no effect), such as network interfaces and instance type.\nImportant Fields When creating a custom launch template, the AMI and User Data are the defining characteristics. Instance Profile (IAM Role) and Security Group (firewall rules) are also important for Karpenter.\nAMI AMI (Amazon Machine Image), is the base image/VM for a launch template.\nReview the instructions for importing a VM to AWS. Note the AMI id generated by this process, such as, ami-074cce78125f09d61.\nUser Data - Autoconfigure Importantly, the AMI must support automatically connecting to a cluster based on “user data”, or a base64 encoded string passed to the instance at startup. The syntax and purpose of the user data varies between images. The Karpenter default OS, Amazon Linux 2 (AL2), accepts shell scripts (bash commands).\nAWS calls data passed to an instance at launch time “user data”.\nIn the default configuration, Karpenter uses an EKS optimized version of AL2 and passes the hostname of the Kubernetes API server, and a certificate. The EKS Optimized AMI includes a bootstrap.sh script which connects the instance to the cluster, based on the passed data.\nAlternatively, you may reference AWS’s bootstrap.sh file when building a custom base image.\n#!/bin/bash\r/etc/eks/bootstrap.sh \u003cmy-cluster-name\u003e \\\r--kubelet-extra-args \u003c'--max-pods=40'\u003e \\\r--b64-cluster-ca \u003ccertificateAuthority\u003e \\\r--apiserver-endpoint \u003cendpoint\u003e \\\r--dns-cluster-ip \u003cserviceIpv4Cidr\u003e \\\r--use-max-pods false\rNote, you must populate this command with live values. Karpenter will not change the user data in the launch template.\nEncode using yaml function !Base64 yaml function or cat userdata.sh | base64 \u003e userdata-encoded.txt shell command.\nBootstrap Script Parameters\nThe sample bootstrap script requires information to join the cluster.\nThese values may be found using:\naws eks describe-cluster --name MyKarpenterCluster\rKubelet Arguments\nSpecifying max-pods can break Karpenter’s binpacking logic (it has no way to know what this setting is). If Karpenter attempts to pack more than this number of pods, the instance may be oversized, and additional pods will reschedule.\nSituational Fields Configure these values in response to a particular use case, such as nodes interacting with another AWS service, or using EBS storage on the node.\nInstance Profile - IAM The launch template must include an “instance profile” – an IAM role.\nThe instance profile must include at least the permissions of the default Karpenter node instance profile. See the default role, KarpenterNodeRole, in the full example below for more information.\nSee also, the managed policy “AmazonEKSWorkerNodePolicy” which includes permission to describe clusters and subnets.\nStorage Karpenter expects nothing of node storage. Configure as needed for your base image.\nSecurity Groups - Firewall The launch template may include a security group (i.e., instance firewall rules) and the security group must be associated with the virtual private cloud (VPC) of the EKS cluster. If none is specified, the default security group of the cluster VPC is used.\nThe security group must permit communication with EKS control plane. Outbound access should be permitted for at least: HTTPS on port 443, DNS (UDP and TCP) on port 53, and your subnet’s network access control list (network ACL).\nFields with Undefined Behavior Resources referenced by these fields are controlled by EKS/Karpenter, and not the launch template.\nInstance Type The instance type should not be specified in the launch template. Karpenter will determine the launch template at run time.\nNetwork Interfaces The AWS CNI will configure the network interfaces. Do not configure network instances in the launch template.\nCreating the Launch Template Launch Templates may be created via the web console, the AWS CLI, or CloudFormation.\nCloudFormation The procedure, in summary, is to:\n Create an AMI as described in the EC2 documentation. Write a EC2 Launch Template specification including the AMI. Push the specification to AWS with CloudFormation. Update the Provisioner CRD to specify the new Launch Template.  An example yaml cloudformation definition of a launch template for Karpenter is provided below.\nCloudFormation yaml is suited for the moderately high configuration density of launch templates, and creating the unusual InstanceProfile resource.\nYou must manually replace these values in the template:\n SecurityGroupID  list all security groups with aws ec2 describe-security-groups   Parameters in UserData AMI  AWSTemplateFormatVersion:'2010-09-09'Resources:# create InstanceProfile wrapper on NodeRoleKarpenterNodeInstanceProfile:Type:\"AWS::IAM::InstanceProfile\"Properties:InstanceProfileName:\"KarpenterNodeInstanceProfile\"Path:\"/\"Roles:- Ref:\"KarpenterNodeRole\"# create role with basic permissions for EKS nodeKarpenterNodeRole:Type:\"AWS::IAM::Role\"Properties:RoleName:\"KarpenterNodeRole\"Path:/AssumeRolePolicyDocument:Version:\"2012-10-17\"Statement:- Effect:AllowPrincipal:Service:!Sub \"ec2.${AWS::URLSuffix}\"Action:- \"sts:AssumeRole\"ManagedPolicyArns:- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEKSWorkerNodePolicy\"- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEKS_CNI_Policy\"- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore\"MyLaunchTemplate:Type:AWS::EC2::LaunchTemplateProperties:LaunchTemplateData:IamInstanceProfile:# Get ARN of InstanceProfile defined aboveArn:!GetAtt- KarpenterNodeInstanceProfile- ArnImageId:ami-074cce78125f09d61# UserData is Base64 EncodedUserData:!Base64 \u003e#!/bin/bash/etc/eks/bootstrap.sh 'MyClusterName' \\--kubelet-extra-args '--node-labels=node.k8s.aws/capacity-type=spot' \\--b64-cluster-ca 'LS0t....0tCg==' \\--apiserver-endpoint 'https://B0385BE29EA792E811CB5866D23C856E.gr7.us-east-2.eks.amazonaws.com'BlockDeviceMappings:- Ebs:VolumeSize:80VolumeType:gp3DeviceName:/dev/xvda# The SecurityGroup must be associated with the cluster VPCSecurityGroupIds:- sg-a69adfdbLaunchTemplateName:KarpenterCustomLaunchTemplateCreate the Launch Template by uploading the CloudFormation yaml file. The sample yaml creates an IAM Object (InstanceProfile), so --capabilities\rCAPABILITY_NAMED_IAM must be indicated.\naws cloudformation create-stack \\\r--stack-name KarpenterLaunchTemplateStack \\\r--template-body file://$(pwd)/lt-cfn-demo.yaml \\\r--capabilities CAPABILITY_NAMED_IAM\rDefine LaunchTemplate for Provisioner The LaunchTemplate is ready to be used. Specify it by name in the Provisioner CRD. Karpenter will use this template when creating new instances.\napiVersion:karpenter.sh/v1alpha5kind:Provisionerspec:provider:launchTemplate:CustomKarpenterLaunchTemplateDemo","categories":"","description":"","excerpt":"By default, Karpenter generates launch templates that use EKS …","ref":"/v0.5.0/aws/launch-templates/","tags":"","title":"Launch Templates and Custom Images"},{"body":"By default, Karpenter generates launch templates that use EKS Optimized AMI for nodes. Often, users need to customize the node image to integrate with existing infrastructure or meet compliance requirements. Karpenter supports custom node images through Launch Templates. If you need to customize the node, then you need a custom launch template.\nNote: By customizing the image, you are taking responsibility for maintaining the image, including security updates. In the default configuration, Karpenter will use the latest version of the EKS optimized AMI, which is maintained by AWS.\nIntroduction Karpenter follows existing AWS patterns for customizing the base image of instances. More specifically, Karpenter uses EC2 launch templates. Launch templates may specify many values. The pivotal value is the base image (AMI). Launch templates further specify many different parameters related to networking, authorization, instance type, and more.\nLaunch Templates and AMIs are unique to AWS regions, similar to EKS clusters. IAM resources are global.\nKarpenter only implements a subset of launch template fields, and some fields should not be set.\nThis guide describes requirements for using launch templates with Karpenter, and later an example procedure.\nLaunch Template Requirements The Launch Template resource includes a large number of fields. AWS accepts launch templates with any subset of these fields defined.\nCertain fields are obviously critical, such as AMI and User Data. Some fields are useful for particular workloads, such as storage and IAM Instance Profile.\nFinally, the majority of Launch Template fields should not be set (or will have no effect), such as network interfaces and instance type.\nImportant Fields When creating a custom launch template, the AMI and User Data are the defining characteristics. Instance Profile (IAM Role) and Security Group (firewall rules) are also important for Karpenter.\nAMI AMI (Amazon Machine Image), is the base image/VM for a launch template.\nReview the instructions for importing a VM to AWS. Note the AMI id generated by this process, such as, ami-074cce78125f09d61.\nUser Data - Autoconfigure Importantly, the AMI must support automatically connecting to a cluster based on “user data”, or a base64 encoded string passed to the instance at startup. The syntax and purpose of the user data varies between images. The Karpenter default OS, Amazon Linux 2 (AL2), accepts shell scripts (bash commands).\nAWS calls data passed to an instance at launch time “user data”.\nIn the default configuration, Karpenter uses an EKS optimized version of AL2 and passes the hostname of the Kubernetes API server, and a certificate. The EKS Optimized AMI includes a bootstrap.sh script which connects the instance to the cluster, based on the passed data.\nAlternatively, you may reference AWS’s bootstrap.sh file when building a custom base image.\n#!/bin/bash\r/etc/eks/bootstrap.sh \u003cmy-cluster-name\u003e \\\r--kubelet-extra-args \u003c'--max-pods=40'\u003e \\\r--b64-cluster-ca \u003ccertificateAuthority\u003e \\\r--apiserver-endpoint \u003cendpoint\u003e \\\r--dns-cluster-ip \u003cserviceIpv4Cidr\u003e \\\r--use-max-pods false\rNote, you must populate this command with live values. Karpenter will not change the user data in the launch template.\nEncode using yaml function !Base64 yaml function or cat userdata.sh | base64 \u003e userdata-encoded.txt shell command.\nBootstrap Script Parameters\nThe sample bootstrap script requires information to join the cluster.\nThese values may be found using:\naws eks describe-cluster --name MyKarpenterCluster\rKubelet Arguments\nSpecifying max-pods can break Karpenter’s binpacking logic (it has no way to know what this setting is). If Karpenter attempts to pack more than this number of pods, the instance may be oversized, and additional pods will reschedule.\nSituational Fields Configure these values in response to a particular use case, such as nodes interacting with another AWS service, or using EBS storage on the node.\nInstance Profile - IAM The launch template must include an “instance profile” – an IAM role.\nThe instance profile must include at least the permissions of the default Karpenter node instance profile. See the default role, KarpenterNodeRole, in the full example below for more information.\nSee also, the managed policy “AmazonEKSWorkerNodePolicy” which includes permission to describe clusters and subnets.\nStorage Karpenter expects nothing of node storage. Configure as needed for your base image.\nSecurity Groups - Firewall The launch template may include a security group (i.e., instance firewall rules) and the security group must be associated with the virtual private cloud (VPC) of the EKS cluster. If none is specified, the default security group of the cluster VPC is used.\nThe security group must permit communication with EKS control plane. Outbound access should be permitted for at least: HTTPS on port 443, DNS (UDP and TCP) on port 53, and your subnet’s network access control list (network ACL).\nFields with Undefined Behavior Resources referenced by these fields are controlled by EKS/Karpenter, and not the launch template.\nInstance Type The instance type should not be specified in the launch template. Karpenter will determine the launch template at run time.\nNetwork Interfaces The AWS CNI will configure the network interfaces. Do not configure network instances in the launch template.\nCreating the Launch Template Launch Templates may be created via the web console, the AWS CLI, or CloudFormation.\nCloudFormation The procedure, in summary, is to:\n Create an AMI as described in the EC2 documentation. Write a EC2 Launch Template specification including the AMI. Push the specification to AWS with CloudFormation. Update the Provisioner CRD to specify the new Launch Template.  An example yaml cloudformation definition of a launch template for Karpenter is provided below.\nCloudFormation yaml is suited for the moderately high configuration density of launch templates, and creating the unusual InstanceProfile resource.\nYou must manually replace these values in the template:\n SecurityGroupID  list all security groups with aws ec2 describe-security-groups   Parameters in UserData AMI  AWSTemplateFormatVersion:'2010-09-09'Resources:# create InstanceProfile wrapper on NodeRoleKarpenterNodeInstanceProfile:Type:\"AWS::IAM::InstanceProfile\"Properties:InstanceProfileName:\"KarpenterNodeInstanceProfile\"Path:\"/\"Roles:- Ref:\"KarpenterNodeRole\"# create role with basic permissions for EKS nodeKarpenterNodeRole:Type:\"AWS::IAM::Role\"Properties:RoleName:\"KarpenterNodeRole\"Path:/AssumeRolePolicyDocument:Version:\"2012-10-17\"Statement:- Effect:AllowPrincipal:Service:!Sub \"ec2.${AWS::URLSuffix}\"Action:- \"sts:AssumeRole\"ManagedPolicyArns:- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEKSWorkerNodePolicy\"- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEKS_CNI_Policy\"- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore\"MyLaunchTemplate:Type:AWS::EC2::LaunchTemplateProperties:LaunchTemplateData:IamInstanceProfile:# Get ARN of InstanceProfile defined aboveArn:!GetAtt- KarpenterNodeInstanceProfile- ArnImageId:ami-074cce78125f09d61# UserData is Base64 EncodedUserData:!Base64 \u003e#!/bin/bash/etc/eks/bootstrap.sh 'MyClusterName' \\--kubelet-extra-args '--node-labels=node.k8s.aws/capacity-type=spot' \\--b64-cluster-ca 'LS0t....0tCg==' \\--apiserver-endpoint 'https://B0385BE29EA792E811CB5866D23C856E.gr7.us-east-2.eks.amazonaws.com'BlockDeviceMappings:- Ebs:VolumeSize:80VolumeType:gp3DeviceName:/dev/xvda# The SecurityGroup must be associated with the cluster VPCSecurityGroupIds:- sg-a69adfdbLaunchTemplateName:KarpenterCustomLaunchTemplateCreate the Launch Template by uploading the CloudFormation yaml file. The sample yaml creates an IAM Object (InstanceProfile), so --capabilities\rCAPABILITY_NAMED_IAM must be indicated.\naws cloudformation create-stack \\\r--stack-name KarpenterLaunchTemplateStack \\\r--template-body file://$(pwd)/lt-cfn-demo.yaml \\\r--capabilities CAPABILITY_NAMED_IAM\rDefine LaunchTemplate for Provisioner The LaunchTemplate is ready to be used. Specify it by name in the Provisioner CRD. Karpenter will use this template when creating new instances.\napiVersion:karpenter.sh/v1alpha5kind:Provisionerspec:provider:launchTemplate:CustomKarpenterLaunchTemplateDemo","categories":"","description":"","excerpt":"By default, Karpenter generates launch templates that use EKS …","ref":"/v0.5.2/aws/launch-templates/","tags":"","title":"Launch Templates and Custom Images"},{"body":"By default, Karpenter generates launch templates that use EKS Optimized AMI for nodes. Often, users need to customize the node image to integrate with existing infrastructure or meet compliance requirements. Karpenter supports custom node images through Launch Templates. If you need to customize the node, then you need a custom launch template.\nNote: By customizing the image, you are taking responsibility for maintaining the image, including security updates. In the default configuration, Karpenter will use the latest version of the EKS optimized AMI, which is maintained by AWS.\nIntroduction Karpenter follows existing AWS patterns for customizing the base image of instances. More specifically, Karpenter uses EC2 launch templates. Launch templates may specify many values. The pivotal value is the base image (AMI). Launch templates further specify many different parameters related to networking, authorization, instance type, and more.\nLaunch Templates and AMIs are unique to AWS regions, similar to EKS clusters. IAM resources are global.\nKarpenter only implements a subset of launch template fields, and some fields should not be set.\nThis guide describes requirements for using launch templates with Karpenter, and later an example procedure.\nLaunch Template Requirements The Launch Template resource includes a large number of fields. AWS accepts launch templates with any subset of these fields defined.\nCertain fields are obviously critical, such as AMI and User Data. Some fields are useful for particular workloads, such as storage and IAM Instance Profile.\nFinally, the majority of Launch Template fields should not be set (or will have no effect), such as network interfaces and instance type.\nImportant Fields When creating a custom launch template, the AMI and User Data are the defining characteristics. Instance Profile (IAM Role) and Security Group (firewall rules) are also important for Karpenter.\nAMI AMI (Amazon Machine Image), is the base image/VM for a launch template.\nReview the instructions for importing a VM to AWS. Note the AMI id generated by this process, such as, ami-074cce78125f09d61.\nUser Data - Autoconfigure Importantly, the AMI must support automatically connecting to a cluster based on “user data”, or a base64 encoded string passed to the instance at startup. The syntax and purpose of the user data varies between images. The Karpenter default OS, Amazon Linux 2 (AL2), accepts shell scripts (bash commands).\nAWS calls data passed to an instance at launch time “user data”.\nIn the default configuration, Karpenter uses an EKS optimized version of AL2 and passes the hostname of the Kubernetes API server, and a certificate. The EKS Optimized AMI includes a bootstrap.sh script which connects the instance to the cluster, based on the passed data.\nAlternatively, you may reference AWS’s bootstrap.sh file when building a custom base image.\n#!/bin/bash\r/etc/eks/bootstrap.sh \u003cmy-cluster-name\u003e \\\r--kubelet-extra-args \u003c'--max-pods=40'\u003e \\\r--b64-cluster-ca \u003ccertificateAuthority\u003e \\\r--apiserver-endpoint \u003cendpoint\u003e \\\r--dns-cluster-ip \u003cserviceIpv4Cidr\u003e \\\r--use-max-pods false\rNote, you must populate this command with live values. Karpenter will not change the user data in the launch template.\nEncode using yaml function !Base64 yaml function or cat userdata.sh | base64 \u003e userdata-encoded.txt shell command.\nBootstrap Script Parameters\nThe sample bootstrap script requires information to join the cluster.\nThese values may be found using:\naws eks describe-cluster --name MyKarpenterCluster\rKubelet Arguments\nSpecifying max-pods can break Karpenter’s binpacking logic (it has no way to know what this setting is). If Karpenter attempts to pack more than this number of pods, the instance may be oversized, and additional pods will reschedule.\nSituational Fields Configure these values in response to a particular use case, such as nodes interacting with another AWS service, or using EBS storage on the node.\nInstance Profile - IAM The launch template must include an “instance profile” – an IAM role.\nThe instance profile must include at least the permissions of the default Karpenter node instance profile. See the default role, KarpenterNodeRole, in the full example below for more information.\nSee also, the managed policy “AmazonEKSWorkerNodePolicy” which includes permission to describe clusters and subnets.\nStorage Karpenter expects nothing of node storage. Configure as needed for your base image.\nSecurity Groups - Firewall The launch template may include a security group (i.e., instance firewall rules) and the security group must be associated with the virtual private cloud (VPC) of the EKS cluster. If none is specified, the default security group of the cluster VPC is used.\nThe security group must permit communication with EKS control plane. Outbound access should be permitted for at least: HTTPS on port 443, DNS (UDP and TCP) on port 53, and your subnet’s network access control list (network ACL).\nFields with Undefined Behavior Resources referenced by these fields are controlled by EKS/Karpenter, and not the launch template.\nInstance Type The instance type should not be specified in the launch template. Karpenter will determine the launch template at run time.\nNetwork Interfaces The AWS CNI will configure the network interfaces. Do not configure network instances in the launch template.\nCreating the Launch Template Launch Templates may be created via the web console, the AWS CLI, or CloudFormation.\nCloudFormation The procedure, in summary, is to:\n Create an AMI as described in the EC2 documentation. Write a EC2 Launch Template specification including the AMI. Push the specification to AWS with CloudFormation. Update the Provisioner CRD to specify the new Launch Template.  An example yaml cloudformation definition of a launch template for Karpenter is provided below.\nCloudFormation yaml is suited for the moderately high configuration density of launch templates, and creating the unusual InstanceProfile resource.\nYou must manually replace these values in the template:\n SecurityGroupID  list all security groups with aws ec2 describe-security-groups   Parameters in UserData AMI  AWSTemplateFormatVersion:'2010-09-09'Resources:# create InstanceProfile wrapper on NodeRoleKarpenterNodeInstanceProfile:Type:\"AWS::IAM::InstanceProfile\"Properties:InstanceProfileName:\"KarpenterNodeInstanceProfile\"Path:\"/\"Roles:- Ref:\"KarpenterNodeRole\"# create role with basic permissions for EKS nodeKarpenterNodeRole:Type:\"AWS::IAM::Role\"Properties:RoleName:\"KarpenterNodeRole\"Path:/AssumeRolePolicyDocument:Version:\"2012-10-17\"Statement:- Effect:AllowPrincipal:Service:!Sub \"ec2.${AWS::URLSuffix}\"Action:- \"sts:AssumeRole\"ManagedPolicyArns:- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEKSWorkerNodePolicy\"- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEKS_CNI_Policy\"- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"- !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore\"MyLaunchTemplate:Type:AWS::EC2::LaunchTemplateProperties:LaunchTemplateData:IamInstanceProfile:# Get ARN of InstanceProfile defined aboveArn:!GetAtt- KarpenterNodeInstanceProfile- ArnImageId:ami-074cce78125f09d61# UserData is Base64 EncodedUserData:!Base64 \u003e#!/bin/bash/etc/eks/bootstrap.sh 'MyClusterName' \\--kubelet-extra-args '--node-labels=node.k8s.aws/capacity-type=spot' \\--b64-cluster-ca 'LS0t....0tCg==' \\--apiserver-endpoint 'https://B0385BE29EA792E811CB5866D23C856E.gr7.us-east-2.eks.amazonaws.com'BlockDeviceMappings:- Ebs:VolumeSize:80VolumeType:gp3DeviceName:/dev/xvda# The SecurityGroup must be associated with the cluster VPCSecurityGroupIds:- sg-a69adfdbLaunchTemplateName:KarpenterCustomLaunchTemplateCreate the Launch Template by uploading the CloudFormation yaml file. The sample yaml creates an IAM Object (InstanceProfile), so --capabilities\rCAPABILITY_NAMED_IAM must be indicated.\naws cloudformation create-stack \\\r--stack-name KarpenterLaunchTemplateStack \\\r--template-body file://$(pwd)/lt-cfn-demo.yaml \\\r--capabilities CAPABILITY_NAMED_IAM\rDefine LaunchTemplate for Provisioner The LaunchTemplate is ready to be used. Specify it by name in the Provisioner CRD. Karpenter will use this template when creating new instances.\napiVersion:karpenter.sh/v1alpha5kind:Provisionerspec:provider:launchTemplate:CustomKarpenterLaunchTemplateDemo","categories":"","description":"","excerpt":"By default, Karpenter generates launch templates that use EKS …","ref":"/v0.5.3/aws/launch-templates/","tags":"","title":"Launch Templates and Custom Images"},{"body":"  #td-cover-block-0 { background-image: url(/background_hu39a8b615144b4974ecd457a3ed8f03fc_140560_960x540_fill_catmullrom_top_2.png); } @media only screen and (min-width: 1200px) { #td-cover-block-0 { background-image: url(/background_hu39a8b615144b4974ecd457a3ed8f03fc_140560_1920x1080_fill_catmullrom_top_2.png); } }  Karpenter Just-in-time Nodes for Any Kubernetes Cluster Get Started           Karpenter simplifies Kubernetes infrastructure with the right nodes at the right time.  Karpenter automatically launches just the right compute resources to handle your cluster's applications. It is designed to let you take full advantage of the cloud with fast and simple compute provisioning for Kubernetes clusters.       Improve application availability  Karpenter responds quickly and automatically to changes in application load, scheduling, and resource requirements, placing new workloads onto a variety of available compute resource capacity.\n   Minimize operational overhead  Karpenter comes with a set of opinionated defaults in a single, declarative Provisioner resource which can easily be customized. No additional configuration required!\n     How It Works Karpenter observes the aggregate resource requests of unscheduled pods and makes decisions to launch and terminate nodes to minimize scheduling latencies and infrastructure cost.      Karpenter is Open Source Software  Karpenter is licensed under the permissive Apache License 2.0. It is designed to work with any Kubernetes cluster running in any environment, including all major cloud providers and on-premises environments. Have an idea for a feature or found something that could work better? Create a GitHub issue and tell us about it.  Get involved     ","categories":"","description":"","excerpt":"  #td-cover-block-0 { background-image: …","ref":"/","tags":"","title":"Karpenter"},{"body":"Karpenter re:Invent 2021 Builders Session ​ ​\nPrerequisites Please install the following tools before starting:\n AWS CLI. If you’re on macOS and have Homebrew installed, simply brew install awscli. Otherwise, follow the AWS CLI user’s guide. Helm, the Kubernetes package manager. If you’re on macOS, feel free to simply brew install helm. Otherwise, follow the Helm installation guide. ​  Get Started Once you have all the necessary tools installed, configure your shell with the credentials for the temporary AWS account created for this session by:\n Navigating to the Event Engine team dashboard and clicking on the “☁️ AWS Console” button Configuring your shell with the credentials required by copy and pasting the command for your operating system. Running the following to set your AWS_ACCOUNT_ID environmental variable: export AWS_ACCOUNT_ID=\"$(aws sts get-caller-identity --query Account --output text)\"  Updating your local Kubernetes configuration (kubeconfig) by running: aws eks update-kubeconfig --name karpenter-demo --role-arn arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterEESetupRole-karpenter-demo  Creating an AWS IAM service-linked role so that Karpenter can provision Spot EC2 instances with the following command: aws iam create-service-linked-role --aws-service-name spot.amazonaws.com N.B. If the role was created previously, you will see:\n# An error occurred (InvalidInput) when calling the CreateServiceLinkedRole operation: Service role name AWSServiceRoleForEC2Spot has been taken in this account, please try a different suffix.   ​ If you can run the following command and see the pods running in your EKS cluster, you’re all set! If not, please ask for help from one of the speakers in the session and they’ll get you squared away. For your reference, the cluster name is karpenter-demo.\nkubectl get pods -A ​ Congratulations! You now have access to an Amazon EKS cluster with an EKS Managed Node Group as well as all the AWS infrastructure necessary to use Karpenter. Happy Building 🔨! ​\nInstall Karpenter Use the following command to install Karpenter into your cluster:\nhelm repo add karpenter https://charts.karpenter.sh helm repo update helm upgrade --install karpenter karpenter/karpenter --namespace karpenter \\  --create-namespace --set serviceAccount.create=false --version 0.5.4 \\  --set controller.clusterName=karpenter-demo \\  --set controller.clusterEndpoint=$(aws eks describe-cluster --name karpenter-demo --query \"cluster.endpoint\" --output json) \\  --wait # for the defaulting webhook to install before creating a Provisioner ​\nNext Steps If you’re a Kubernetes expert, feel free to start exploring how Karpenter works on your own and if you have any questions, one of the AWS speakers will be happy to answer them. ​ If you’d like a guided walkthrough of Karpenter’s features and capabilities, you can follow the Karpenter Getting Started guide starting at the “Provisioner” step. Please don’t hesitate to ask your AWS speaker any questions you might have!\n","categories":"","description":"","excerpt":"Karpenter re:Invent 2021 Builders Session ​ ​\nPrerequisites Please …","ref":"/preview/reinvent/","tags":"","title":""},{"body":"Karpenter re:Invent 2021 Builders Session ​ ​\nPrerequisites Please install the following tools before starting:\n AWS CLI. If you’re on macOS and have Homebrew installed, simply brew install awscli. Otherwise, follow the AWS CLI user’s guide. Helm, the Kubernetes package manager. If you’re on macOS, feel free to simply brew install helm. Otherwise, follow the Helm installation guide. ​  Get Started Once you have all the necessary tools installed, configure your shell with the credentials for the temporary AWS account created for this session by:\n Navigating to the Event Engine team dashboard and clicking on the “☁️ AWS Console” button Configuring your shell with the credentials required by copy and pasting the command for your operating system. Running the following to set your AWS_ACCOUNT_ID environmental variable: export AWS_ACCOUNT_ID=\"$(aws sts get-caller-identity --query Account --output text)\"  Updating your local Kubernetes configuration (kubeconfig) by running: aws eks update-kubeconfig --name karpenter-demo --role-arn arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterEESetupRole-karpenter-demo  Creating an AWS IAM service-linked role so that Karpenter can provision Spot EC2 instances with the following command: aws iam create-service-linked-role --aws-service-name spot.amazonaws.com N.B. If the role was created previously, you will see:\n# An error occurred (InvalidInput) when calling the CreateServiceLinkedRole operation: Service role name AWSServiceRoleForEC2Spot has been taken in this account, please try a different suffix.   ​ If you can run the following command and see the pods running in your EKS cluster, you’re all set! If not, please ask for help from one of the speakers in the session and they’ll get you squared away. For your reference, the cluster name is karpenter-demo.\nkubectl get pods -A ​ Congratulations! You now have access to an Amazon EKS cluster with an EKS Managed Node Group as well as all the AWS infrastructure necessary to use Karpenter. Happy Building 🔨! ​\nInstall Karpenter Use the following command to install Karpenter into your cluster:\nhelm repo add karpenter https://charts.karpenter.sh helm repo update helm upgrade --install karpenter karpenter/karpenter --namespace karpenter \\  --create-namespace --set serviceAccount.create=false --version 0.5.4 \\  --set controller.clusterName=karpenter-demo \\  --set controller.clusterEndpoint=$(aws eks describe-cluster --name karpenter-demo --query \"cluster.endpoint\" --output json) \\  --wait # for the defaulting webhook to install before creating a Provisioner ​\nNext Steps If you’re a Kubernetes expert, feel free to start exploring how Karpenter works on your own and if you have any questions, one of the AWS speakers will be happy to answer them. ​ If you’d like a guided walkthrough of Karpenter’s features and capabilities, you can follow the Karpenter Getting Started guide starting at the “Provisioner” step. Please don’t hesitate to ask your AWS speaker any questions you might have!\n","categories":"","description":"","excerpt":"Karpenter re:Invent 2021 Builders Session ​ ​\nPrerequisites Please …","ref":"/v0.5.0/reinvent/","tags":"","title":""},{"body":"Karpenter re:Invent 2021 Builders Session ​ ​\nPrerequisites Please install the following tools before starting:\n AWS CLI. If you’re on macOS and have Homebrew installed, simply brew install awscli. Otherwise, follow the AWS CLI user’s guide. Helm, the Kubernetes package manager. If you’re on macOS, feel free to simply brew install helm. Otherwise, follow the Helm installation guide. ​  Get Started Once you have all the necessary tools installed, configure your shell with the credentials for the temporary AWS account created for this session by:\n Navigating to the Event Engine team dashboard and clicking on the “☁️ AWS Console” button Configuring your shell with the credentials required by copy and pasting the command for your operating system. Running the following to set your AWS_ACCOUNT_ID environmental variable: export AWS_ACCOUNT_ID=\"$(aws sts get-caller-identity --query Account --output text)\"  Updating your local Kubernetes configuration (kubeconfig) by running: aws eks update-kubeconfig --name karpenter-demo --role-arn arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterEESetupRole-karpenter-demo  Creating an AWS IAM service-linked role so that Karpenter can provision Spot EC2 instances with the following command: aws iam create-service-linked-role --aws-service-name spot.amazonaws.com N.B. If the role was created previously, you will see:\n# An error occurred (InvalidInput) when calling the CreateServiceLinkedRole operation: Service role name AWSServiceRoleForEC2Spot has been taken in this account, please try a different suffix.   ​ If you can run the following command and see the pods running in your EKS cluster, you’re all set! If not, please ask for help from one of the speakers in the session and they’ll get you squared away. For your reference, the cluster name is karpenter-demo.\nkubectl get pods -A ​ Congratulations! You now have access to an Amazon EKS cluster with an EKS Managed Node Group as well as all the AWS infrastructure necessary to use Karpenter. Happy Building 🔨! ​\nInstall Karpenter Use the following command to install Karpenter into your cluster:\nhelm repo add karpenter https://charts.karpenter.sh helm repo update helm upgrade --install karpenter karpenter/karpenter --namespace karpenter \\  --create-namespace --set serviceAccount.create=false --version 0.5.4 \\  --set controller.clusterName=karpenter-demo \\  --set controller.clusterEndpoint=$(aws eks describe-cluster --name karpenter-demo --query \"cluster.endpoint\" --output json) \\  --wait # for the defaulting webhook to install before creating a Provisioner ​\nNext Steps If you’re a Kubernetes expert, feel free to start exploring how Karpenter works on your own and if you have any questions, one of the AWS speakers will be happy to answer them. ​ If you’d like a guided walkthrough of Karpenter’s features and capabilities, you can follow the Karpenter Getting Started guide starting at the “Provisioner” step. Please don’t hesitate to ask your AWS speaker any questions you might have!\n","categories":"","description":"","excerpt":"Karpenter re:Invent 2021 Builders Session ​ ​\nPrerequisites Please …","ref":"/v0.5.2/reinvent/","tags":"","title":""},{"body":"Karpenter re:Invent 2021 Builders Session ​ ​\nPrerequisites Please install the following tools before starting:\n AWS CLI. If you’re on macOS and have Homebrew installed, simply brew install awscli. Otherwise, follow the AWS CLI user’s guide. Helm, the Kubernetes package manager. If you’re on macOS, feel free to simply brew install helm. Otherwise, follow the Helm installation guide. ​  Get Started Once you have all the necessary tools installed, configure your shell with the credentials for the temporary AWS account created for this session by:\n Navigating to the Event Engine team dashboard and clicking on the “☁️ AWS Console” button Configuring your shell with the credentials required by copy and pasting the command for your operating system. Running the following to set your AWS_ACCOUNT_ID environmental variable: export AWS_ACCOUNT_ID=\"$(aws sts get-caller-identity --query Account --output text)\"  Updating your local Kubernetes configuration (kubeconfig) by running: aws eks update-kubeconfig --name karpenter-demo --role-arn arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterEESetupRole-karpenter-demo  Creating an AWS IAM service-linked role so that Karpenter can provision Spot EC2 instances with the following command: aws iam create-service-linked-role --aws-service-name spot.amazonaws.com N.B. If the role was created previously, you will see:\n# An error occurred (InvalidInput) when calling the CreateServiceLinkedRole operation: Service role name AWSServiceRoleForEC2Spot has been taken in this account, please try a different suffix.   ​ If you can run the following command and see the pods running in your EKS cluster, you’re all set! If not, please ask for help from one of the speakers in the session and they’ll get you squared away. For your reference, the cluster name is karpenter-demo.\nkubectl get pods -A ​ Congratulations! You now have access to an Amazon EKS cluster with an EKS Managed Node Group as well as all the AWS infrastructure necessary to use Karpenter. Happy Building 🔨! ​\nInstall Karpenter Use the following command to install Karpenter into your cluster:\nhelm repo add karpenter https://charts.karpenter.sh helm repo update helm upgrade --install karpenter karpenter/karpenter --namespace karpenter \\  --create-namespace --set serviceAccount.create=false --version 0.5.4 \\  --set controller.clusterName=karpenter-demo \\  --set controller.clusterEndpoint=$(aws eks describe-cluster --name karpenter-demo --query \"cluster.endpoint\" --output json) \\  --wait # for the defaulting webhook to install before creating a Provisioner ​\nNext Steps If you’re a Kubernetes expert, feel free to start exploring how Karpenter works on your own and if you have any questions, one of the AWS speakers will be happy to answer them. ​ If you’d like a guided walkthrough of Karpenter’s features and capabilities, you can follow the Karpenter Getting Started guide starting at the “Provisioner” step. Please don’t hesitate to ask your AWS speaker any questions you might have!\n","categories":"","description":"","excerpt":"Karpenter re:Invent 2021 Builders Session ​ ​\nPrerequisites Please …","ref":"/v0.5.3/reinvent/","tags":"","title":""}]